{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LvziXoe21z_G_gh"
   },
   "source": [
    "# **Hugging Face Transformers Library**\n",
    "[Hugging Face](https://huggingface.co) is a large open-source community. Their [Hub](https://huggingface.co/models) has a very large number of pre-trained deep learning models, mainly aimed at NLP, using Transformers. In this lab, you will learn how to use pre-trained Hugging Face models to solve various NLP, CV, and Audio tasks.\n",
    "\n",
    "**Instructions**\n",
    "- Write code in the space indicated with `### START CODE HERE ###`\n",
    "- Do not use loops (for/while) unless instructions explicitly tell you so. Parallelization in Deep Learning is key!\n",
    "- If you get stuck, ask for help in Slack or DM `@DRU Team`\n",
    "\n",
    "**You will learn**\n",
    "- How to use `pipeline()` from the [Models Hub](https://huggingface.co/models) for inference on a variety of NLP, CV, and Audio tasks\n",
    "- How to use [Gradio](https://gradio.app/docs/) library for easy creating user interface and handling the pipeline inference directly on Google Colab notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PQuWgaHU13zo"
   },
   "source": [
    "# **Import packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Kbi2PMjy109L",
    "ExecuteTime": {
     "end_time": "2024-08-08T12:39:17.912602Z",
     "start_time": "2024-08-08T12:39:12.297769Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "#!pip install transformers==4.17.0\n",
    "import transformers\n",
    "from transformers import pipeline\n",
    "from transformers import Conversation\n",
    "\n",
    "#!pip install sentence_transformers\n",
    "import sentence_transformers\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "#!pip install torch-scatter -f https://pytorch-geometric.com/whl/torch-1.10.0+cu111.html\n",
    "\n",
    "#!pip install gradio\n",
    "import gradio as gr\n",
    "\n",
    "#!pip install sentencepiece\n",
    "\n",
    "#!pip install timm\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "cpLrb0yup3v4",
    "ExecuteTime": {
     "end_time": "2024-08-08T12:39:17.927616Z",
     "start_time": "2024-08-08T12:39:17.914609Z"
    }
   },
   "outputs": [],
   "source": [
    "# VALIDATION_FIELD[func] set_seed\n",
    "\n",
    "def set_seed(seed):\n",
    "    np.random.seed(seed) \n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "83w7BY3B1-HT"
   },
   "source": [
    "**List of tasks that we will cover:**\n",
    "\n",
    "- **NLP**:\n",
    "    - Masked language modeling\n",
    "    - Question Answering\n",
    "    - Table Question Answering\n",
    "    - Summarization\n",
    "    - Sentence Similarity\n",
    "    - Text Classification\n",
    "    - Token Classification\n",
    "    - Zero-Shot Classification\n",
    "    - Translation\n",
    "    - Text generation\n",
    "    - Text2Text Generation\n",
    "    - Conversational\n",
    "- **Audio**:\n",
    "    - Text-to-Speech\n",
    "    - Automatic Speech Recognition\n",
    "\n",
    "- **Computer Vision**:\n",
    "    - Image Classification\n",
    "    - Image Segmentation\n",
    "\n",
    "The easiest way to use a pre-trained Transformer model on a given task is to use a `pipeline()`.\n",
    "\n",
    "[Pipeline](https://huggingface.co/docs/transformers/pipeline_tutorial) helps to perform all pre-processing and post-processing steps on your input data:\n",
    "1. **Tokenization:** Split the initial input into multiple sub-entities with properties (tokens).\n",
    "2. **Inference:** Maps every token into a more meaningful representation.\n",
    "3. **Decoding:** Use the above representation to generate and/or extract the final output for the given task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DhEId9ru5WJ2"
   },
   "source": [
    "# **NLP Tasks**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Fg2fNqD2FHC"
   },
   "source": [
    "## **Masked language modeling**\n",
    "\n",
    "[Masked language modeling](https://huggingface.co/tasks/fill-mask) (MLM) is the task of masking tokens (e.g., replaced by `[MASK]`) in a sequence and predicting which words should replace this masked word with an appropriate token.\n",
    "\n",
    "### Inference with Fill-Mask Pipeline\n",
    "\n",
    "You can use the Transformers library `fill-mask` pipeline to infer with masked language models, which you can find on the [Hugging Face Hub](https://huggingface.co/models?pipeline_tag=fill-mask&sort=downloads). There are many pretrained models here that can be used to solve the MLM task.\n",
    "\n",
    "**Excercise:**\n",
    ">For the proposed `masked_sequences`, we suggest finding you among the models on the [hub](https://huggingface.co/models?pipeline_tag=fill-mask&sort=downloads) a pretrained base BERT (uncased) model, which is already pretrained on a large corpus of English data, and use it in the pipeline for inference. Your model should return `top_k=2` predictions for each sentence in `unmasked_sequences`. All the parameters that the `fill-mask` pipeline uses are described [here](https://huggingface.co/docs/transformers/main_classes/pipelines#transformers.FillMaskPipeline).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "BANDUtPF150z",
    "ExecuteTime": {
     "end_time": "2024-08-08T12:39:26.698610Z",
     "start_time": "2024-08-08T12:39:17.929964Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Masked sentence 1: I love animals very much, that's why I work as a [MASK].\n",
      "Suggested unmasked sentences:\n",
      "\ti love animals very much, that's why i work as a vet. \t | Masked word: vet\n",
      "\ti love animals very much, that's why i work as a farmer. \t | Masked word: farmer\n",
      "\n",
      "Masked sentence 2: I live several years in Russia, but now I live in [MASK].\n",
      "Suggested unmasked sentences:\n",
      "\ti live several years in russia, but now i live in france. \t | Masked word: france\n",
      "\ti live several years in russia, but now i live in ukraine. \t | Masked word: ukraine\n",
      "\n",
      "Masked sentence 3: I love to play [MASK], so my bought me a ball.\n",
      "Suggested unmasked sentences:\n",
      "\ti love to play football, so my bought me a ball. \t | Masked word: football\n",
      "\ti love to play baseball, so my bought me a ball. \t | Masked word: baseball\n"
     ]
    }
   ],
   "source": [
    "# VALIDATION_FIELD[func] model_mlm\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "masked_sequences = [\"I love animals very much, that's why I work as a [MASK].\",\n",
    "        \"I live several years in Russia, but now I live in [MASK].\",\n",
    "        \"I love to play [MASK], so my bought me a ball.\"\n",
    "        ]\n",
    "\n",
    "### START CODE HERE ### (â‰ˆ 2-4 lines of code)\n",
    "model_mlm = transformers.pipeline(task='fill-mask', model='bert-base-uncased') # specify: task and model\n",
    "unmasked_sequences = model_mlm(masked_sequences, top_k=2) # use the pipeline with parameter top_k=2\n",
    "### END CODE HERE ###\n",
    "\n",
    "for text_i in range(len(masked_sequences)):\n",
    "    print(f\"\\nMasked sentence {text_i + 1}: {masked_sequences[text_i]}\\nSuggested unmasked sentences:\")\n",
    "    for output_i in unmasked_sequences[text_i]:\n",
    "        print(f\"\\t{output_i['sequence']} \\t | Masked word: {output_i['token_str']}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gpClPQK1qi5j"
   },
   "source": [
    "**Expected output:**\n",
    "```\n",
    "Masked sentence 1: I love animals very much, that's why I work as a [MASK].\n",
    "Suggested unmasked sentences:\n",
    "\ti love animals very much, that's why i work as a vet. \t | Masked word: vet\n",
    "\ti love animals very much, that's why i work as a farmer.   | Masked word: farmer\n",
    "\n",
    "Masked sentence 2: I live several years in Russia, but now I live in [MASK].\n",
    "Suggested unmasked sentences:\n",
    "\ti live several years in russia, but now i live in france. \t | Masked word: france\n",
    "\ti live several years in russia, but now i live in ukraine. \t| Masked word: ukraine\n",
    "\n",
    "Masked sentence 3: I love to play [MASK], so my bought me a ball.\n",
    "Suggested unmasked sentences:\n",
    "\ti love to play football, so my bought me a ball. \t | Masked word: football\n",
    "\ti love to play baseball, so my bought me a ball. \t | Masked word: baseball\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lV9_ctyt9MkG"
   },
   "source": [
    "## **Question Answering**\n",
    "[Question Answering](https://huggingface.co/tasks/question-answering) (QA) is a type of language task that produces answers based on a given document/text or generates answers without context.\n",
    "\n",
    "### Inference with Question Answering Pipeline\n",
    "You can find on the Hugging Face Hub a pretrained model among a wide variety of models for [QA tasks](https://huggingface.co/models?pipeline_tag=question-answering&sort=downloads). [Here](https://huggingface.co/docs/transformers/main_classes/pipelines#transformers.QuestionAnsweringPipeline) is a description of the parameters for the `question-answering` pipeline.\n",
    "\n",
    "**Excercise:** \n",
    ">Find on the [Hugging Face Hub](https://huggingface.co/models?pipeline_tag=question-answering&sort=downloads) an original pretrained model to produce answers based on a given text. This model makes up the composition of transformer encoders, and it is an autoencoder. Architecture is: `24-layer`, `1024-hidden`, `16-heads`, `340M params`. This model was trained on lower-cased English text using the technique that masks all subwords corresponding to a word at once and was fine-tuned on the Stanford Question Answering Dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "qJGsKjaU2N_Z",
    "ExecuteTime": {
     "end_time": "2024-08-08T12:39:38.392740Z",
     "start_time": "2024-08-08T12:39:26.699849Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Which fish has long fins?\n",
      "Answer: Finny\n",
      "Question: Which fish has great big eyes?\n",
      "Answer: Igor\n",
      "Question: Which fish has a big tail?\n",
      "Answer: Tayla\n",
      "Question: Which of these fish would you most like to have as a pet?\n",
      "Answer: Igor\n"
     ]
    }
   ],
   "source": [
    "# VALIDATION_FIELD[func] model_qa\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "### START CODE HERE ### (â‰ˆ 1-2 lines of code)\n",
    "model_qa = transformers.pipeline(task='question-answering', \n",
    "                                model='bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "#model_qa = transformers.pipeline(task='question-answering', model=\"distilbert/distilbert-base-cased-distilled-squad\")\n",
    "### END CODE HERE ###\n",
    "\n",
    "context = r\"\"\"\n",
    "These fish have names.\n",
    "This is Finny.\n",
    "Finny has beautiful long fins that help her swim fast.\n",
    "This is Tayla.\n",
    "Taylaâ€™s big tail moves from side to side and it helps her to go this way or that way.\n",
    "This is Igor.\n",
    "Igor has great big eyes. Igorâ€™s great big eyes help him to see where he is going and they also help him to see\n",
    "big scary fish!\n",
    "\"\"\"\n",
    "\n",
    "questions = [\n",
    "    \"Which fish has long fins?\",\n",
    "    \"Which fish has great big eyes?\",\n",
    "    \"Which fish has a big tail?\",\n",
    "    \"Which of these fish would you most like to have as a pet?\"\n",
    "    ]\n",
    "\n",
    "for question in questions:\n",
    "    answ = model_qa(question=question, context=context)\n",
    "    print(f\"Question: {question}\\nAnswer: {answ['answer']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S1QjRCfXrbKL"
   },
   "source": [
    "**Expected output:**\n",
    "```\n",
    "Question: Which fish has long fins?\n",
    "Answer: Finny\n",
    "Question: Which fish has great big eyes?\n",
    "Answer: Igor\n",
    "Question: Which fish has a big tail?\n",
    "Answer: Tayla\n",
    "Question: Which of these fish would you most like to have as a pet?\n",
    "Answer: Igor\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q2oAMFt-_tFu"
   },
   "source": [
    "## **Table Question Answering**\n",
    "\n",
    "The task aims to find semantic question answering based on structured tabular data.\n",
    "\n",
    "### Inference with Table Question Answering Pipeline\n",
    "\n",
    "For solving the TQA task, you can find a model on the [Hugging Face Hub](https://huggingface.co/models?pipeline_tag=table-question-answering&sort=downloads) and use it for inference with the `table-question-answering` pipeline.\n",
    "\n",
    "**Excercise:** \n",
    ">Find on the [Hugging Face Hub](https://huggingface.co/models?pipeline_tag=table-question-answering&sort=downloads) a pretrained model and use it to find answers based on the given table. It is a BERT-based model from Google with architecture: `24-layer`, `1024-hidden`, `16-heads`, `340M params`. The model is fine-tuned on WikiTable Questions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "ogfrKfRG_2F6",
    "ExecuteTime": {
     "end_time": "2024-08-08T12:39:48.339963Z",
     "start_time": "2024-08-08T12:39:38.394898Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: how did the country donate the most?, \n",
      "Answer: France\n",
      "------------------------------\n",
      "Question: how much humanitarian aid poland gave?, \n",
      "Answer: 50 tons\n",
      "------------------------------\n",
      "Question: who donated 1.5 billion?, \n",
      "Answer: USA\n",
      "------------------------------\n",
      "Question: when did france donate the money?, \n",
      "Answer: 10 march 2022\n",
      "------------------------------\n",
      "Question: whoes donated amount is lowest among all?, \n",
      "Answer: Poland\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# VALIDATION_FIELD[func] model_tqa_de\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "data = {\n",
    "    \"country\": [\"USA\", \"France\", \"Poland\"],\n",
    "    \"donated money\": [\"1.5 billion\", \"2 billion\", \"0.5 billion\"],\n",
    "    \"humanitarian aid\": [\"30 tons\", \"2 tons\", \"50 tons\"],\n",
    "    \"date\": [\"7 march 2022\", \"10 march 2022\", \"28 march 2022\"],\n",
    "}\n",
    "\n",
    "questions = [\"how did the country donate the most?\",\n",
    "             \"how much humanitarian aid poland gave?\",\n",
    "             \"who donated 1.5 billion?\",\n",
    "             \"when did france donate the money?\",\n",
    "             \"whoes donated amount is lowest among all?\"]\n",
    "\n",
    "\n",
    "table = pd.DataFrame.from_dict(data)\n",
    "\n",
    "### START CODE HERE ### (â‰ˆ 1-2 lines of code)\n",
    "model_tqa = transformers.pipeline(task='table-question-answering', model='google/tapas-large-finetuned-wtq')\n",
    "### END CODE HERE ###\n",
    "\n",
    "for question in questions:\n",
    "    output = model_tqa(table=table, query=question)\n",
    "    print(f\"Question: {question}, \\nAnswer: {output['cells'][0]}\")\n",
    "    print(30*'-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BSGzJ0xerp0K"
   },
   "source": [
    "**Expected output:**\n",
    "```\n",
    "Question: how did the country donate the most?, \n",
    "Answer: France\n",
    "------------------------------\n",
    "Question: how much humanitarian aid poland gave?, \n",
    "Answer: 50 tons\n",
    "------------------------------\n",
    "Question: who donated 1.5 billion?, \n",
    "Answer: USA\n",
    "------------------------------\n",
    "Question: when did france donate the money?, \n",
    "Answer: 10 march 2022\n",
    "------------------------------\n",
    "Question: whoes donated amount is lowest among all?, \n",
    "Answer: Poland\n",
    "------------------------------\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fX4osP1xHLNa"
   },
   "source": [
    "## **Summarization**\n",
    "\n",
    "[Summarization](https://huggingface.co/tasks/summarization) is the task of producing a shorter version of one or several docs preserving the meaning of the text.\n",
    "\n",
    "### Inference with Summarization Pipeline\n",
    "\n",
    "On the [Hugging Face Hub](https://huggingface.co/models?pipeline_tag=summarization&sort=downloads), you can find the Transformers library `summarization` pipeline to infer with models, which are pretrained for summarization tasks.\n",
    "\n",
    "**Excercise:** \n",
    ">Using a model description, find it and use it in the pipeline summarization for the given text. It is a transformer encoder-decoder that combines Google's BERT (encoder) and OpenAI's GPT (decoder). The model contains `1024-hidden` layers and `406M params` and has been fine-tuned using CNN (news summarization dataset).\n",
    "\n",
    "Generate summary using parameters:\n",
    "\n",
    " - `min_length=50`\n",
    " - `max_length=120`\n",
    " - `length_penalty=2.0`\n",
    " - `num_beams=4`\n",
    " - `early_stopping=True`\n",
    "\n",
    "[This article](https://github.com/huggingface/blog/blob/main/notebooks/02_how_to_generate.ipynb) will help you understand a bit more about these parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "z7EE0WSDADZo",
    "ExecuteTime": {
     "end_time": "2024-08-08T12:40:09.290485Z",
     "start_time": "2024-08-08T12:39:48.341137Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boris Akunin: 'I was always, let's say, suspicious of him' 'The first thing that Putin did, he attacked the independent media' 'He monopolised propaganda and used TV channels to zombify people' 'It's absolutely not surprising that lots of Russians believe all the lies'\n"
     ]
    }
   ],
   "source": [
    "# VALIDATION_FIELD[func] model_summarizer\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "text = \"\"\"Boris Akunin: Initially, the reason was that I have no trust for KGB people at all. \n",
    "So someone who comes from the KGB, from my point of view, should not become the president of Russia. \n",
    "So I never voted for Putin. I was always, letâ€™s say, suspicious of him. And my suspicions proved to be true very fast, \n",
    "because the first thing that Putin did, he attacked the independent media. I remember quite well signing letters in support \n",
    "of independent media, and well, we lost.\n",
    "\n",
    "And then that man was very methodically killing all the branches of democracy. He started with television, \n",
    "he monopolised propaganda and used TV channels to zombify people, and this has been going on for more than 20 years.\n",
    "\n",
    "So itâ€™s absolutely not surprising that lots of Russians believe all the lies that are being told about Ukraine and about \n",
    "the world in general, not surprising at all.\n",
    "\n",
    "Then he destroyed the independent courts and judicial system, he destroyed the parliamentary system, then he corrupted presidential \n",
    "elections. There hasnâ€™t been any free presidential election during Putinâ€™s times.\n",
    "\"\"\"\n",
    "\n",
    "### START CODE HERE ### (â‰ˆ 2-8 lines of code)\n",
    "model_summarizer = transformers.pipeline(task='summarization', model='facebook/bart-large-cnn')\n",
    "summarized_text = model_summarizer(text, min_length=50, max_length=120, length_penalty=2.0, num_beams=4,\n",
    "                                   early_stopping=True)\n",
    "### END CODE HERE ###  \n",
    "                                 \n",
    "print(summarized_text[0]['summary_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vib5QdGUrwBF"
   },
   "source": [
    "**Expected output:**\n",
    "```\n",
    "Akunin: 'I was always, let's say, suspicious of him' 'The first thing that Putin did, he attacked the independent media' 'He monopolised propaganda and used TV channels to zombify people' 'It's absolutely not surprising that lots of Russians believe all the lies'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c87ZWHtOKT3p"
   },
   "source": [
    "## **Sentence Similarity**\n",
    "\n",
    "[Sentence Similarity](https://huggingface.co/tasks/sentence-similarity) is the task of determining how similar the semantic similarity between two texts.\n",
    "\n",
    "### The Sentence Transformers library\n",
    "The [Sentence Transformers](https://www.sbert.net) library helps to calculate embeddings of textual documents. An embedding is just a vector that captures the semantic information. They help find how similar texts are.\n",
    "\n",
    "**Excercise:** \n",
    ">Your task is to find a [sentence transformer](https://www.sbert.net/docs/pretrained_models.html) on the [Hugging Face Hub](https://huggingface.co/models?library=sentence-transformers&sort=downloads) to calculate the similarity between given sentences. Transformer model description: it is an all-round tuned model with one of the highest encoding speeds (sentences/ sec ~ 14200). Model architecture: `max-seq-len 256`, `384-hidden`, `6-layer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "YAEgR0CFHs5N",
    "ExecuteTime": {
     "end_time": "2024-08-08T12:40:09.785444Z",
     "start_time": "2024-08-08T12:40:09.292487Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I love data science! <--VS--> To be honest, I don't really like data science. | Score: 0.805\n",
      "\n",
      "I love data science! <--VS--> Data science is a very powerful thing. | Score: 0.741\n",
      "\n",
      "To be honest, I don't really like data science. <--VS--> Data science is a very powerful thing. | Score: 0.739\n",
      "\n",
      "I love data science! <--VS--> I love programming in python. | Score: 0.482\n",
      "\n",
      "To be honest, I don't really like data science. <--VS--> I love programming in python. | Score: 0.417\n",
      "\n",
      "Data science is a very powerful thing. <--VS--> I love programming in python. | Score: 0.382\n"
     ]
    }
   ],
   "source": [
    "# VALIDATION_FIELD[func] model_sentence_similarity\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "sentences = ['I love data science!',\n",
    "             \"To be honest, I don't really like data science.\", \n",
    "             \"Data science is a very powerful thing.\", \n",
    "             \"I love programming in python.\"]\n",
    "\n",
    "### START CODE HERE ### (â‰ˆ 2 lines of code)\n",
    "model_sentence_similarity = sentence_transformers.SentenceTransformer('all-MiniLM-L6-v2')\n",
    "embeddings = model_sentence_similarity.encode(sentences)\n",
    "### END CODE HERE ###\n",
    "\n",
    "# compute cosine similarities\n",
    "cosine_scores = util.cos_sim(embeddings, embeddings)\n",
    "\n",
    "# find the pairs with the highest cosine similarity scores\n",
    "seq_pairs = []\n",
    "for i in range(len(cosine_scores) - 1):\n",
    "    for j in range(i + 1, len(cosine_scores)):\n",
    "        seq_pairs.append({'index': [i, j], 'score': cosine_scores[i][j]})\n",
    "\n",
    "# sort scores\n",
    "seq_pairs = sorted(seq_pairs, key=lambda x: x['score'], reverse=True)\n",
    "\n",
    "for seqs_pair in seq_pairs:\n",
    "    i, j = seqs_pair['index']\n",
    "    print(\"{} <--VS--> {} | Score: {:.3f}\\n\".format(sentences[i], sentences[j], seqs_pair['score']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zcor51mKrxeU"
   },
   "source": [
    "**Expected output:**\n",
    "```\n",
    "I love data science! <--VS--> To be honest, I don't really like data science. | Score: 0.805\n",
    "\n",
    "I love data science! <--VS--> Data science is a very powerful thing. | Score: 0.741\n",
    "\n",
    "To be honest, I don't really like data science. <--VS--> Data science is a very powerful thing. | Score: 0.739\n",
    "\n",
    "I love data science! <--VS--> I love programming in python. | Score: 0.482\n",
    "\n",
    "To be honest, I don't really like data science. <--VS--> I love programming in python. | Score: 0.417\n",
    "\n",
    "Data science is a very powerful thing. <--VS--> I love programming in python. | Score: 0.382\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "etU2QJgkRvAY"
   },
   "source": [
    "## **Text Classification**\n",
    "\n",
    "[Text classification](https://huggingface.co/tasks/text-classification) is the process of classifying documents into predefined categories (labels) based on their content. Text classification models use in sentiment analysis, natural language inference, and assessing grammatical correctness cases.\n",
    "\n",
    "### Inference with Text Classification Pipeline\n",
    "On the [Hugging Face Hub](https://huggingface.co/models?pipeline_tag=text-classification), you can find the Transformers library `text-classification` pipeline to infer with Text Classification models.\n",
    "\n",
    "**Excercise:** \n",
    ">Find described model on the Hub and classify emotions for the given English text data. This model is a distilled version of the same model name and, on average, is twice as fast as the base version. It follows the same training procedure as DistilBERT. The model has `6-layers`, `768-hidden`, and `12-heads`, `82M params`.\n",
    "\n",
    "As one of the pipeline parameters, use `return_all_scores=False` to return one max scored emotion per sentence.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "wPml7FgCO6HC",
    "ExecuteTime": {
     "end_time": "2024-08-08T12:40:15.318318Z",
     "start_time": "2024-08-08T12:40:09.786446Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All human souls are connected, which explains why it hurts so much to detach from others. | Emoji: ðŸ˜­(sadness) | Score: 0.807\n",
      "\n",
      "The present moment is filled with joy and happiness. If you are attentive, you will see it. | Emoji: ðŸ˜(joy) | Score: 0.925\n",
      "\n",
      "Learn to use the criticism as fuel and you will never run out of energy. | Emoji: ðŸ˜(neutral) | Score: 0.758\n"
     ]
    }
   ],
   "source": [
    "# VALIDATION_FIELD[func] model_text_classifier_de\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# emotions model can predict\n",
    "emoji_dict = {\n",
    "    'sadness': \"\\U0001F62D\",\n",
    "    'anger': \"\\U0001F621\",\n",
    "    'disgust': \"\\U0001F922\",\n",
    "    'fear': \"\\U0001F628\",\n",
    "    'joy': \"\\U0001F601\",\n",
    "    'neutral': \"\\U0001F610\",\n",
    "    'surprise': \"\\U0001F62F\"\n",
    "}\n",
    "\n",
    "texts = [\"All human souls are connected, which explains why it hurts so much to detach from others.\",\n",
    "        \"The present moment is filled with joy and happiness. If you are attentive, you will see it.\",\n",
    "        \"Learn to use the criticism as fuel and you will never run out of energy.\"]\n",
    "\n",
    "### START CODE HERE ### (â‰ˆ 2-4 lines of code)\n",
    "model_text_classifier = transformers.pipeline(task='text-classification', model='j-hartmann/emotion-english-distilroberta-base')\n",
    "appropriate_emotions = model_text_classifier(texts, return_all_scores=False)\n",
    "### END CODE HERE ###\n",
    "\n",
    "for text, emotion in zip(texts, appropriate_emotions):\n",
    "    print(\"{} | Emoji: {}({}) | Score: {:.3f}\\n\".format(text, emoji_dict[emotion['label']],emotion['label'], emotion['score']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b1aM9VP6r0Fs"
   },
   "source": [
    "**Expected output:**\n",
    "```\n",
    "All human souls are connected, which explains why it hurts so much to detach from others. | Emoji: ðŸ˜­(sadness) | Score: 0.807\n",
    "\n",
    "The present moment is filled with joy and happiness. If you are attentive, you will see it. | Emoji: ðŸ˜(joy) | Score: 0.925\n",
    "\n",
    "Learn to use the criticism as fuel and you will never run out of energy. | Emoji: ðŸ˜(neutral) | Score: 0.758\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E09mWONKcCi5"
   },
   "source": [
    "## **Token Classification**\n",
    "\n",
    "It is one of the NLP tasks in which a label is assigned to some tokens in a text. [Token Classification](https://huggingface.co/tasks/token-classification) models use in Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models are aimed to identify specific entities in a text (dates, individuals, and places). PoS tagging uses in identifying verbs, nouns, and punctuation marks.\n",
    "\n",
    "Our task here will be to identify the words' entity for the given text so that we will solve the NER problem.\n",
    "\n",
    "### Inference with Token Classification Pipeline\n",
    "\n",
    "Here on the [Hugging Face Hub](https://huggingface.co/models?pipeline_tag=token-classification&sort=downloads), you can find pretrained models that can solve the Named Entity Recognition problem and use them in `token-classification` or `ner` pipelines.\n",
    "\n",
    "**Excercise:** \n",
    ">You should do NER for the given sentence and find a described model on the [Hub](https://huggingface.co/models?pipeline_tag=token-classification&sort=downloads) to solve the task. The model is a fine-tuned famous bidirectional transformer for NER, trained on upper-cased English text. Model architecture: `768-hidden`, `12-heads`, and `6-layers`.\n",
    "\n",
    "Each token will be classified by the model as one of the following classes:\n",
    "\n",
    "| Abbreviation | Description             |\n",
    "|-------|--------------------------------|\n",
    "| O     | Outside of a named entity      |\n",
    "| B-MIS | Beginning of a miscellaneous entity right after another miscellaneous entity                     |\n",
    "| I-MIS | Miscellaneous entity           |\n",
    "| B-PER | Beginning of a personâ€™s name right after another personâ€™s name |\n",
    "| I-PER | Personâ€™s name                  |\n",
    "| B-ORG | Beginning of an organization right after another organization |\n",
    "| I-ORG | Organization |\n",
    "| B-LOC | Beginning of a location right after another location |\n",
    "| I-LOC | Location |\n",
    "\n",
    "\t\n",
    "\t\n",
    "\t\n",
    "\t\n",
    "\t\n",
    "\t\n",
    "\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "7L3j-pJscHYN",
    "ExecuteTime": {
     "end_time": "2024-08-08T12:40:21.466966Z",
     "start_time": "2024-08-08T12:40:15.319320Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": "   entity     score  index     word  start  end\n0   B-PER  0.999165      1     Mark      0    4\n1   B-PER  0.997683      3     Mary      9   13\n2   B-LOC  0.999792      6  Ukraine     22   29\n3   B-LOC  0.999481     20        K     99  100\n4   I-LOC  0.998003     21    ##har    100  103\n5   I-LOC  0.996411     22     ##ki    103  105\n6   I-LOC  0.994031     23      ##v    105  106\n7   B-LOC  0.999337     25       Ch    111  113\n8   I-LOC  0.938038     26    ##ern    113  116\n9   I-LOC  0.998129     27     ##ih    116  118\n10  I-LOC  0.995637     28     ##iv    118  120",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>entity</th>\n      <th>score</th>\n      <th>index</th>\n      <th>word</th>\n      <th>start</th>\n      <th>end</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>B-PER</td>\n      <td>0.999165</td>\n      <td>1</td>\n      <td>Mark</td>\n      <td>0</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>B-PER</td>\n      <td>0.997683</td>\n      <td>3</td>\n      <td>Mary</td>\n      <td>9</td>\n      <td>13</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>B-LOC</td>\n      <td>0.999792</td>\n      <td>6</td>\n      <td>Ukraine</td>\n      <td>22</td>\n      <td>29</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>B-LOC</td>\n      <td>0.999481</td>\n      <td>20</td>\n      <td>K</td>\n      <td>99</td>\n      <td>100</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>I-LOC</td>\n      <td>0.998003</td>\n      <td>21</td>\n      <td>##har</td>\n      <td>100</td>\n      <td>103</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>I-LOC</td>\n      <td>0.996411</td>\n      <td>22</td>\n      <td>##ki</td>\n      <td>103</td>\n      <td>105</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>I-LOC</td>\n      <td>0.994031</td>\n      <td>23</td>\n      <td>##v</td>\n      <td>105</td>\n      <td>106</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>B-LOC</td>\n      <td>0.999337</td>\n      <td>25</td>\n      <td>Ch</td>\n      <td>111</td>\n      <td>113</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>I-LOC</td>\n      <td>0.938038</td>\n      <td>26</td>\n      <td>##ern</td>\n      <td>113</td>\n      <td>116</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>I-LOC</td>\n      <td>0.998129</td>\n      <td>27</td>\n      <td>##ih</td>\n      <td>116</td>\n      <td>118</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>I-LOC</td>\n      <td>0.995637</td>\n      <td>28</td>\n      <td>##iv</td>\n      <td>118</td>\n      <td>120</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# VALIDATION_FIELD[func] model_token_cls\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "sequence = \"Mark and Mary live in Ukraine. They love to travel around their country. Their favorite cities are Kharkiv and Chernihiv.\"\n",
    "\n",
    "### START CODE HERE ### (â‰ˆ 1-3 lines of code)\n",
    "model_token_cls = transformers.pipeline(task='ner', model='dslim/bert-base-NER')\n",
    "### END CODE HERE ###\n",
    "\n",
    "ner_result = model_token_cls(sequence)\n",
    "ner_result_df = pd.DataFrame(ner_result)\n",
    "ner_result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EK5E4STJr1Qb"
   },
   "source": [
    "**Expected output:**\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td></td>\n",
    "        <td>entity</td>\n",
    "        <td>score</td>\n",
    "        <td>index</td>\n",
    "        <td>word</td>\n",
    "        <td>start</td>\n",
    "        <td>end</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>0</td>\n",
    "        <td>B-PER</td>\n",
    "        <td>0.999165</td>\n",
    "        <td>1</td>\n",
    "        <td>Mark</td>\n",
    "        <td>0</td>\n",
    "        <td>4</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>1</td>\n",
    "        <td>B-PER</td>\n",
    "        <td>0.997683</td>\n",
    "        <td>3</td>\n",
    "        <td>Mary</td>\n",
    "        <td>9</td>\n",
    "        <td>13</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>2</td>\n",
    "        <td>B-LOC</td>\n",
    "        <td>0.999792</td>\n",
    "        <td>6</td>\n",
    "        <td>Ukraine</td>\n",
    "        <td>22</td>\n",
    "        <td>29</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>3</td>\n",
    "        <td>B-LOC</td>\n",
    "        <td>0.999481</td>\n",
    "        <td>20</td>\n",
    "        <td>K</td>\n",
    "        <td>99</td>\n",
    "        <td>100</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>4</td>\n",
    "        <td>I-LOC</td>\n",
    "        <td>0.998003</td>\n",
    "        <td>21</td>\n",
    "        <td>\\##har</td>\n",
    "        <td>100</td>\n",
    "        <td>103</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>5</td>\n",
    "        <td>I-LOC</td>\n",
    "        <td>0.996411</td>\n",
    "        <td>22</td>\n",
    "        <td>\\##ki</td>\n",
    "        <td>103</td>\n",
    "        <td>105</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>6</td>\n",
    "        <td>I-LOC</td>\n",
    "        <td>0.994031</td>\n",
    "        <td>23</td>\n",
    "        <td>\\##v</td>\n",
    "        <td>105</td>\n",
    "        <td>106</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>7</td>\n",
    "        <td>B-LOC</td>\n",
    "        <td>0.999337</td>\n",
    "        <td>25</td>\n",
    "        <td>Ch</td>\n",
    "        <td>111</td>\n",
    "        <td>113</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>8</td>\n",
    "        <td>I-LOC</td>\n",
    "        <td>0.938038</td>\n",
    "        <td>26</td>\n",
    "        <td>\\##ern</td>\n",
    "        <td>113</td>\n",
    "        <td>116</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>9</td>\n",
    "        <td>I-LOC</td>\n",
    "        <td>0.998129</td>\n",
    "        <td>27</td>\n",
    "        <td>\\##ih</td>\n",
    "        <td>116</td>\n",
    "        <td>118</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>10</td>\n",
    "        <td>I-LOC</td>\n",
    "        <td>0.995637</td>\n",
    "        <td>28</td>\n",
    "        <td>\\##iv</td>\n",
    "        <td>118</td>\n",
    "        <td>120</td>\n",
    "    </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g_mkqBSIgeha"
   },
   "source": [
    "## **Zero-Shot Classification**\n",
    "\n",
    "The zero-shot classification task is characterized by the lack of labeled examples for the classes of interest. So, you can specify any class names you want to classify for texts.\n",
    "\n",
    "### Inference with Zero-Shot Classification Pipeline\n",
    "\n",
    "A model can be loaded with the `zero-shot-classification` pipeline from the [Hugging Face Hub](https://huggingface.co/models?pipeline_tag=zero-shot-classification&sort=downloads). You can then use found pipeline to classify sequences into any class names you specify.\n",
    "\n",
    "**Excercise:** \n",
    ">Find the described model and do zero-shot classification for the given sequences. The target model is an original transformer encoder-decoder that combines Google's BERT and OpenAI's GPT with a two-layer classification head, finetuned on the MNLI dataset. Architecture: `1024-hidden`, `16-heads` and `12-layers`. When calling the pipeline, pass `multi_label=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "aDaKMc9KcVlQ",
    "ExecuteTime": {
     "end_time": "2024-08-08T12:40:33.364439Z",
     "start_time": "2024-08-08T12:40:21.472968Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sequence: My dog died yesterday\n",
      "Labels and their Scores:\n",
      "\tsadness | 0.961\n",
      "\tpain | 0.886\n",
      "\tlove | 0.0318\n",
      "\tstudies | 0.00189\n",
      "\thappy | 0.000906\n",
      "\tuniversity | 0.000197\n",
      "\teducation | 0.000145\n",
      "\n",
      "Sequence: Tomorrow I will have a very difficult exam\n",
      "Labels and their Scores:\n",
      "\tstudies | 0.923\n",
      "\tpain | 0.89\n",
      "\teducation | 0.766\n",
      "\tuniversity | 0.634\n",
      "\tsadness | 0.0558\n",
      "\thappy | 0.000531\n",
      "\tlove | 0.000432\n"
     ]
    }
   ],
   "source": [
    "# VALIDATION_FIELD[func] model_zsc\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "sequences_to_classify = [\"My dog died yesterday\",\n",
    "                         \"Tomorrow I will have a very difficult exam\"]\n",
    "# you specify any class names \n",
    "candidate_labels = ['education', 'university', 'studies', 'pain', 'sadness', 'happy', 'love']\n",
    "\n",
    "### START CODE HERE ### (â‰ˆ 2-5 lines of code)\n",
    "model_zsc = transformers.pipeline(task='zero-shot-classification', model='facebook/bart-large-mnli', multi_label=True)\n",
    "labels_and_scores = model_zsc(sequences_to_classify, candidate_labels)\n",
    "### END CODE HERE ###\n",
    "\n",
    "for output_dict in labels_and_scores:\n",
    "    print(f\"\\nSequence: {output_dict['sequence']}\\nLabels and their Scores:\")\n",
    "    for labe, scr in zip(output_dict['labels'], output_dict['scores']):\n",
    "        print(\"\\t{} | {:.3}\".format(labe, scr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kZW3dUBkr2Wu"
   },
   "source": [
    "**Expected output:**\n",
    "```\n",
    "Sequence: My dog died yesterday\n",
    "Labels and their Scores:\n",
    "\tsadness | 0.961\n",
    "\tpain | 0.886\n",
    "\tlove | 0.0318\n",
    "\tstudies | 0.00189\n",
    "\thappy | 0.000906\n",
    "\tuniversity | 0.000197\n",
    "\teducation | 0.000145\n",
    "\n",
    "Sequence: Tomorrow I will have a very difficult exam\n",
    "Labels and their Scores:\n",
    "\tstudies | 0.923\n",
    "\tpain | 0.89\n",
    "\teducation | 0.766\n",
    "\tuniversity | 0.634\n",
    "\tsadness | 0.0558\n",
    "\thappy | 0.000531\n",
    "\tlove | 0.000432\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VcyrzWGDsry-"
   },
   "source": [
    "## **Translation**\n",
    "\n",
    "The [translation](https://huggingface.co/tasks/translation) is the task of automatically converting one natural language into another.\n",
    "\n",
    "### Inference with Translation Pipeline\n",
    "You can use a specific model checkpoint in your pipeline with the `translation_xx_to_yy` pattern where `xx` is the source language code, and `yy` is the target language code. [Here](https://huggingface.co/models?pipeline_tag=translation&sort=downloads) you can find already pretrained models for translation tasks.\n",
    "\n",
    "**Excercise:** \n",
    ">Find on the Hugging Face Hub a described model for translating given sentences from English to German (`translation_en_to_de`). The model is Google's encoder-decoder Transformer that uses a text-to-text approach. Architecture: `~220M params`, `12-layers`, `768-hidden`, `3072 feed-forward hidden-state`, `12-heads`. Trained in the English text: the Colossal Clean Crawled Corpus (C4). Set `max_length=40`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "YaP1oHx4o0mU",
    "ExecuteTime": {
     "end_time": "2024-08-08T12:40:44.373019Z",
     "start_time": "2024-08-08T12:40:33.366436Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: I like data science! It is my hobby. I want to improve my knowledge in this sphere.\n",
      "Translated text: Ich mag die Datenwissenschaft, sie ist mein Hobby und ich mÃ¶chte meine Kenntnisse in diesem Bereich verbessern.\n",
      "\n",
      "Original text: Transformers is our natural language processing library and our hub is now open to all Machine Learning models.\n",
      "Translated text: Transformers ist unsere Bibliothek zur Verarbeitung natÃ¼rlicher Sprachen und unser Hub ist jetzt fÃ¼r alle Modelle des Machine Learning offen.\n"
     ]
    }
   ],
   "source": [
    "# VALIDATION_FIELD[func] model_tranlator\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "texts_to_translate = [\"I like data science! It is my hobby. I want to improve my knowledge in this sphere.\",\n",
    "                     \"Transformers is our natural language processing library and our hub is now open to all Machine Learning models.\"]\n",
    "\n",
    "### START CODE HERE ### (â‰ˆ 2-4 lines of code)\n",
    "model_tranlator = transformers.pipeline(task='translation_en_to_de', model='t5-base')\n",
    "tranlated_texts = model_tranlator(texts_to_translate, max_length=40)\n",
    "### END CODE HERE ###4\n",
    "\n",
    "for original_text, tranlated_text in zip(texts_to_translate, tranlated_texts):\n",
    "    print(f\"Original text: {original_text}\\nTranslated text: {tranlated_text['translation_text']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "af3wjb7kr3Tu"
   },
   "source": [
    "**Expected output:**\n",
    "```\n",
    "Original text: I like data science! It is my hobby. I want to improve my knowledge in this sphere.\n",
    "Translated text: Ich mag die Datenwissenschaft, sie ist mein Hobby und ich mÃ¶chte meine Kenntnisse in diesem Bereich verbessern.\n",
    "\n",
    "Original text: Transformers is our natural language processing library and our hub is now open to all Machine Learning models.\n",
    "Translated text: Transformers ist unsere Bibliothek zur Verarbeitung natÃ¼rlicher Sprachen und unser Hub ist jetzt fÃ¼r alle Modelle des Machine Learning offen.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JCV2ehIVBtkE"
   },
   "source": [
    "## **Text generation**\n",
    "[Text generation](https://huggingface.co/tasks/text-generation) is the task of generating texts to appear indistinguishable from human-written texts.\n",
    "\n",
    "### Inference with Text generation Pipeline\n",
    "You can use the Transformers library `text-generatio` pipeline from the [Hugging Face Hub](https://huggingface.co/models?pipeline_tag=text-generation&sort=downloads) to infer with Text Generation models. As input, the model takes an incomplete text and returns multiple outputs which can complete the text.\n",
    "\n",
    "**Excercise:** \n",
    ">Find on the Hub one of the most popular models for the text generation task, which is a GPT-based model. This model has architecture: `6-layers`, `768-hidden`, `12 heads`, and `82M params`. Use the found model to generate the given text using parameters `max_length=60`, `num_return_sequences=5` and `pad_token_id=50256`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "4GUznvHNBz8q",
    "ExecuteTime": {
     "end_time": "2024-08-08T12:40:55.277624Z",
     "start_time": "2024-08-08T12:40:44.391020Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Hello! I learn Deep Learning and I want to be able to use it to create things for my friends, and I've done some work on it so I can see my friends doing something with it and have them write their own tutorials or explain what's inside them and where it comes from.\n",
      "\n",
      "--------------------------------------------------\n",
      "Hello! I learn Deep Learning and I want to join the field of Deep Learning & Prediction (DAD) with you!\"\n",
      "\n",
      "The \"new\" feature is a new feature, known as Deep Convolutional Neural Networks for Deep Learning (DLL).\n",
      "\n",
      "DLL is the current standard\n",
      "--------------------------------------------------\n",
      "Hello! I learn Deep Learning and I want to learn the basics quickly. I am taking classes. I love the idea of learning on my computer.\n",
      "\n",
      "And that is just how to start learning Deep Learning. How do you manage your time so effectively, and what you really need to be learning\n",
      "--------------------------------------------------\n",
      "Hello! I learn Deep Learning and I want to build a web framework for you!\n",
      "\n",
      "So please join me in my new blog so we can build something.\n",
      "\n",
      "So please join me or go to my website and download it! ðŸ™‚\n",
      "\n",
      "And also, join me on Telegram.\n",
      "\n",
      "--------------------------------------------------\n",
      "Hello! I learn Deep Learning and I want to be used to learning it, too! So I made another challenge, but it involved actually reading through the results to figure out if it was useful to you? A problem with some old games? Did he really look that good?\n",
      "\n",
      "Here is\n"
     ]
    }
   ],
   "source": [
    "# VALIDATION_FIELD[func] model_text_generator\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "base_text = \"Hello! I learn Deep Learning and I want to\"\n",
    "\n",
    "### START CODE HERE ### (â‰ˆ 2-6 lines of code)\n",
    "model_text_generator = transformers.pipeline(task='text-generation', model='gpt2')\n",
    "genereted_texts = model_text_generator(base_text, max_length=60, num_return_sequences=5, pad_token_id=50256)\n",
    "### END CODE HERE ###\n",
    "\n",
    "for gen_text in genereted_texts:\n",
    "    print(50 * \"-\")\n",
    "    print(gen_text['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N-IHDWcRr4nJ"
   },
   "source": [
    "**Expected output:**\n",
    "```\n",
    "--------------------------------------------------\n",
    "Hello! I learn Deep Learning and I want to teach you deep learning to create my own unique algorithm. My algorithm is based solely on finding the most perfect solution for your problem. In my post I give you examples for the algorithm. The best way to find and get started: Let me know what\n",
    "--------------------------------------------------\n",
    "Hello! I learn Deep Learning and I want to help you to learn how to do this!\n",
    "--------------------------------------------------\n",
    "Hello! I learn Deep Learning and I want to help you develop this knowledge over time!\n",
    "\n",
    "\n",
    "I want to thank my friend, Eric Dershowitz for allowing me to help me make this possible!\n",
    "Also, thank you for supporting me by supporting me and by helping us.\n",
    "\n",
    "--------------------------------------------------\n",
    "Hello! I learn Deep Learning and I want to help build a new learning pipeline that provides the best opportunities to learn and get started using deep learning algorithms. Deep learning is all about knowing the underlying principles of learning. So for the time being, I don't want to repeat this. Deep learning is\n",
    "--------------------------------------------------\n",
    "Hello! I learn Deep Learning and I want to help to help with this. It is in every student's life that as long as I can have fun learning the language, I will help you. For a class on what language is most like, there are lots of different ways to teach it.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DnoHD6uJ-dpq"
   },
   "source": [
    "## **Text2Text Generation**\n",
    "\n",
    "It is the task for text-to-text generation using seq2seq models.\n",
    "\n",
    "### Inference with Text2Text Generation Pipeline\n",
    "\n",
    "Text2Text generation pipeline can be loaded using the following task identifier: `text2text-generation`. On the [Hugging Face Hub](https://huggingface.co/models?pipeline_tag=text2text-generation&sort=downloads), you can find an already pretrained model and use it in your pipeline.\n",
    "\n",
    "**Excercise:**\n",
    ">Do paraphrase task using described model. The Google research team introduced the model in 2019 as a summarization model using self-supervised objective Gap Sentences Generation to train a transformer encoder-decoder. However, the model we are interested in was finetuned, especially for the paraphrasing task. Set `num_return_sequences=5`.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "zG6-MLaRJnv_",
    "ExecuteTime": {
     "end_time": "2024-08-08T12:43:10.714142Z",
     "start_time": "2024-08-08T12:40:55.279625Z"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Descriptors cannot be created directly.\nIf this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.\nIf you cannot immediately regenerate your protos, some other possible workarounds are:\n 1. Downgrade the protobuf package to 3.20.x or lower.\n 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).\n\nMore information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[13], line 8\u001B[0m\n\u001B[0;32m      5\u001B[0m text_to_paraphrase \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMath helps a lot in everyday life, so you need to know it.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m      7\u001B[0m \u001B[38;5;66;03m### START CODE HERE ### (â‰ˆ 2-4 lines of code)\u001B[39;00m\n\u001B[1;32m----> 8\u001B[0m model_paraphrase \u001B[38;5;241m=\u001B[39m \u001B[43mtransformers\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpipeline\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mtext2text-generation\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mtuner007/pegasus_paraphrase\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m      9\u001B[0m paraphrased_texts \u001B[38;5;241m=\u001B[39m model_paraphrase(text_to_paraphrase, num_return_sequences\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m5\u001B[39m)\n\u001B[0;32m     10\u001B[0m \u001B[38;5;66;03m### END CODE HERE ###\u001B[39;00m\n",
      "File \u001B[1;32m~\\PycharmProjects\\DRU_labs\\.venv\\lib\\site-packages\\transformers\\pipelines\\__init__.py:598\u001B[0m, in \u001B[0;36mpipeline\u001B[1;34m(task, model, config, tokenizer, feature_extractor, framework, revision, use_fast, use_auth_token, model_kwargs, pipeline_class, **kwargs)\u001B[0m\n\u001B[0;32m    595\u001B[0m             tokenizer_identifier \u001B[38;5;241m=\u001B[39m tokenizer\n\u001B[0;32m    596\u001B[0m             tokenizer_kwargs \u001B[38;5;241m=\u001B[39m model_kwargs\n\u001B[1;32m--> 598\u001B[0m         tokenizer \u001B[38;5;241m=\u001B[39m \u001B[43mAutoTokenizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    599\u001B[0m \u001B[43m            \u001B[49m\u001B[43mtokenizer_identifier\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrevision\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrevision\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43muse_fast\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_fast\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m_from_pipeline\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtask\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mtokenizer_kwargs\u001B[49m\n\u001B[0;32m    600\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    602\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m load_feature_extractor:\n\u001B[0;32m    603\u001B[0m     \u001B[38;5;66;03m# Try to infer feature extractor from model or config name (if provided as str)\u001B[39;00m\n\u001B[0;32m    604\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m feature_extractor \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[1;32m~\\PycharmProjects\\DRU_labs\\.venv\\lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:546\u001B[0m, in \u001B[0;36mAutoTokenizer.from_pretrained\u001B[1;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001B[0m\n\u001B[0;32m    544\u001B[0m tokenizer_class_py, tokenizer_class_fast \u001B[38;5;241m=\u001B[39m TOKENIZER_MAPPING[\u001B[38;5;28mtype\u001B[39m(config)]\n\u001B[0;32m    545\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m tokenizer_class_fast \u001B[38;5;129;01mand\u001B[39;00m (use_fast \u001B[38;5;129;01mor\u001B[39;00m tokenizer_class_py \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m--> 546\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtokenizer_class_fast\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpretrained_model_name_or_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    547\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    548\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m tokenizer_class_py \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[1;32m~\\PycharmProjects\\DRU_labs\\.venv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1788\u001B[0m, in \u001B[0;36mPreTrainedTokenizerBase.from_pretrained\u001B[1;34m(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)\u001B[0m\n\u001B[0;32m   1785\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   1786\u001B[0m         logger\u001B[38;5;241m.\u001B[39minfo(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mloading file \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfile_path\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m from cache at \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresolved_vocab_files[file_id]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m-> 1788\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mcls\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_from_pretrained\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1789\u001B[0m \u001B[43m    \u001B[49m\u001B[43mresolved_vocab_files\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1790\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpretrained_model_name_or_path\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1791\u001B[0m \u001B[43m    \u001B[49m\u001B[43minit_configuration\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1792\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43minit_inputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1793\u001B[0m \u001B[43m    \u001B[49m\u001B[43muse_auth_token\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_auth_token\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1794\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcache_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcache_dir\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1795\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1796\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\DRU_labs\\.venv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1923\u001B[0m, in \u001B[0;36mPreTrainedTokenizerBase._from_pretrained\u001B[1;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, use_auth_token, cache_dir, *init_inputs, **kwargs)\u001B[0m\n\u001B[0;32m   1921\u001B[0m \u001B[38;5;66;03m# Instantiate tokenizer.\u001B[39;00m\n\u001B[0;32m   1922\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 1923\u001B[0m     tokenizer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mcls\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43minit_inputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43minit_kwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1924\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mOSError\u001B[39;00m:\n\u001B[0;32m   1925\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mOSError\u001B[39;00m(\n\u001B[0;32m   1926\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUnable to load vocabulary from file. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1927\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPlease check that the provided vocabulary is accessible and not corrupted.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1928\u001B[0m     )\n",
      "File \u001B[1;32m~\\PycharmProjects\\DRU_labs\\.venv\\lib\\site-packages\\transformers\\models\\pegasus\\tokenization_pegasus_fast.py:140\u001B[0m, in \u001B[0;36mPegasusTokenizerFast.__init__\u001B[1;34m(self, vocab_file, tokenizer_file, pad_token, eos_token, unk_token, mask_token, mask_token_sent, additional_special_tokens, offset, **kwargs)\u001B[0m\n\u001B[0;32m    137\u001B[0m     additional_special_tokens \u001B[38;5;241m=\u001B[39m [mask_token_sent] \u001B[38;5;28;01mif\u001B[39;00m mask_token_sent \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m []\n\u001B[0;32m    138\u001B[0m     additional_special_tokens \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m [\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m<unk_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mi\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m>\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m2\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moffset)]\n\u001B[1;32m--> 140\u001B[0m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__init__\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[0;32m    141\u001B[0m \u001B[43m    \u001B[49m\u001B[43mvocab_file\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    142\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtokenizer_file\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtokenizer_file\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    143\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpad_token\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpad_token\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    144\u001B[0m \u001B[43m    \u001B[49m\u001B[43meos_token\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43meos_token\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    145\u001B[0m \u001B[43m    \u001B[49m\u001B[43munk_token\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43munk_token\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    146\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmask_token\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmask_token\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    147\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmask_token_sent\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmask_token_sent\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    148\u001B[0m \u001B[43m    \u001B[49m\u001B[43moffset\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moffset\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    149\u001B[0m \u001B[43m    \u001B[49m\u001B[43madditional_special_tokens\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43madditional_special_tokens\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    150\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    151\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    152\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mvocab_file \u001B[38;5;241m=\u001B[39m vocab_file\n\u001B[0;32m    153\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcan_save_slow_tokenizer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mvocab_file \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "File \u001B[1;32m~\\PycharmProjects\\DRU_labs\\.venv\\lib\\site-packages\\transformers\\tokenization_utils_fast.py:113\u001B[0m, in \u001B[0;36mPreTrainedTokenizerFast.__init__\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m    110\u001B[0m     fast_tokenizer \u001B[38;5;241m=\u001B[39m TokenizerFast\u001B[38;5;241m.\u001B[39mfrom_file(fast_tokenizer_file)\n\u001B[0;32m    111\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m slow_tokenizer \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    112\u001B[0m     \u001B[38;5;66;03m# We need to convert a slow tokenizer to build the backend\u001B[39;00m\n\u001B[1;32m--> 113\u001B[0m     fast_tokenizer \u001B[38;5;241m=\u001B[39m \u001B[43mconvert_slow_tokenizer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mslow_tokenizer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    114\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mslow_tokenizer_class \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    115\u001B[0m     \u001B[38;5;66;03m# We need to create and convert a slow tokenizer to build the backend\u001B[39;00m\n\u001B[0;32m    116\u001B[0m     slow_tokenizer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mslow_tokenizer_class(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\PycharmProjects\\DRU_labs\\.venv\\lib\\site-packages\\transformers\\convert_slow_tokenizer.py:1033\u001B[0m, in \u001B[0;36mconvert_slow_tokenizer\u001B[1;34m(transformer_tokenizer)\u001B[0m\n\u001B[0;32m   1026\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m   1027\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn instance of tokenizer class \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtokenizer_class_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m cannot be converted in a Fast tokenizer instance. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1028\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNo converter was found. Currently available slow->fast convertors: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlist\u001B[39m(SLOW_TO_FAST_CONVERTERS\u001B[38;5;241m.\u001B[39mkeys())\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1029\u001B[0m     )\n\u001B[0;32m   1031\u001B[0m converter_class \u001B[38;5;241m=\u001B[39m SLOW_TO_FAST_CONVERTERS[tokenizer_class_name]\n\u001B[1;32m-> 1033\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mconverter_class\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtransformer_tokenizer\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mconverted()\n",
      "File \u001B[1;32m~\\PycharmProjects\\DRU_labs\\.venv\\lib\\site-packages\\transformers\\convert_slow_tokenizer.py:425\u001B[0m, in \u001B[0;36mSpmConverter.__init__\u001B[1;34m(self, *args)\u001B[0m\n\u001B[0;32m    421\u001B[0m requires_backends(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mprotobuf\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    423\u001B[0m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;241m*\u001B[39margs)\n\u001B[1;32m--> 425\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m sentencepiece_model_pb2 \u001B[38;5;28;01mas\u001B[39;00m model_pb2\n\u001B[0;32m    427\u001B[0m m \u001B[38;5;241m=\u001B[39m model_pb2\u001B[38;5;241m.\u001B[39mModelProto()\n\u001B[0;32m    428\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moriginal_tokenizer\u001B[38;5;241m.\u001B[39mvocab_file, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrb\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m f:\n",
      "File \u001B[1;32m~\\PycharmProjects\\DRU_labs\\.venv\\lib\\site-packages\\transformers\\utils\\sentencepiece_model_pb2.py:52\u001B[0m\n\u001B[0;32m     35\u001B[0m DESCRIPTOR \u001B[38;5;241m=\u001B[39m _descriptor\u001B[38;5;241m.\u001B[39mFileDescriptor(\n\u001B[0;32m     36\u001B[0m     name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msentencepiece_model.proto\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m     37\u001B[0m     package\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msentencepiece\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     41\u001B[0m     ),\n\u001B[0;32m     42\u001B[0m )\n\u001B[0;32m     43\u001B[0m _sym_db\u001B[38;5;241m.\u001B[39mRegisterFileDescriptor(DESCRIPTOR)\n\u001B[0;32m     46\u001B[0m _TRAINERSPEC_MODELTYPE \u001B[38;5;241m=\u001B[39m _descriptor\u001B[38;5;241m.\u001B[39mEnumDescriptor(\n\u001B[0;32m     47\u001B[0m     name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mModelType\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m     48\u001B[0m     full_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msentencepiece.TrainerSpec.ModelType\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m     49\u001B[0m     filename\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m     50\u001B[0m     file\u001B[38;5;241m=\u001B[39mDESCRIPTOR,\n\u001B[0;32m     51\u001B[0m     values\u001B[38;5;241m=\u001B[39m[\n\u001B[1;32m---> 52\u001B[0m         \u001B[43m_descriptor\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mEnumValueDescriptor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mUNIGRAM\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mindex\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnumber\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mtype\u001B[39;49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m,\n\u001B[0;32m     53\u001B[0m         _descriptor\u001B[38;5;241m.\u001B[39mEnumValueDescriptor(name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mBPE\u001B[39m\u001B[38;5;124m\"\u001B[39m, index\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m, number\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m, options\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;28mtype\u001B[39m\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m),\n\u001B[0;32m     54\u001B[0m         _descriptor\u001B[38;5;241m.\u001B[39mEnumValueDescriptor(name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWORD\u001B[39m\u001B[38;5;124m\"\u001B[39m, index\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m, number\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m3\u001B[39m, options\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;28mtype\u001B[39m\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m),\n\u001B[0;32m     55\u001B[0m         _descriptor\u001B[38;5;241m.\u001B[39mEnumValueDescriptor(name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCHAR\u001B[39m\u001B[38;5;124m\"\u001B[39m, index\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m3\u001B[39m, number\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m4\u001B[39m, options\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;28mtype\u001B[39m\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m),\n\u001B[0;32m     56\u001B[0m     ],\n\u001B[0;32m     57\u001B[0m     containing_type\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m     58\u001B[0m     options\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m     59\u001B[0m     serialized_start\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1121\u001B[39m,\n\u001B[0;32m     60\u001B[0m     serialized_end\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1174\u001B[39m,\n\u001B[0;32m     61\u001B[0m )\n\u001B[0;32m     62\u001B[0m _sym_db\u001B[38;5;241m.\u001B[39mRegisterEnumDescriptor(_TRAINERSPEC_MODELTYPE)\n\u001B[0;32m     64\u001B[0m _MODELPROTO_SENTENCEPIECE_TYPE \u001B[38;5;241m=\u001B[39m _descriptor\u001B[38;5;241m.\u001B[39mEnumDescriptor(\n\u001B[0;32m     65\u001B[0m     name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mType\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m     66\u001B[0m     full_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msentencepiece.ModelProto.SentencePiece.Type\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     79\u001B[0m     serialized_end\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1943\u001B[39m,\n\u001B[0;32m     80\u001B[0m )\n",
      "File \u001B[1;32m~\\PycharmProjects\\DRU_labs\\.venv\\lib\\site-packages\\google\\protobuf\\descriptor.py:917\u001B[0m, in \u001B[0;36mEnumValueDescriptor.__new__\u001B[1;34m(cls, name, index, number, type, options, serialized_options, create_key)\u001B[0m\n\u001B[0;32m    914\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__new__\u001B[39m(\u001B[38;5;28mcls\u001B[39m, name, index, number,\n\u001B[0;32m    915\u001B[0m             \u001B[38;5;28mtype\u001B[39m\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m,  \u001B[38;5;66;03m# pylint: disable=redefined-builtin\u001B[39;00m\n\u001B[0;32m    916\u001B[0m             options\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, serialized_options\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, create_key\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m--> 917\u001B[0m   \u001B[43m_message\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mMessage\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_CheckCalledFromGeneratedFile\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    918\u001B[0m   \u001B[38;5;66;03m# There is no way we can build a complete EnumValueDescriptor with the\u001B[39;00m\n\u001B[0;32m    919\u001B[0m   \u001B[38;5;66;03m# given parameters (the name of the Enum is not known, for example).\u001B[39;00m\n\u001B[0;32m    920\u001B[0m   \u001B[38;5;66;03m# Fortunately generated files just pass it to the EnumDescriptor()\u001B[39;00m\n\u001B[0;32m    921\u001B[0m   \u001B[38;5;66;03m# constructor, which will ignore it, so returning None is good enough.\u001B[39;00m\n\u001B[0;32m    922\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "\u001B[1;31mTypeError\u001B[0m: Descriptors cannot be created directly.\nIf this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.\nIf you cannot immediately regenerate your protos, some other possible workarounds are:\n 1. Downgrade the protobuf package to 3.20.x or lower.\n 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).\n\nMore information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates"
     ]
    }
   ],
   "source": [
    "# VALIDATION_FIELD[func] model_paraphrase\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "text_to_paraphrase = \"Math helps a lot in everyday life, so you need to know it.\"\n",
    "\n",
    "### START CODE HERE ### (â‰ˆ 2-4 lines of code)\n",
    "model_paraphrase = transformers.pipeline(task='text2text-generation', model='tuner007/pegasus_paraphrase')\n",
    "paraphrased_texts = model_paraphrase(text_to_paraphrase, num_return_sequences=5)\n",
    "### END CODE HERE ###\n",
    "\n",
    "for text in paraphrased_texts:\n",
    "    print(text['generated_text'] + '\\n' + 50*'-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ra65oCSrr6Bi"
   },
   "source": [
    "**Expected output:**\n",
    "```\n",
    "You need to know that math helps a lot in everyday life.\n",
    "--------------------------------------------------\n",
    "You need to know that math helps in everyday life.\n",
    "--------------------------------------------------\n",
    "You need to know math because it helps a lot in everyday life.\n",
    "--------------------------------------------------\n",
    "It's important to know that math helps in everyday life.\n",
    "--------------------------------------------------\n",
    "You need to know about math because it helps a lot in everyday life.\n",
    "--------------------------------------------------\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "401ltCDdBGKA"
   },
   "source": [
    "## **Conversational**\n",
    "\n",
    "The Conversational task mimics human conversation by recognizing speech and text, understanding intent, and deciphering different languages.\n",
    "\n",
    "### Inference with Conversational Pipeline\n",
    "On the [Hugging Face Hub](https://huggingface.co/models?pipeline_tag=conversational&sort=downloads), you can find a pretrained model to create a bot to communicate with you using the `conversational` pipeline. [Here](https://huggingface.co/docs/transformers/main_classes/pipelines#transformers.Conversation) you can find some examples of using this kind of pipeline.\n",
    "\n",
    "**Excercise:**\n",
    ">Your task is to find the described model and use it in the pipeline to interact with a conversational bot. The model is based on a transformer architecture similar to BERT or Turing-NLG. The training was carried out on the Reddit dataset and fine-tuned with the [BST](https://arxiv.org/abs/2004.08449) dataset. The model architecture: `2-hidden` layers, `32-heads` and `400M` params.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "H71ZqVEJgtV7",
    "ExecuteTime": {
     "end_time": "2024-08-08T12:44:03.930125Z",
     "start_time": "2024-08-08T12:43:28.154053Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversation id: ed533a90-2782-4928-8b3e-f56543d1bafd \n",
      "user >> Hello! Have you any movie recommendations? \n",
      "bot >>  I like action and comedy movies. What kind of movies do you like to watch?  \n",
      "Conversation id: ed533a90-2782-4928-8b3e-f56543d1bafd \n",
      "user >> Hello! Have you any movie recommendations? \n",
      "bot >>  I like action and comedy movies. What kind of movies do you like to watch?  \n",
      "user >> I also like comedies. What is your favorite comedy? \n",
      "bot >>  I don't really have a favourite comedy, but I do like a good drama.  \n"
     ]
    }
   ],
   "source": [
    "# VALIDATION_FIELD[func] model_conv\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "### START CODE HERE ### (â‰ˆ 1-2 lines of code)\n",
    "model_conv = transformers.pipeline(task='conversational', model='facebook/blenderbot-400M-distill')\n",
    "### END CODE HERE ###\n",
    "\n",
    "msgs_for_test_list = [\"Hello! Have you any movie recommendations?\", \"I also like comedies. What is your favorite comedy?\"]\n",
    "\n",
    "def test_bot(msgs_for_test):\n",
    "    for i, msg in enumerate(msgs_for_test_list):\n",
    "        conv_msg = msg\n",
    "        if i==0:\n",
    "            conv = Conversation(conv_msg)\n",
    "            model_conv([conv], pad_token_id=50256)\n",
    "            print(conv)\n",
    "        else:\n",
    "            conv.add_user_input(conv_msg)\n",
    "            print(model_conv([conv], pad_token_id=50256))\n",
    "\n",
    "# function to test the bot by yourself\n",
    "def test_bot_by_yourself():\n",
    "    conv_input = input()\n",
    "    customConv = Conversation(conv_input)\n",
    "    model_conv([customConv], pad_token_id=50256)\n",
    "\n",
    "    while conv_input != \"stop\": # the message you should input to STOP the bot\n",
    "        print(customConv)\n",
    "        conv_input = input()\n",
    "        customConv.add_user_input(conv_input)\n",
    "        model_conv([customConv], pad_token_id=50256)\n",
    "\n",
    "test_bot(msgs_for_test_list)\n",
    "#test_bot_by_yourself() # uncomment if you want to test the bot by yourself"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aHPGp5D1_Wsm"
   },
   "source": [
    "**Expected output:**\n",
    "```\n",
    "Conversation id: 53d4d5f7-bd26-4f9a-929b-775f1ab4ab33 \n",
    "user >> Hello! Have you any movie recommendations? \n",
    "bot >>  I like action and comedy movies. What kind of movies do you like to watch?  \n",
    "\n",
    "Conversation id: 53d4d5f7-bd26-4f9a-929b-775f1ab4ab33 \n",
    "user >> Hello! Have you any movie recommendations? \n",
    "bot >>  I like action and comedy movies. What kind of movies do you like to watch?  \n",
    "user >> I also like comedies. What is your favorite comedy? \n",
    "bot >>  I don't really have a favourite comedy, but I do like a good drama.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l6uDFGrV5gSZ"
   },
   "source": [
    "# **Audio Tasks**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sx_UkBlUV8yl"
   },
   "source": [
    "## **Text to Speech**\n",
    "\n",
    "It is the [task](https://huggingface.co/tasks/text-to-speech) of converting text input into natural human speech. \n",
    "\n",
    "### Inference\n",
    "On the [Hub](https://huggingface.co/models?pipeline_tag=text-to-speech&sort=downloads), you can find over 160 TTS models that you can use right away by trying out the widgets directly in the browser or calling the models as a service using the Accelerated Inference API.\n",
    "\n",
    "We will use [gradio](https://gradio.app/docs/#load) library to handle the Inference directly on the Google Colab notebook. Gradio interface can be created by constructing an `Interface` object with `Interface.load()` with the path to a Hugging Face model repo.\n",
    "\n",
    "**Excercise:** \n",
    ">Find the described model and use it to generate speech by inputting text. It is Facebook's non-autoregressive Transformer-based model, trained on The [LJ Speech Dataset](https://keithito.com/LJ-Speech-Dataset/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "nZcECsZnsFUA",
    "ExecuteTime": {
     "end_time": "2024-08-08T13:08:10.646828Z",
     "start_time": "2024-08-08T13:08:09.506814Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching model from: https://huggingface.co/facebook/fastspeech2-en-ljspeech\n"
     ]
    }
   ],
   "source": [
    "# VALIDATION_FIELD[func] model_tts_de\n",
    "\n",
    "### START CODE HERE ### (â‰ˆ 1-4 lines of code)\n",
    "# construct an Interface\n",
    "iface_tts =  gr.load(\"facebook/fastspeech2-en-ljspeech\", src=\"models\")\n",
    "    \n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "q7xPpx7UsJJt",
    "ExecuteTime": {
     "end_time": "2024-08-08T13:19:24.630111Z",
     "start_time": "2024-08-08T13:08:11.544531Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyboard interruption in main thread... closing server.\n"
     ]
    },
    {
     "data": {
      "text/plain": ""
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# launch the UI for the interface\n",
    "iface_tts.launch(debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vRYGiBzmkL2y"
   },
   "source": [
    "## **Automatic Speech Recognition**\n",
    "\n",
    "[ASR](https://huggingface.co/tasks/automatic-speech-recognition) is the task of transcribing given audio to text.\n",
    "### Inference\n",
    "\n",
    "On the [Hub](https://huggingface.co/models?pipeline_tag=automatic-speech-recognition&sort=downloads), you can find over 1800 ASR models that you can use right away by trying out the widgets directly in the browser or calling the models as a service using the Accelerated Inference API.\n",
    "\n",
    "We will use [gradio](https://gradio.app/docs/#interface) library to handle the Inference directly on the Google Colab notebook. Gradio interface can be created by constructing an `Interface` object with `Interface.from_pipeline()` with a Transformers Pipeline.\n",
    "\n",
    "**Excercise:** \n",
    ">Find the described model and try to use the model to recognize your speech converting into text. It is Facebook's ASR model, trained on the [LibriSpeech](https://huggingface.co/datasets/librispeech_asr) dataset. Model architecture: `512-dim` encoder/decoder input, `12-hidden-layers`, `2-conv-layers`.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "yx93UPNTeUIL",
    "ExecuteTime": {
     "end_time": "2024-08-08T13:19:29.679120Z",
     "start_time": "2024-08-08T13:19:24.632407Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxog\\PycharmProjects\\DRU_labs\\.venv\\lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# VALIDATION_FIELD[func] model_asr_de\n",
    "\n",
    "### START CODE HERE ### (â‰ˆ 2-5 lines of code)\n",
    "model_asr = transformers.pipeline(task='automatic-speech-recognition',\n",
    "                                  model='facebook/wav2vec2-base-960h')  # create pipeline\n",
    "iface_asr = gr.Interface.from_pipeline(model_asr) # construct an Interface\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "ab_maHUPsOuM",
    "ExecuteTime": {
     "end_time": "2024-08-08T13:23:04.442758Z",
     "start_time": "2024-08-08T13:19:29.680273Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyboard interruption in main thread... closing server.\n"
     ]
    },
    {
     "data": {
      "text/plain": ""
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# launch the UI for the interface\n",
    "iface_asr.launch(debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3-zWI77i5lZj"
   },
   "source": [
    "# **Computer Vision Tasks**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vxQ1dwsVkhXv"
   },
   "source": [
    "## **Image Classification**\n",
    "\n",
    "It is the [task](https://huggingface.co/tasks/image-classification) of assigning a label or class for a given image. Models take an image as input and return a prediction about which category the image belongs to.\n",
    "\n",
    "### Inference\n",
    "\n",
    "You can use the `image-classification` pipeline to infer with image classification models, which you can find on the [Hub](https://huggingface.co/models?pipeline_tag=image-classification&sort=downloads). When you call the pipeline, you need to specify an image path or link. We will use [gradio](https://gradio.app/docs/#interface) library to make it easier to handle the Inference directly on the Google Colab notebook. We will use `Interface.from_pipeline()` with a Transformers Pipeline to create the Gradio interface.\n",
    "\n",
    "**Excercise:** \n",
    ">Try to do image classification using the described model pipeline. It is a Microsoft transformer encoder BERT-like model. This model is pretrained and fine-tuned on the ImageNet-22k dataset, consisting of 14 million images and 21k classes at 224x224 pixels. The model has a `768-hidden` size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "VN3WuqhbkgsT",
    "ExecuteTime": {
     "end_time": "2024-08-08T13:23:07.542266Z",
     "start_time": "2024-08-08T13:23:04.443964Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxog\\PycharmProjects\\DRU_labs\\.venv\\lib\\site-packages\\torch\\functional.py:512: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\TensorShape.cpp:3588.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "# VALIDATION_FIELD[func] model_img_cls_de\n",
    "\n",
    "### START CODE HERE ### (â‰ˆ 2-5 lines of code)\n",
    "model_img_cls = transformers.pipeline(task='image-classification', model='microsoft/beit-base-patch16-224-pt22k-ft22k')\n",
    "iface_img_cls = gr.Interface.from_pipeline(model_img_cls, inputs=\"image\", outputs=\"label\")\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "gljDLHIOsg7x",
    "ExecuteTime": {
     "end_time": "2024-08-08T13:23:15.281973Z",
     "start_time": "2024-08-08T13:23:07.543385Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyboard interruption in main thread... closing server.\n"
     ]
    },
    {
     "data": {
      "text/plain": ""
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# launch the UI for the interface\n",
    "iface_img_cls.launch(debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lHWBfF6JsgaD"
   },
   "source": [
    "## **Image Segmentation**\n",
    "\n",
    "It is the [task](https://huggingface.co/tasks/image-segmentation) of dividing an image into segments by creating a pixel-wise mask for each object in the picture. This task has multiple variants, such as instance segmentation, panoptic segmentation, and semantic segmentation.\n",
    "\n",
    "### Inference\n",
    "On the [Hub](https://huggingface.co/models?pipeline_tag=image-segmentation&sort=downloads), you can find an already pretrained model and infer it using the `image-segmentation` pipeline. [Here](https://huggingface.co/docs/transformers/main_classes/pipelines#transformers.ImageSegmentationPipeline) you also can discover segmentation pipeline parameters. For doing image segmentation, you need to install [timm](https://github.com/rwightman/pytorch-image-models).\n",
    "\n",
    "We will not use the gradio library because it doesn't support the `image-segmentation` pipeline type.\n",
    "\n",
    "**Excercise:** \n",
    ">Find the described model to do image segmentation. This model consists of a convolutional (50-layers) backbone followed by an encoder-decoder transformer trained on [COCO 2017](https://cocodataset.org/#download). Set `threshold=0.95` and `mask_threshold=0.7`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "owctmkWnzpeI",
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "!wget https://dru.fra1.digitaloceanspaces.com/DL_pytorch/static/ntbk_images/fruits.jpg -O fruits.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "zg0_dabYtZI4",
    "ExecuteTime": {
     "end_time": "2024-08-08T13:06:17.522715Z",
     "start_time": "2024-08-08T13:05:58.499056Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/11.3k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "30f60bc529324a60bb8c60fb1ad47063"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/164M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6811b848187c41579e810a9e07ded470"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "model.safetensors:   0%|          | 0.00/102M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5b9fb685e5014312940877e5c7300ef2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxog\\PycharmProjects\\DRU_labs\\.venv\\lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\maxog\\.cache\\huggingface\\hub\\models--timm--resnet50.a1_in1k. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/289 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ce51d19c05ed4fc3887c617dbf924230"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# VALIDATION_FIELD[func] model_img_segm_de\n",
    "\n",
    "### START CODE HERE ### (â‰ˆ 2 lines of code)\n",
    "model_img_segm = transformers.pipeline(task='image-segmentation', model='facebook/detr-resnet-50-panoptic')\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FRGu1If6sx8D"
   },
   "outputs": [],
   "source": [
    "def print_img_and_masks(model_output, original_img_path):\n",
    "\n",
    "    img_masks_list = [i['mask'] for i in model_output][:9] #select top 9 results\n",
    "    masks_labels = [i['label'] for i in model_output][:9] #select top 9 results\n",
    "\n",
    "    original_img = Image.open(r\"fruits.jpg\")\n",
    "    plt.figure(figsize = (10,10))\n",
    "    plt.title('Original Image', fontsize=18)\n",
    "    plt.imshow(original_img.resize((600, 400)));\n",
    "\n",
    "    # print segmentation masks\n",
    "    _, axs = plt.subplots(3, 3, figsize=(20, 20)) # 3x3 grid\n",
    "    axs = axs.flatten()\n",
    "    for img, ax, l in zip(img_masks_list, axs, masks_labels):\n",
    "        ax.set_title(f'{l}', fontsize=18)\n",
    "        ax.imshow(img)\n",
    "    plt.show()\n",
    "\n",
    "img_masks_out = model_img_segm(\"fruits.jpg\", threshold=0.95, mask_threshold=0.7)\n",
    "print_img_and_masks(img_masks_out, 'fruits.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "template.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
