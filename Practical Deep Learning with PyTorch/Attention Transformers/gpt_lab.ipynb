{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "collapsed_sections": [
    "OViz29PaVRK8",
    "EdBTHjm8V0FU",
    "r7Y01TjcaExz",
    "D-1rF5q_urXG"
   ]
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# **Introduction in GPT features**\n",
    "\n",
    "**GPU** is recomended for this assignment. `Runtime` -> `Change runtime type` -> `GPU`\n",
    "\n",
    "**Instructions**\n",
    "- Write code in the space indicated with `### START CODE HERE ###`\n",
    "- Do not use loops (for/while) unless instructions explicitly tell you so. Parallelization in Deep Learning is key!\n",
    "- If you get stuck, ask for help in Slack or DM `@DRU Team`\n",
    "\n",
    "**You will learn**\n",
    "- Main features GPT.\n",
    "- How to build generative QA based on the given data and pre-trained GPT\n",
    "- Prepare own dataset and fine-tune GPT model on them."
   ],
   "metadata": {
    "id": "IqpROOs-GJQb"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Add OpenAI key to Config\n",
    "\n",
    "Before you, sign up or log in to your OpenAI account and generate an [API key](https://platform.openai.com/account/api-keys). \n",
    "\n",
    ">**Note that while the first queries are free, there is a specific limit after which payment is required. OpenAI grants an initial budget of $18, more than enough to complete the lab, experiment with the pipeline amd fine-tune model.**\n",
    "\n",
    ">**Please keep your API key for use during the lab review. We don't store your key, so after verifying your work and earning points, you can delete it in the API keys.**"
   ],
   "metadata": {
    "id": "Ja16Sf-OcUPw"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# VALIDATION_FIELD[cls] Config\n",
    "\n",
    "class Config:\n",
    "\n",
    "  # Section 1\n",
    "  davinci_model_cost = 0.02/1000\n",
    "  ada_model_cost = 0.0004/1000\n",
    "\n",
    "  # Section 2\n",
    "  wiki_article_path = 'wiki_pages'\n",
    "  \n",
    "  # names of Wikipedia's articles\n",
    "  article_titles = ['Kyiv', 'History of Kyiv', 'Kyiv Metro', 'Kyiv culture', 'Kyiv Music Fest', 'FC Dynamo Kyiv', 'Igor Sikorsky Kyiv Polytechnic Institute', 'Paton Bridge', 'Saint Sophia Cathedral', 'Transport in Kyiv', 'Kyiv Zoo', 'Kyiv metropolitan area', 'Taras Shevchenko National University of Kyiv', 'Euromaidan', 'Motherland Monument', 'Podil', 'Kyiv TV Tower']\n",
    "\n",
    "  ### START CODE HERE ###\n",
    "  # your OpenAI API token key\n",
    "  openai_api_key = ''\n",
    "  ### END CODE HERE ###\n",
    "\n",
    "  "
   ],
   "metadata": {
    "id": "HyEvRz51vcy5",
    "ExecuteTime": {
     "end_time": "2024-08-07T12:10:10.119685Z",
     "start_time": "2024-08-07T12:10:10.108674Z"
    }
   },
   "execution_count": 23,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Section 1 - Try GPT"
   ],
   "metadata": {
    "id": "Hu31QrbtVqcs"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "[Generative Pre-trained Transformer (GPT)](https://arxiv.org/abs/2005.14165) models by OpenAI have taken the natural language processing (NLP) community by introducing compelling language models. These models can perform various NLP tasks like question answering, textual entailment, text summarisation, Etc. Without any supervised training. These language models need very few to no examples to understand the tasks and perform equivalent to or even better than the state-of-the-art models trained in a supervised fashion.\n",
    "\n",
    "With the recent releases of [GPT-4](https://openai.com/product/gpt-4) and other models, more powerful new versions of OpenAI’s GPT model may take much time before we can exploit their full potential.\n",
    "\n",
    "We propose considering the possibilities of the OpenAI models they offer and how they can be applied."
   ],
   "metadata": {
    "id": "AzQK8Il2JB8I"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "!pip install openai==0.27.4\n",
    "import openai\n",
    "\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ],
   "metadata": {
    "id": "gULX7IPxNtHr",
    "ExecuteTime": {
     "end_time": "2024-08-07T12:10:13.664290Z",
     "start_time": "2024-08-07T12:10:11.788962Z"
    }
   },
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai==0.27.4 in c:\\users\\maxog\\pycharmprojects\\dru_labs\\.venv\\lib\\site-packages (0.27.4)\n",
      "Requirement already satisfied: requests>=2.20 in c:\\users\\maxog\\pycharmprojects\\dru_labs\\.venv\\lib\\site-packages (from openai==0.27.4) (2.31.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\maxog\\pycharmprojects\\dru_labs\\.venv\\lib\\site-packages (from openai==0.27.4) (4.66.4)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\maxog\\pycharmprojects\\dru_labs\\.venv\\lib\\site-packages (from openai==0.27.4) (3.9.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\maxog\\pycharmprojects\\dru_labs\\.venv\\lib\\site-packages (from requests>=2.20->openai==0.27.4) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\maxog\\pycharmprojects\\dru_labs\\.venv\\lib\\site-packages (from requests>=2.20->openai==0.27.4) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\maxog\\pycharmprojects\\dru_labs\\.venv\\lib\\site-packages (from requests>=2.20->openai==0.27.4) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\maxog\\pycharmprojects\\dru_labs\\.venv\\lib\\site-packages (from requests>=2.20->openai==0.27.4) (2024.2.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\maxog\\pycharmprojects\\dru_labs\\.venv\\lib\\site-packages (from aiohttp->openai==0.27.4) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\maxog\\pycharmprojects\\dru_labs\\.venv\\lib\\site-packages (from aiohttp->openai==0.27.4) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\maxog\\pycharmprojects\\dru_labs\\.venv\\lib\\site-packages (from aiohttp->openai==0.27.4) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\maxog\\pycharmprojects\\dru_labs\\.venv\\lib\\site-packages (from aiohttp->openai==0.27.4) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\maxog\\pycharmprojects\\dru_labs\\.venv\\lib\\site-packages (from aiohttp->openai==0.27.4) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in c:\\users\\maxog\\pycharmprojects\\dru_labs\\.venv\\lib\\site-packages (from aiohttp->openai==0.27.4) (4.0.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\maxog\\pycharmprojects\\dru_labs\\.venv\\lib\\site-packages (from tqdm->openai==0.27.4) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Set OpenAI key"
   ],
   "metadata": {
    "id": "EcyAMQWEuvNt"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# VALIDATION_FIELD[str] set_openai_api_key\n",
    "\n",
    "openai.api_key = Config.openai_api_key"
   ],
   "metadata": {
    "id": "Hk9fL-LAt4Ib",
    "ExecuteTime": {
     "end_time": "2024-08-07T12:10:15.055974Z",
     "start_time": "2024-08-07T12:10:15.049737Z"
    }
   },
   "execution_count": 25,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **OpenAI API start**\n",
    "\n",
    "Now, using OpenAI API, we can give a prompt to a ChatGPT. Create a [chat completions](https://platform.openai.com/docs/api-reference/chat/create) example from the [official guide](https://platform.openai.com/docs/guides/chat/introduction)."
   ],
   "metadata": {
    "id": "_HjwjZKROFjP"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "### START CODE HERE ###\n",
    "answer = openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo-0301\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"What is the 2020 World Series?\"},\n",
    "    ]\n",
    "    )\n",
    "### END CODE HERE\n",
    "\n",
    "print(answer)"
   ],
   "metadata": {
    "id": "5gbfwwfyOFHI",
    "ExecuteTime": {
     "end_time": "2024-08-07T12:10:21.615985Z",
     "start_time": "2024-08-07T12:10:21.273643Z"
    }
   },
   "execution_count": 27,
   "outputs": [
    {
     "ename": "AuthenticationError",
     "evalue": "Incorrect API key provided: sk-proj-********************************************TAQJ. You can find your API key at https://platform.openai.com/account/api-keys.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAuthenticationError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[27], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m### START CODE HERE ###\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[43mopenai\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mChatCompletion\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcreate\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m      3\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mgpt-3.5-turbo-0301\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m      4\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmessages\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\n\u001B[0;32m      5\u001B[0m \u001B[43m        \u001B[49m\u001B[43m{\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mrole\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43msystem\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcontent\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mYou are a helpful assistant.\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      6\u001B[0m \u001B[43m        \u001B[49m\u001B[43m{\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mrole\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43muser\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcontent\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mWhat is the 2020 World Series?\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      7\u001B[0m \u001B[43m    \u001B[49m\u001B[43m]\u001B[49m\n\u001B[0;32m      8\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      9\u001B[0m \u001B[38;5;66;03m### END CODE HERE\u001B[39;00m\n\u001B[0;32m     11\u001B[0m \u001B[38;5;28mprint\u001B[39m(answer)\n",
      "File \u001B[1;32m~\\PycharmProjects\\DRU_labs\\.venv\\lib\\site-packages\\openai\\api_resources\\chat_completion.py:25\u001B[0m, in \u001B[0;36mChatCompletion.create\u001B[1;34m(cls, *args, **kwargs)\u001B[0m\n\u001B[0;32m     23\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m     24\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m---> 25\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcreate\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     26\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m TryAgain \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m     27\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m timeout \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m time\u001B[38;5;241m.\u001B[39mtime() \u001B[38;5;241m>\u001B[39m start \u001B[38;5;241m+\u001B[39m timeout:\n",
      "File \u001B[1;32m~\\PycharmProjects\\DRU_labs\\.venv\\lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py:153\u001B[0m, in \u001B[0;36mEngineAPIResource.create\u001B[1;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001B[0m\n\u001B[0;32m    127\u001B[0m \u001B[38;5;129m@classmethod\u001B[39m\n\u001B[0;32m    128\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcreate\u001B[39m(\n\u001B[0;32m    129\u001B[0m     \u001B[38;5;28mcls\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    136\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mparams,\n\u001B[0;32m    137\u001B[0m ):\n\u001B[0;32m    138\u001B[0m     (\n\u001B[0;32m    139\u001B[0m         deployment_id,\n\u001B[0;32m    140\u001B[0m         engine,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    150\u001B[0m         api_key, api_base, api_type, api_version, organization, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mparams\n\u001B[0;32m    151\u001B[0m     )\n\u001B[1;32m--> 153\u001B[0m     response, _, api_key \u001B[38;5;241m=\u001B[39m \u001B[43mrequestor\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    154\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mpost\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    155\u001B[0m \u001B[43m        \u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    156\u001B[0m \u001B[43m        \u001B[49m\u001B[43mparams\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    157\u001B[0m \u001B[43m        \u001B[49m\u001B[43mheaders\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    158\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    159\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrequest_id\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrequest_id\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    160\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrequest_timeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrequest_timeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    161\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    163\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m stream:\n\u001B[0;32m    164\u001B[0m         \u001B[38;5;66;03m# must be an iterator\u001B[39;00m\n\u001B[0;32m    165\u001B[0m         \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(response, OpenAIResponse)\n",
      "File \u001B[1;32m~\\PycharmProjects\\DRU_labs\\.venv\\lib\\site-packages\\openai\\api_requestor.py:226\u001B[0m, in \u001B[0;36mAPIRequestor.request\u001B[1;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001B[0m\n\u001B[0;32m    205\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mrequest\u001B[39m(\n\u001B[0;32m    206\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m    207\u001B[0m     method,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    214\u001B[0m     request_timeout: Optional[Union[\u001B[38;5;28mfloat\u001B[39m, Tuple[\u001B[38;5;28mfloat\u001B[39m, \u001B[38;5;28mfloat\u001B[39m]]] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m    215\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001B[38;5;28mbool\u001B[39m, \u001B[38;5;28mstr\u001B[39m]:\n\u001B[0;32m    216\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrequest_raw(\n\u001B[0;32m    217\u001B[0m         method\u001B[38;5;241m.\u001B[39mlower(),\n\u001B[0;32m    218\u001B[0m         url,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    224\u001B[0m         request_timeout\u001B[38;5;241m=\u001B[39mrequest_timeout,\n\u001B[0;32m    225\u001B[0m     )\n\u001B[1;32m--> 226\u001B[0m     resp, got_stream \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_interpret_response\u001B[49m\u001B[43m(\u001B[49m\u001B[43mresult\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    227\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m resp, got_stream, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mapi_key\n",
      "File \u001B[1;32m~\\PycharmProjects\\DRU_labs\\.venv\\lib\\site-packages\\openai\\api_requestor.py:620\u001B[0m, in \u001B[0;36mAPIRequestor._interpret_response\u001B[1;34m(self, result, stream)\u001B[0m\n\u001B[0;32m    612\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m (\n\u001B[0;32m    613\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_interpret_response_line(\n\u001B[0;32m    614\u001B[0m             line, result\u001B[38;5;241m.\u001B[39mstatus_code, result\u001B[38;5;241m.\u001B[39mheaders, stream\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m    615\u001B[0m         )\n\u001B[0;32m    616\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m line \u001B[38;5;129;01min\u001B[39;00m parse_stream(result\u001B[38;5;241m.\u001B[39miter_lines())\n\u001B[0;32m    617\u001B[0m     ), \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m    618\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    619\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m (\n\u001B[1;32m--> 620\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_interpret_response_line\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    621\u001B[0m \u001B[43m            \u001B[49m\u001B[43mresult\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcontent\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecode\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mutf-8\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    622\u001B[0m \u001B[43m            \u001B[49m\u001B[43mresult\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstatus_code\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    623\u001B[0m \u001B[43m            \u001B[49m\u001B[43mresult\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    624\u001B[0m \u001B[43m            \u001B[49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    625\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m,\n\u001B[0;32m    626\u001B[0m         \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[0;32m    627\u001B[0m     )\n",
      "File \u001B[1;32m~\\PycharmProjects\\DRU_labs\\.venv\\lib\\site-packages\\openai\\api_requestor.py:683\u001B[0m, in \u001B[0;36mAPIRequestor._interpret_response_line\u001B[1;34m(self, rbody, rcode, rheaders, stream)\u001B[0m\n\u001B[0;32m    681\u001B[0m stream_error \u001B[38;5;241m=\u001B[39m stream \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124merror\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m resp\u001B[38;5;241m.\u001B[39mdata\n\u001B[0;32m    682\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m stream_error \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;241m200\u001B[39m \u001B[38;5;241m<\u001B[39m\u001B[38;5;241m=\u001B[39m rcode \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m300\u001B[39m:\n\u001B[1;32m--> 683\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandle_error_response(\n\u001B[0;32m    684\u001B[0m         rbody, rcode, resp\u001B[38;5;241m.\u001B[39mdata, rheaders, stream_error\u001B[38;5;241m=\u001B[39mstream_error\n\u001B[0;32m    685\u001B[0m     )\n\u001B[0;32m    686\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m resp\n",
      "\u001B[1;31mAuthenticationError\u001B[0m: Incorrect API key provided: sk-proj-********************************************TAQJ. You can find your API key at https://platform.openai.com/account/api-keys."
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "**Expected output:**\n",
    "\n",
    "```\n",
    "{\n",
    "  \"choices\": [\n",
    "    {\n",
    "      \"finish_reason\": \"stop\",\n",
    "      \"index\": 0,\n",
    "      \"message\": {\n",
    "        \"content\": \"The 2020 World Series was played at a neutral site, Globe Life Field, in Arlington, Texas.\",\n",
    "        \"role\": \"assistant\"\n",
    "      }\n",
    "    }\n",
    "  ],\n",
    "  \"created\": **********,\n",
    "  \"id\": \"chatcmpl-***\",\n",
    "  \"model\": \"gpt-3.5-turbo-0301\",\n",
    "  \"object\": \"chat.completion\",\n",
    "  \"usage\": {\n",
    "    \"completion_tokens\": 22,\n",
    "    \"prompt_tokens\": 57,\n",
    "    \"total_tokens\": 79\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "> Note: The model`s answer can be different but have the same meaning\n"
   ],
   "metadata": {
    "id": "bgAkJ1UNge9t"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We got the result with all the query parameters and data. The assistant’s reply in `['choices'][0]['message']['content']`.\n",
    "\n",
    "`finish_reason` is an end status model output. The possible values are:\n",
    "* `stop`: API returned complete model output\n",
    "* `length`: Incomplete model output due to `max_tokens` parameter or token limit\n",
    "* `content_filter`: Omitted content due to a flag from our content filters\n",
    "* `null`: API response still in progress or incomplete\n",
    "\n",
    "`usage` needs to see how many tokens an API uses. This parameter is important to calculate the costs of your request. We will talk about pricing later. "
   ],
   "metadata": {
    "id": "RerFvfi0QisC"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **Models usage examples**\n",
    "\n",
    "`gpt-3.5-turbo` is the current ChatGPT model. This model is accessible on the [web interface](https://chat.openai.com). Your start budget will be enough to end this lab and more.\n",
    "\n",
    "> Note: You can try the `GPT-4` model if you join [waitlist](https://openai.com/waitlist/gpt-4-api) and get access. However, for lab models, free access will be enough."
   ],
   "metadata": {
    "id": "CEI6490YevUE"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Using [Completions](https://platform.openai.com/docs/api-reference/completions), you can try the most available powerful model - `text-davinci-003`. Let`s ask something about Kyiv.\n",
    "\n",
    "Create a Completion request function with the parameters:\n",
    "- `max_tokens = 30`\n",
    "- `temperature = 0`\n",
    "- `model = \"text-davinci-003\"`\n",
    "- `promp is a function argument`"
   ],
   "metadata": {
    "id": "VaYTHyDdUIEm"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# VALIDATION_FIELD[func] gpt_completion\n",
    "\n",
    "### START CODE HERE ###\n",
    "def gpt_completion(prompt):\n",
    "  answer = ...\n",
    "  return answer\n",
    "### END CODE HERE ###"
   ],
   "metadata": {
    "id": "GvFwabekCtRQ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Add prompt `\"What is the height of the Kyiv TV tower in metres?\"`"
   ],
   "metadata": {
    "id": "qJuqsCuPDEH0"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# VALIDATION_FIELD[str] kyiv_prompt\n",
    "\n",
    "### START CODE HERE ###\n",
    "kyiv_prompt = ...\n",
    "### END CODE HERE ###"
   ],
   "metadata": {
    "id": "wGD2RKY2DSsq"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Show GPT's answer"
   ],
   "metadata": {
    "id": "_TB3IfxmDjj4"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "answer = gpt_completion(kyiv_prompt)\n",
    "\n",
    "print(answer['choices'][0]['text'])"
   ],
   "metadata": {
    "id": "tusCUCAEQhEg"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Expected output:**\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <td>Result:</td>\n",
    "    <td> 385 meters</td> \n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "> Note: The model`s answer can be different but have the same meaning"
   ],
   "metadata": {
    "id": "Z8tSPCWaV5Ad"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The correct answer has likely been obtained. Finally, you can check on the page in [Wikipedia](https://en.wikipedia.org/wiki/Kyiv_TV_Tower).\n",
    "\n",
    "You have tried to ask a simple question that does not require analytical work. It is like a simple Google query. Let us try some math and logical questions.\n",
    "\n",
    "* Create prompt to calculate `8 * 6 + 6`."
   ],
   "metadata": {
    "id": "ckSrAqO7XTbX"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# VALIDATION_FIELD[str] math_prompt\n",
    "\n",
    "### START CODE HERE ###\n",
    "math_prompt = ...\n",
    "### END CODE HERE ###"
   ],
   "metadata": {
    "id": "yNjaCWWLCkmp"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "answer = gpt_completion(math_prompt)\n",
    "\n",
    "print(answer['choices'][0]['text'])"
   ],
   "metadata": {
    "id": "J4MomfBAXShf"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Expected output:**\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <td>Answer:</td>\n",
    "    <td> 54</td> \n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "> Note: The model`s answer can be different but have the same meaning"
   ],
   "metadata": {
    "id": "2taY6-JSbMaY"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "GPT was able to consider and solve a simple arithmetic example. Let us use the GPT to solve the logic puzzle.\n",
    "\n",
    "* Create prompt to solve this exercise: `Maks have five apples. Mask give two apples to Maria, after what mother give one apple more to Maks. How many apples have Maks now??`"
   ],
   "metadata": {
    "id": "aVTc71lXbeDB"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# VALIDATION_FIELD[str] logic_prompt\n",
    "\n",
    "### START CODE HERE ###\n",
    "logic_prompt = ...\n",
    "### END CODE HERE ###"
   ],
   "metadata": {
    "id": "OkBKbq0yanFP"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "answer = gpt_completion(logic_prompt)\n",
    "\n",
    "print(answer['choices'][0]['text'])"
   ],
   "metadata": {
    "id": "Rag_keZqIY_7"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Expected output:**\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <td>Answer:</td>\n",
    "    <td> Maks has 4 apples now.</td> \n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "> Note: The model`s answer can be different but have the same meaning"
   ],
   "metadata": {
    "id": "9WjLnffab03v"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **Models mistakes**\n",
    "\n",
    "davinci-003 performed our tasks well. You could notice that the results were not immediately correct, and some prompt changes are necessary before getting the correct answer.\n",
    "\n",
    "GPT is imperfect, so let us look at this issue in more detail.\n",
    "\n",
    "For example, recent events. davinci-003 is `up to June 2021` trained model, so it cannot know who is a Monarch of the United Kingdom now.\n",
    "\n",
    "* Create prompt `Who is a monarch of the United Kingdom?`"
   ],
   "metadata": {
    "id": "EmEZkVjFcWE3"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# VALIDATION_FIELD[str] monarch_UK_prompt\n",
    "\n",
    "### START CODE HERE ###\n",
    "monarch_UK_prompt = ...\n",
    "### END CODE HERE ###"
   ],
   "metadata": {
    "id": "mZnjZe-IIlQa"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "answer = gpt_completion(monarch_UK_prompt)\n",
    "\n",
    "print(answer['choices'][0]['text'])"
   ],
   "metadata": {
    "id": "RjEoYt63am5x"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Expected output:**\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <td>Answer:</td>\n",
    "    <td> Queen Elizabeth II</td> \n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "> Note: The model`s answer can be different but have the same meaning"
   ],
   "metadata": {
    "id": "rEkQMNUbfWzu"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We got the wrong answer, and you can check it on [Wikipedia page](https://en.wikipedia.org/wiki/Monarchy_of_the_United_Kingdom) (Charles III has been the monarch since 8 September 2022).\n",
    "\n",
    "Let's test the mathematical abilities of the model and try a more complex exercise.\n",
    "\n",
    "* Use your previous math prompt to solve `sqrt(1213*4345)`"
   ],
   "metadata": {
    "id": "YYX60HcYnE8V"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# VALIDATION_FIELD[str] harder_math_prompt\n",
    "\n",
    "### START CODE HERE ###\n",
    "harder_math_prompt = ...\n",
    "### END CODE HERE ###"
   ],
   "metadata": {
    "id": "5j1RP4lqJDUO"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "answer = gpt_completion(harder_math_prompt)\n",
    "\n",
    "print(answer['choices'][0]['text'])"
   ],
   "metadata": {
    "id": "mrSy2RmQfSl1"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Expected output:**\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <td>Answer:</td>\n",
    "    <td> Anything except 2295.75</td> \n",
    "  </tr>\n",
    "</table>"
   ],
   "metadata": {
    "id": "FyQsj2MWjBmB"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "GPT models are a powerful tool. As we have seen, he can also make mistakes and answer incorrectly to complex and simple questions (try to solve the simple exercise `8*6 + 6*8`).\n"
   ],
   "metadata": {
    "id": "ogdUFSoljVTv"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **Generate haiku**\n",
    "\n",
    "\n",
    "However, models can understand what you ask, act according to the limits of the user's words, and even come up with something of your own.\n",
    "\n",
    "Let us generate a [Haiku](https://www.britannica.com/art/haiku) about ChatGPT using the `text-ada-001` model as the fastest text model.\n",
    "\n",
    "Create a Completion request with next parameters:\n",
    "- `max_tokens = 120`\n",
    "- `temperature = 0.3`\n",
    "- `model = \"text-ada-001\"`\n",
    "- `\"Tell me a Haiku about ChatGPT\"`"
   ],
   "metadata": {
    "id": "ykw9ROlvt-ov"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "### START CODE HERE ###\n",
    "answer_ada = ...\n",
    "### END CODE HERE ###\n",
    "\n",
    "print(answer_ada['choices'][0]['text'])"
   ],
   "metadata": {
    "id": "0fi1NElgq-vt"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Not very well. Let's compare with `text-davinci-003` model as the bigger model.\n",
    "\n",
    "Create a Completion request with next parameters:\n",
    "- `max_tokens = 120`\n",
    "- `temperature = 0.3`\n",
    "- `model = \"text-davinci-003\"`\n",
    "- `\"Tell me a Haiku about ChatGPT\"`"
   ],
   "metadata": {
    "id": "eNBsdFVZrMCk"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "### START CODE HERE ###\n",
    "answer_davinci = ...\n",
    "### END CODE HERE ###\n",
    "\n",
    "print(answer_davinci['choices'][0]['text'])"
   ],
   "metadata": {
    "id": "OMeBOd0nrLfs"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we can compare both answers"
   ],
   "metadata": {
    "id": "g-HCIKgrlX4I"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "print('Davinci model haiku:')\n",
    "print(answer_davinci['choices'][0]['text'])\n",
    "print('------------------------------------')\n",
    "print('Ada model haiku:')\n",
    "print(answer_ada['choices'][0]['text'])\n",
    "print('------------------------------------')\n",
    "print('Davinci cost: %.8f $' % (Config.davinci_model_cost * answer_davinci['usage']['total_tokens']))\n",
    "print('Ada cost: %.8f $' % (Config.ada_model_cost * answer_ada['usage']['total_tokens']))"
   ],
   "metadata": {
    "id": "mT52802WlXLz"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "As you can see, the Davinci model creates more creative and better haiku than Ada. However, using Davinchi is more expensive than Ada, following this table:\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <td>Davinchi:</td>\n",
    "    <td> $0.0200 / 1K tokens</td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Ada:</td>\n",
    "    <td> $0.0004 / 1K tokens</td> \n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "We need to pay more for better results. You can learn more about OpenAI pricing on the [official website](https://openai.com/pricing)."
   ],
   "metadata": {
    "id": "Ais8zaHHrhTe"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **Creative solution**\n",
    "\n",
    "GPT is a powerful multitasking model that can solve many different tasks. You can ask any question, how who is the Great Britan monarch or information about a favorite character, and solve math and logic tasks. However, the capabilities of the models are wider than this.\n",
    "\n",
    "Using GPT, we can solve more complex tasks, and let's try to do something."
   ],
   "metadata": {
    "id": "Bwv9IpFo7Qfy"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### **GraphGPT**\n",
    "\n",
    "The main idea is to create a prompt that will allow us to convert unstructured natural language into a knowledge graph.\n",
    "\n",
    "To do this, we need to create a prompt from our task and provide the GPT recording formats and an example of execution.\n",
    "\n",
    "\n",
    "You need fill `//-- add your text here --//` in the following prompt:\n",
    "> * Describe the task of finding as many connections as possible and writing them down in a list.\n",
    "* Note that the single format `[ENTITY 1, RELATIONSHIP, ENTITY 2]` should be used, relations are directed, and the order is important.\n",
    "* For testing, use `Maks, Petro and Vlad are colleagues.`"
   ],
   "metadata": {
    "id": "OS3KwsXsdZiT"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "### START CODE HERE ###\n",
    "new_prompt = \"\"\"Given a prompt, //-- add your text here --//.\n",
    "\n",
    "If an update is a relationship, //-- add your text here --//.\n",
    "\n",
    "Example:\n",
    "prompt: Alice is Bob's roommate.\n",
    "updates:\n",
    "[[\"Alice\", \"roommate\", \"Bob\"]]\n",
    "\n",
    "prompt: //-- add your text here --//\n",
    "updates:\n",
    "\"\"\"\n",
    "### END CODE HERE ###"
   ],
   "metadata": {
    "id": "KN2F7ZD2Te3r"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "answer = openai.Completion.create(\n",
    "  model='text-davinci-003',\n",
    "  prompt=new_prompt,\n",
    "  max_tokens=100,\n",
    "  temperature=0.3\n",
    ")\n",
    "\n",
    "print(answer['choices'][0]['text'])"
   ],
   "metadata": {
    "id": "HsvuRxJeZp9K"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "<table>\n",
    "  <tr>\n",
    "    <td> '[[\"Maks\", \"colleague\", \"Petro\"], [\"Maks\", \"colleague\", \"Vlad\"], [\"Petro\", \"colleague\", \"Maks\"], [\"Petro\", \"colleague\", \"Vlad\"], [\"Vlad\", \"colleague\", \"Maks\"], [\"Vlad\", \"colleague\", \"Petro\"]]'</td> \n",
    "  </tr>\n",
    "</table>\n",
    "\n"
   ],
   "metadata": {
    "id": "68GhQbGNiywE"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create functions using [networkx](https://networkx.org/documentation/stable/reference/introduction.html) for visualizing the obtained connections:"
   ],
   "metadata": {
    "id": "Q9fYjuWXibKo"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def create_relationships_dataframe(answer_data):\n",
    "  relationships = json.loads(answer_data)\n",
    "  from_lable = [relationship[0] for relationship in relationships]\n",
    "  weight_lable = [relationship[1] for relationship in relationships]\n",
    "  to_lable = [relationship[2] for relationship in relationships]\n",
    "  df_relationships = pd.DataFrame({'from':from_lable, 'to':to_lable, 'weight':weight_lable})\n",
    "  return df_relationships\n",
    "\n",
    "def get_connection_relationship_weight(edge, df_relation):\n",
    "  name = df_relation.loc[(df_relation['from']==edge[0]) & (df_relation['to']==edge[1])]['weight'].iloc[0]\n",
    "  return name\n",
    "\n",
    "def show_graph(answer):\n",
    "  answer_relationship = answer['choices'][0]['text']\n",
    "  df_relationships = create_relationships_dataframe(answer_relationship)\n",
    "  G = nx.from_pandas_edgelist(df_relationships, 'from', 'to', create_using=nx.DiGraph())\n",
    "  pos = nx.spring_layout(G)\n",
    "  nx.draw(G, pos, with_labels=True, node_size=800, node_color=\"lightblue\")\n",
    "  labels = {e: get_connection_relationship_weight(e, df_relationships) for e in G.edges}\n",
    "  nx.draw_networkx_edge_labels(G, pos, edge_labels=labels)\n",
    "  plt.show()"
   ],
   "metadata": {
    "id": "2JfStcBXfiy9"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now you can visualize your graph."
   ],
   "metadata": {
    "id": "c5EYe0EwipZy"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "show_graph(answer)"
   ],
   "metadata": {
    "id": "PorvCsy4Vp1d"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "The graph you obtained show the relationship between Peter, Vlad and Max, as specified in your prompt. All of them are colleagues and related to each other.\n",
    "\n",
    "To make sure that the prompt you created works correctly, let us consider a more complex condition by adding external relations\n",
    "\n",
    "\n",
    "You need fill `//-- add your text here --//` in the following prompt:\n",
    "> * Paste your description and formatting.\n",
    "* Generate next relationships graph: `Markus, Mario, Clara and Mykyta are friends. Maria is Clara teacher and Markus mother and Sam lives with Mario.`"
   ],
   "metadata": {
    "id": "u_qNE3s0liPR"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# VALIDATION_FIELD[str] graph_prompt\n",
    "\n",
    "### START CODE HERE ###\n",
    "graph_prompt = \"\"\"Given a prompt, //-- add your text here --//.\n",
    "\n",
    "If an update is a relationship, //-- add your text here --//.\n",
    "\n",
    "Example:\n",
    "prompt: Alice is Bob's roommate.\n",
    "updates:\n",
    "[[\"Alice\", \"roommate\", \"Bob\"]]\n",
    "\n",
    "prompt: //-- add your text here --//\n",
    "updates:\n",
    "\"\"\"\n",
    "### END CODE HERE ###"
   ],
   "metadata": {
    "id": "iRUm4NDeTkSk"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "answer = openai.Completion.create(\n",
    "  model='text-davinci-003',\n",
    "  prompt=graph_prompt,\n",
    "  max_tokens=100,\n",
    "  temperature=0.3,\n",
    "  stop = '.\\n'\n",
    ")\n",
    "\n",
    "print(answer['choices'][0]['text'])\n",
    "show_graph(answer)"
   ],
   "metadata": {
    "id": "7Dt7lVWQj2m3"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Make sure that your graph is right and `graph_prompt` is your final version. All nodes should  have a correct connections (not always right dirrection).\n",
    "\n"
   ],
   "metadata": {
    "id": "K84dNMOHpMFn"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **Section 2 - Search Engine with GPT-3**\n",
    "\n",
    "[Haystack](https://haystack.deepset.ai) is an open-source framework for building search systems that work intelligently over large document collections. Besides providing a comfortable entry point to the [OpenAI API](https://openai.com/product), Haystack offers all the other components we need to successfully implement an end-to-end NLP system with GPT: a vector database, a module for retrieval, and the pipeline that combines all those elements into one queryable system.\n",
    "In this lab, we’ll demonstrate how to build a generative question-answering system that uses the GPT-3 `davinci-003` model to present results in convincing natural language.\n"
   ],
   "metadata": {
    "id": "bAYfWMbX4-XB"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tXn7yIQ5ILeF"
   },
   "outputs": [],
   "source": [
    "!pip install openai==0.27.4\n",
    "!pip install farm-haystack[colab]==1.15.1\n",
    "!pip install Wikipedia-API==0.5.8\n",
    "!pip install faiss-cpu==1.7.2\n",
    "!pip install farm-haystack[faiss]==1.15.1\n",
    "!pip install wget"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import wikipediaapi\n",
    "import json\n",
    "import os\n",
    "import shutil\n",
    "from haystack.utils import convert_files_to_docs, clean_wiki_text\n",
    "from haystack.nodes import PreProcessor\n",
    "from haystack.document_stores import FAISSDocumentStore\n",
    "from haystack.nodes import EmbeddingRetriever\n",
    "from haystack.nodes import OpenAIAnswerGenerator\n",
    "from haystack.pipelines import GenerativeQAPipeline\n",
    "from haystack.utils import print_answers"
   ],
   "metadata": {
    "id": "9HbbOjQ_kVPT"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **Build a Search Engine with GPT-3**\n",
    "\n",
    "If you’ve been online lately, you’ve likely seen the excitement about OpenAI’s newest language model, ChatGPT. ChatGPT is astonishingly good at many things, including debugging code and rewriting text in whatever style you ask it. As an offshoot of GPT-3.5, a large language model (LLM) with billions of parameters, ChatGPT owes its impressive knowledge to the fact that it’s seen a large portion of the internet during training — in the form of the Common Crawl corpus and other data.\n",
    "\n",
    "Chatbots are understandable that people are excited by a language model that can hold a conversation and create a solid semblance of intelligence. But we need to stay critical when it comes to the validity of answers generated by these models. LLMs especially are prone to hallucinations: producing text that sounds sensible at first but doesn’t hold up to closer scrutiny and presenting things as facts that are made up entirely.\n",
    "Semantic search engines — our specialty at deepset — are often powered by extractive question-answering models. These models return verbatim snippets from the knowledge base rather than generating text from scratch the way ChatGPT does.\n",
    "\n",
    "[Haystack](https://docs.haystack.deepset.ai/docs/intro), deepset’s open-source framework for applied natural language processing (NLP), allows you to leverage multiple GPT models in your pipeline. With this approach, you can build a GPT-powered semantic search engine that uses your data as ground truth and bases its natural-language answers on the information it contains. Besides providing a comfortable entry point to the OpenAI API, Haystack offers all the other components you need to successfully implement an end-to-end NLP system with GPT: a vector database, a module for retrieval, and the pipeline that combines all those elements into one queryable system."
   ],
   "metadata": {
    "id": "OViz29PaVRK8"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **The advent of large language models**\n",
    "\n",
    "But while the largest BERT model has 336 million parameters, OpenAI’s largest GPT-3.5 model — which ChatGPT is based on — has 520 times as many.\n",
    "\n",
    "From observation, we can say that GPT is exceptionally good at understanding implication and intent. It can remember what’s been discussed earlier in the conversation, including figuring out what you’re referring to with words like “he” or “before that,” It can tell you when your question doesn’t make sense. All of these properties account for the increased importance of actual intelligence. It also has to generate language from scratch, a much more challenging task than returning the correct section from a corpus."
   ],
   "metadata": {
    "id": "EdBTHjm8V0FU"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **Different types of search engines**\n",
    "\n",
    "Semantic search engines come in different varieties and can roughly be distinguished by the type of answer they return. The answers could consist of matching documents (in document search), answer spans (in extractive QA), or newly generated answers (in generative QA)."
   ],
   "metadata": {
    "id": "r7Y01TjcaExz"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **The GenerativeQAPipeline: Haystack’s component for a generative search engine**\n",
    "\n",
    "In this lab, we use the GenerativeQAPipeline. It consists of a retriever (to find relevant documents) and a generator (to write the text) chained together. The retriever connects to the database. Like the generator, it is often (but not necessarily) based on a Transformer model. Its task is to retrieve the documents from the database that are most likely to contain valuable information based on a user’s input query."
   ],
   "metadata": {
    "id": "_wlutQhXahMM"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Download dataset:"
   ],
   "metadata": {
    "id": "kNP2VhwFlJ2g"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J40qKQEwJw2U"
   },
   "outputs": [],
   "source": [
    "# VALIDATION_FIELD[str] download_kyiv_dataset\n",
    "import wget\n",
    "import zipfile\n",
    "\n",
    "wget.download('https://dru.fra1.digitaloceanspaces.com/DL_pytorch/datasets/07_attention_transformers/GPT-3/wikipedia_articles.zip')\n",
    "with zipfile.ZipFile(\"wikipedia_articles.zip\",\"r\") as zip_ref:\n",
    "    zip_ref.extractall(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "If you want to try your own dataset, set your Wikipedia article values in `Config.article_titles`.\n",
    "\n",
    "Use the code below to create a new dataset with [Wikipedia API](https://github.com/martin-majlis/Wikipedia-API)."
   ],
   "metadata": {
    "id": "U3AW_jmwxCAx"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "'''\n",
    "# you can use this function to create your own dataset\n",
    "# add your Wikipedia articles to Config.article_titles\n",
    "\n",
    "def create_own_dataset(wiki_articles_path, article_titles):\n",
    "  wiki_files_path = wiki_articles_path\n",
    "  if os.path.exists(wiki_files_path):\n",
    "    shutil.rmtree(wiki_files_path)\n",
    "  os.mkdir(wiki_files_path)\n",
    "  wiki_wiki = wikipediaapi.Wikipedia('en')\n",
    "  docs = []\n",
    "  for title in article_titles:\n",
    "    page = wiki_wiki.page(title)\n",
    "    if page.exists():\n",
    "      content = page.text\n",
    "      doc = {'content': content, 'meta': {'title': title}}\n",
    "      docs.append(doc)\n",
    "      with open(f'{wiki_files_path}/{title}.txt', 'w') as f:\n",
    "        f.write(doc['content'])\n",
    "\n",
    "create_own_dataset(Config.wiki_article_path, Config.article_titles)\n",
    "\n",
    "'''"
   ],
   "metadata": {
    "id": "pHdntEBWwqrB"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f_pYuT63M2qv"
   },
   "source": [
    "#### **Converting and preprocessing**\n",
    "\n",
    "Before setting up the pipeline, you need to preprocess your data and add them to the document store or database. This lab uses [FAISS](https://faiss.ai), which is a vector database.\n",
    "\n",
    "The [`DocumentStore`](https://docs.haystack.deepset.ai/docs/document_store) expects data to be supplied as a Haystack data type called `Document` — a dictionary data type that stores information as a set of related fields.\n",
    "\n",
    "Use [convert_files_to_docs](https://docs.haystack.deepset.ai/reference/utils-api#convert_files_to_docs) function with arguments:\n",
    "* `dir_path = wiki_files_path`\n",
    "* `clean_func = clean_wiki_text`\n",
    "* `split_paragraphs = True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vPdG1QpCL53R"
   },
   "outputs": [],
   "source": [
    "# VALIDATION_FIELD[str] docs\n",
    "\n",
    "### START CODE HERE ###\n",
    "wiki_files_path = Config.wiki_article_path\n",
    "docs = ...\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Many documents, including Wikipedia articles about popular topics, can be long. It would be best to ensure that the documents in your database are short enough for the embedding model to capture their meaning adequately.\n",
    "\n",
    ">To do this, use the [PreProcessor](https://docs.haystack.deepset.ai/docs/preprocessor) to split them into shorter text snippets. We suggest a split `length` of **100** tokens per snippet, and an `overlap` of three tokens, to make sure no information gets lost, split by `\"word\"`. Clean `empty lines` and `whitespace`, but true for `header footer` and `respect sentence boundary`:"
   ],
   "metadata": {
    "id": "jY1ioxkplfNu"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MQYbx4-UHSQ0"
   },
   "outputs": [],
   "source": [
    "# VALIDATION_FIELD[str] PreProcessor\n",
    "\n",
    "### START CODE HERE ###\n",
    "preprocessor = ...\n",
    "### END CODE HERE ###\n",
    "processed_docs = preprocessor.process(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Every document has been turned into an object of the `Document` class. This dictionary contains the document’s text and some automatically generated metadata, like which file the text came from\n",
    "\n",
    "\n",
    "What do these processed documents look like? Let’s have a look at one of them:"
   ],
   "metadata": {
    "id": "wAc26UbRliPN"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "phk8KJvpM8fR"
   },
   "outputs": [],
   "source": [
    "processed_docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### **Initializing the DocumentStore**\n",
    "\n",
    "Time to set up the document store — for example, the vector-optimized FAISS database. When you initialize the document store, you need to know the length of your retriever’s document vector embeddings — the internal representations it will produce for each document. Since you’ll be working with the high-dimensional `text-embedding-ada-002` model from OpenAI, you need to set the vectors `embedding_dim` to **1536**.\n",
    "\n",
    ">Use [FAISSDocumentStore](https://docs.haystack.deepset.ai/docs/document_store) with `faiss_index_factory_str = \"Flat\"`:"
   ],
   "metadata": {
    "id": "QRJgXmT-lpYF"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w-Q2sqpLNQbm"
   },
   "outputs": [],
   "source": [
    "# VALIDATION_FIELD[str] FAISSDocumentStore\n",
    "\n",
    "### START CODE HERE ###\n",
    "if os.path.exists(\"faiss_document_store.db\"):\n",
    "  os.remove(\"faiss_document_store.db\")\n",
    "document_store = ...\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, delete any existing documents in the database, and add the preprocessed documents you generated earlier:"
   ],
   "metadata": {
    "id": "NmgZ1wOelvca"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vWXEYQSnNtdB"
   },
   "outputs": [],
   "source": [
    "# VALIDATION_FIELD[cls] write_document\n",
    "\n",
    "document_store.delete_documents()\n",
    "document_store.write_documents(processed_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    ">Note that so far, the database only contains plain-text documents. To add the high-dimensional vector embeddings — the representations of each document that make sense to the language model and that it can use for semantic search — you need to set up the model for retrieval.\n",
    "\n"
   ],
   "metadata": {
    "id": "srTfjUNbl1aM"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### **Retriever**\n",
    "\n",
    "The retriever is the module that matches your query to the documents in the database and retrieves those it deems most likely to contain the answer. Retrievers can be keyword-based (like tf-idf and BM25) or encode semantic similarity using Transformer-generated text vectors. In the latter case, the retriever is also used to index the documents in your database , turning them into high-dimensional embeddings that the retriever can search.\n",
    "\n",
    "You’ll be working with OpenAI’s most recent retrieval model, `text-embedding-ada-002`. To initialize it in Haystack, you need to provide your `OpenAI API key` and use [EmbedingRetriever](https://docs.haystack.deepset.ai/reference/retriever-api#embeddingretriever) with `batch size` **32**, `the longest length of each document sequence` **2048** for your `document store`:"
   ],
   "metadata": {
    "id": "RuC7doAfmK_i"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "exoKF-o_Owi6"
   },
   "outputs": [],
   "source": [
    "# VALIDATION_FIELD[str] EmbeddingRetriever\n",
    "\n",
    "### START CODE HERE ###\n",
    "retriever = ...\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "When you set up the retriever, you connect it directly to your document store. Now you can use the update_embeddings method to turn the raw documents in the document store into high-dimensional vectors that the retrieval model can search and compare."
   ],
   "metadata": {
    "id": "FDOJPWHBmRgV"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QGmhdE2dO0g5"
   },
   "outputs": [],
   "source": [
    "# VALIDATION_FIELD[cls] update_embeddings\n",
    "\n",
    "document_store.update_embeddings(retriever)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### **Generator**\n",
    "\n",
    "You are now ready to initialize the GPT model to generate text for you. The [OpenAIAnswerGenerator](https://docs.haystack.deepset.ai/reference/answer-generator-api#openaianswergenerator) node can use four different GPT models. You can use the highest performing GPT-3.5 model, `text-davinci-003`, with `temperature=.5`, `max_tokens=30` parameters and your `OpenAI key`:\n"
   ],
   "metadata": {
    "id": "X1SvTCGXmiw6"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qC3mvtO_O3HE"
   },
   "outputs": [],
   "source": [
    "# VALIDATION_FIELD[str] generator\n",
    "\n",
    "### START CODE HERE ###\n",
    "generator = ...\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### **Pipeline**\n",
    "\n",
    "Now that all the individual elements of your GPT search engine are set up, it’s time to pass them to your generative QA pipeline.\n",
    "\n",
    ">Use [GenerativeQAPipeline](https://docs.haystack.deepset.ai/reference/pipelines-api#generativeqapipeline) with generator and retriever created early:"
   ],
   "metadata": {
    "id": "pTTNyjUImlMO"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mLPtL_IgPB8o"
   },
   "outputs": [],
   "source": [
    "# VALIDATION_FIELD[str] GenerativeQAPipeline\n",
    "\n",
    "### START CODE HERE ###\n",
    "gpt_search_engine = ...\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### **Querying the pipeline**\n",
    "\n",
    "Now you can ask your system general questions about Berlin (or whatever another topic your dataset is about). In addition to the query, you can pass a few parameters to the search engine, like the number of documents the retriever should deliver to the generator and the number of answers generated (designated **“top_k”**).\n",
    "\n",
    ">Try ask GPT about `\"What is Kyiv known for?\"`:"
   ],
   "metadata": {
    "id": "HEEDawIsmouX"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eS9piUrbPEfK"
   },
   "outputs": [],
   "source": [
    "### START CODE HERE ###\n",
    "query = ...\n",
    "### END CODE HERE ###\n",
    "\n",
    "params = {\"Retriever\": {\"top_k\": 5}, \"Generator\": {\"top_k\": 1}}\n",
    "answer = gpt_search_engine.run(query=query, params=params)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "To print the answer generated by your pipeline, import Haystack’s handy print_answers function. When printing the answer, it lets you determine the detail you want to see. Setting it to a minimum will print only the answer string. So what’s the search engine’s response to the question above?"
   ],
   "metadata": {
    "id": "ffUZXzsJmxc5"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TAeVPFM4PFtb"
   },
   "outputs": [],
   "source": [
    "print_answers(answer, details=\"minimum\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### **Generated answers are context-dependen**\n",
    "\n",
    "GPT-3 model generates its answers based on the documents that it receives. You can now test that by running the generator in isolation without the retriever. You can’t run it without any documents at all, though, so you need to pass it a single snippet. Here’s what happens if you use the snippet about the football team Dynamo that was printed out above.\n"
   ],
   "metadata": {
    "id": "OYUVQ3LLs_op"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "dinamo_document = processed_docs[0]\n",
    "\n",
    "for doc in processed_docs:\n",
    "  if doc.meta['name'] == 'FC Dynamo Kyiv.txt':\n",
    "    dinamo_document = doc\n",
    "    break"
   ],
   "metadata": {
    "id": "tVaHxUrKKZ3M"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9AVww4PoPrxc"
   },
   "outputs": [],
   "source": [
    "answers = generator.predict(\"What is Kyiv known for?\", documents=[dinamo_document], top_k = 1)\n",
    "print_answers(answers, details=\"minimum\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "And let's see what happens if you try to generate an answer for a document that doesn't have any information about your query.\n",
    "\n",
    "We can find a document not about Kyiv Dinamo:"
   ],
   "metadata": {
    "id": "Oz7CN5Ppv4FX"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "wrong_dinamo_file = processed_docs[0]\n",
    "print(wrong_dinamo_file.meta)"
   ],
   "metadata": {
    "id": "aCvCuKkzOU9P"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's ask GPT about the Kyiv Dinamo football team in this document:"
   ],
   "metadata": {
    "id": "UIF4VmVPxi1X"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "answers = generator.predict(\"How was the Kyiv Dinamo football team created?\", documents=[wrong_dinamo_file], top_k = 1)\n",
    "print_answers(answers, details=\"minimum\")"
   ],
   "metadata": {
    "id": "J3VJerbDwNhv"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, go back to the full version of the search engine — the one that’s ingested our whole dataset and ask a few more questions to understand better how your search engine operates:\n"
   ],
   "metadata": {
    "id": "JSds9_bjtZ5l"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uxvi1T-sRaET"
   },
   "outputs": [],
   "source": [
    "query = \"When is the best time to visit Kyiv?\"\n",
    "params = {\"Retriever\": {\"top_k\": 5}, \"Generator\": {\"top_k\": 1}}\n",
    "\n",
    "answer = gpt_search_engine.run(query=query, params=params)\n",
    "print_answers(answer, details=\"minimum\")"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "query = \"Do people from Kyiv have a own culture?\"\n",
    "params = {\"Retriever\": {\"top_k\": 5}, \"Generator\": {\"top_k\": 1}}\n",
    "\n",
    "answer = gpt_search_engine.run(query=query, params=params)\n",
    "print_answers(answer, details=\"minimum\")"
   ],
   "metadata": {
    "id": "UHwxmg03PCS-"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "query = \"Tell me about some interesting place in Kyiv\"\n",
    "params = {\"Retriever\": {\"top_k\": 5}, \"Generator\": {\"top_k\": 1}}\n",
    "\n",
    "answer = gpt_search_engine.run(query=query, params=params)\n",
    "print_answers(answer, details=\"minimum\")"
   ],
   "metadata": {
    "id": "dvYbXDTZPIoX"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "query = \"How was the TV tower built?\"\n",
    "params = {\"Retriever\": {\"top_k\": 5}, \"Generator\": {\"top_k\": 1}}\n",
    "\n",
    "answer = gpt_search_engine.run(query=query, params=params)\n",
    "print_answers(answer, details=\"minimum\")"
   ],
   "metadata": {
    "id": "FzhB6KuQPJLl"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cDOEc3ImRU9n"
   },
   "outputs": [],
   "source": [
    "query = \"Is Kyiv a good place for clubbing?\"\n",
    "params = {\"Retriever\": {\"top_k\": 5}, \"Generator\": {\"top_k\": 1}}\n",
    "\n",
    "answer = gpt_search_engine.run(query=query, params=params)\n",
    "print_answers(answer, details=\"minimum\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **Section 3 - Fine-tune GPT**\n",
    "\n",
    "Fine-tuning is the process of training a [Large Language Model (LLM)](https://en.wikipedia.org/wiki/Large_language_model) to recognize a specific pattern of input and output that can be applied to any custom NLP task.\n",
    "\n",
    "Taken from the [official docs](https://platform.openai.com/docs/guides/fine-tuning), fine-tuning lets you get more out of the GPT-3 models by providing the following:\n",
    "* Higher quality results than prompt design\n",
    "* Ability to train on more examples than can fit in a * prompt\n",
    "* Token savings due to shorter prompts\n",
    "* Lower latency requests\n",
    "\n",
    "Fine-tuning involves the following steps:\n",
    "* Prepare and upload training data\n",
    "* Train a new fine-tuned model\n",
    "* Use your fine-tuned model"
   ],
   "metadata": {
    "id": "Bbdq9ruyVA-i"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "!pip install openai==0.27.4\n",
    "import openai\n",
    "import pandas"
   ],
   "metadata": {
    "id": "Dal5XrbIV5RW"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Add OpenAI API key\n",
    "\n",
    "You need to make an account and generate an [API key](https://platform.openai.com/account/api-keys)."
   ],
   "metadata": {
    "id": "M_kFoqEzdyV8"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# VALIDATION_FIELD[str] set_openai_api_key_fine_tune\n",
    "\n",
    "openai.api_key = Config.openai_api_key"
   ],
   "metadata": {
    "id": "0_Y_Z16pV-k5"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's download our dataset.\n",
    "\n",
    "You can also create your dataset with your questions and answers. The dataset should be of the following format:\n",
    "\n",
    "```\n",
    "{\"prompt\": \"<question>\", \"completion\": \"<ideal answer>\"}\n",
    "{\"prompt\": \"<question>\", \"completion\": \"<ideal answer>\"}\n",
    "{\"prompt\": \"<question>\", \"completion\": \"<ideal answer>\"}\n",
    "```\n",
    "and saved in `.jsonl` format file."
   ],
   "metadata": {
    "id": "D-1rF5q_urXG"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install wget\n",
    "import wget\n",
    "\n",
    "wget.download('https://dru.fra1.digitaloceanspaces.com/DL_pytorch/datasets/07_attention_transformers/GPT-3/QA_DataRoot_Labs.csv')"
   ],
   "metadata": {
    "id": "hXh3-PpFmRpH"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let`s check our dataset"
   ],
   "metadata": {
    "id": "f2yz8OxbsqXn"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "data = pandas.read_csv('QA_DataRoot_Labs.csv')"
   ],
   "metadata": {
    "id": "v15letoEnRr1"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "print('Prompt:', data.iloc[0]['prompt'])\n",
    "print('Completion:', data.iloc[0]['completion'])"
   ],
   "metadata": {
    "id": "tk0QWivVgKtl"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Expected output:**\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <td>Prompt:</td>\n",
    "    <td>What is it DataRoot Labs?</td>\n",
    " </tr>\n",
    " <tr>\n",
    "    <td>Completion: </td>\n",
    "    <td>DataRoot Labs is a full-service Data Science & Artificial Intelligence R&D company with main focus on Big Data Management & Strategy Consulting, Data Science & Engineering.DataRoot Labs consists of AI, HighLoad and Science teams — geeks, really good at building & assembling AI-Enabled solutions & Infrastructures, complex scientific R&D.AI Lab delivers to our partners and clients the unique value leveraging Deep Learning, Computer Vision, NLP, Advanced Scoring Models'} </td>\n",
    "  </tr>\n",
    "</table>"
   ],
   "metadata": {
    "id": "G9Mgpl73nSTL"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Add suffixs and unload data\n",
    "\n",
    "Make sure to end each prompt with a suffix. According to the [OpenAI API reference](https://platform.openai.com/docs/guides/fine-tuning/preparing-your-dataset), we will use \"` ->`\".\n",
    "\n",
    "Also, make sure to end each completion with a suffix as well; We are using \"`.\\n`\"."
   ],
   "metadata": {
    "id": "m9ZVzG3QkQqF"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def add_suffix(dataset):\n",
    "  # add suffix\n",
    "  for index, train_request in dataset.iterrows():\n",
    "    # check that prompt end to \" ->\" and add\n",
    "    if train_request['prompt'][-3:] != ' ->':\n",
    "      train_request['prompt'] = ''.join([train_request['prompt'], ' ->'])\n",
    "\n",
    "    # completion that prompt end to \".\"\" and add '\\n' if exist or '.\\n' if dot (.) not exist\n",
    "    if train_request['completion'][-3:] != '.\\n':\n",
    "      if train_request['completion'][-1] == '.':\n",
    "        train_request['completion'] = ''.join([train_request['completion'], '\\n'])\n",
    "      else:\n",
    "        train_request['completion'] = ''.join([train_request['completion'], '.\\n'])\n",
    "  return dataset"
   ],
   "metadata": {
    "id": "ykcwxNdDkLhk"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "data = add_suffix(data)"
   ],
   "metadata": {
    "id": "Mx5LJXdw7Kn6"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's see some QA examples from our dataset:"
   ],
   "metadata": {
    "id": "8AzCipM5wrVo"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "data[0:2]"
   ],
   "metadata": {
    "id": "1aWD_6S4prsF"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Expected output:**\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <td>0</td>\n",
    "    <td>What is it DataRoot Labs? -></td>\n",
    "    <td>DataRoot Labs is a full-service Data Science ..</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>1</td>\n",
    "    <td>Can the DataRoot Labs provide advice? -></td>\n",
    "    <td>During the discovery call with a client, we p...</td>\n",
    "  </tr>\n",
    "</table>"
   ],
   "metadata": {
    "id": "BCmoWdE72j05"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Convert the preprocessed dataset to a proper JSONL file.\n",
    "\n",
    "JSONL file is a newline-delimited JSON file, so we'll add a `\\n` at the end of each object:"
   ],
   "metadata": {
    "id": "fB1k8tDewqOZ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "file_name = \"training_data.jsonl\"\n",
    "\n",
    "data = data.to_dict('records')\n",
    "\n",
    "with open(file_name, \"w\") as output_file:\n",
    " for entry in data:\n",
    "  json.dump(entry, output_file)\n",
    "  output_file.write(\"\\n\")"
   ],
   "metadata": {
    "id": "qpLE8BG_XpVe"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Prapare data by OpenAI\n",
    "\n",
    "You can check the training data using a CLI data preparation tool provided by OpenAI. It gives you suggestions about how you can reformat the training data.\n",
    "Let's try it out with our training data. Run this line in Jupyter Notebook:\n",
    ">**Note that during the computation can prompt requiring some actions. Be sure to respond with \"y\" to confirm and agree to all of them.**"
   ],
   "metadata": {
    "id": "hvQWDYlkxNb4"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!openai tools fine_tunes.prepare_data -f training_data.jsonl"
   ],
   "metadata": {
    "id": "ad5X5J_tXsVt"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "You will see something similar to this:\n",
    "\n",
    "```\n",
    "Analyzing...\n",
    "\n",
    "- Your file contains 36 prompt-completion pairs. In general, we recommend having at least a few hundred examples. We've found that performance tends to linearly increase for every doubling of the number of examples\n",
    "- All prompts end with suffix ` ->`\n",
    "- All completions end with suffix `.\\n`\n",
    "  WARNING: Some of your completions contain the suffix `.\n",
    "` more than once. We suggest that you review your completions and add a unique ending\n",
    "- The completion should start with a whitespace character (` `). This tends to produce better results due to the tokenization we use. See https://platform.openai.com/docs/guides/fine-tuning/preparing-your-dataset for more details\n",
    "\n",
    "Based on the analysis we will perform the following actions:\n",
    "- [Recommended] Add a whitespace character to the beginning of the completion [Y/n]: y\n",
    "\n",
    "\n",
    "Your data will be written to a new JSONL file. Proceed [Y/n]: y\n",
    "\n",
    "Wrote modified file to `training_data_prepared.jsonl`\n",
    "Feel free to take a look!\n",
    "\n",
    "Now use that file when fine-tuning:\n",
    "> openai api fine_tunes.create -t \"training_data_prepared.jsonl\"\n",
    "\n",
    "After you’ve fine-tuned a model, remember that your prompt has to end with the indicator string ` ->` for the model to start generating completions, rather than continuing with the prompt. Make sure to include `stop=[\".\\n\"]` so that the generated texts ends at the expected place.\n",
    "Once your model starts training, it'll approximately take 2.94 minutes to train a `curie` model, and less for `ada` and `babbage`. Queue will approximately take half an hour per job ahead of you.\n",
    "```\n",
    "\n"
   ],
   "metadata": {
    "id": "zLBBaaIoxUhI"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Fine-tuning\n",
    "\n",
    "With prepared data we can start fine-tulilng GRP model."
   ],
   "metadata": {
    "id": "V0evUA1cSvMy"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Create a File\n",
    "\n",
    "For fine-tuning you need upload files to OpenAI.\n",
    "\n",
    "> Add a prepared dataset file and create an upload response by [File create](https://platform.openai.com/docs/api-reference/files/upload) with parameters: `file=open(file_name, \"rb\")` and `purpose='fine-tune'` "
   ],
   "metadata": {
    "id": "vV0dtynk1Ia2"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "### START CODE HERE ###\n",
    "file_name = 'training_data_prepared.jsonl'\n",
    "upload_response = ...\n",
    "### END CODE HERE ###\n",
    "\n",
    "file_id = upload_response.id\n",
    "upload_response"
   ],
   "metadata": {
    "id": "b8fRBvlZXxtH"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Start the fine-tuning\n",
    "\n",
    "In the final result, you can see `file_id`. This parameter is your uploaded dataset ID, which needs to fine-tune the `davinci model`.\n",
    "\n",
    "Start the fine-tuning by [FineTune create](https://platform.openai.com/docs/api-reference/fine-tunes/create) method whith this parameters:\n",
    "- `training_file = file_id`\n",
    "- `model = 'davinci'`\n",
    "- `n_epochs = 8`"
   ],
   "metadata": {
    "id": "VlyRPFgb1tjP"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "### START CODE HERE ###\n",
    "fine_tune_response = ...\n",
    "### END CODE HERE ###"
   ],
   "metadata": {
    "id": "i41KVbN0X0Dg"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "fine_tune_id = fine_tune_response.id\n",
    "fine_tune_response"
   ],
   "metadata": {
    "id": "u7Wg48F0iBPT"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "> Write your fine-tune response id to `fine_tune_id` variable in `str format`. "
   ],
   "metadata": {
    "id": "Ujaje_4PiOju"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# VALIDATION_FIELD[str] fine_tune_response_id\n",
    "\n",
    "### START CODE HERE ###\n",
    "# for example 'ft-xxxxxxxxxxxxxxxxxxx'\n",
    "fine_tune_id = ...\n",
    "### END CODE HERE ###"
   ],
   "metadata": {
    "id": "f5vjMFxfiMd4"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Training can take `from 15 to 25 minutes`. You can monitor the progress with [retrieve](https://platform.openai.com/docs/api-reference/fine-tunes/retrieve).\n",
    "\n",
    "> `id = fine_tune_id` \n",
    "\n",
    "The message `\"Fine-tune succeeded\"` will tell you that process finally gone."
   ],
   "metadata": {
    "id": "dr1dSXyovfxr"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "### START CODE HERE ###\n",
    "response = ...\n",
    "### END CODE HERE ###\n",
    "\n",
    "print(response.events)"
   ],
   "metadata": {
    "id": "MRS2cXcmX0xm"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Get the name of your fine-tuned model."
   ],
   "metadata": {
    "id": "_6hXBH-b3g1X"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "fine_tuned_model = response.fine_tuned_model\n",
    "fine_tuned_model"
   ],
   "metadata": {
    "id": "AP4HcGjiotjA"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "> Write your fine-tuned model name to `fine_tuned_model` variable in `str format`."
   ],
   "metadata": {
    "id": "g7BYwUJ3gyKV"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# VALIDATION_FIELD[str] fine_tuned_model_name\n",
    "\n",
    "### START CODE HERE ###\n",
    "# for example 'davinci:ft-personal-2022-02-12-22-22-22'\n",
    "fine_tuned_model = ...\n",
    "### END CODE HERE ###"
   ],
   "metadata": {
    "id": "MP2IxIR4gxfY"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Competiton\n",
    "\n",
    "Let's ask our model about who is the COO of DataRoot Labs.\n",
    "\n",
    "> Your prompt should end with the suffix \"` ->`\""
   ],
   "metadata": {
    "id": "ltIsUJEm5Pw7"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "### START CODE HERE ###\n",
    "new_prompt = ...\n",
    "### END CODE HERE ###"
   ],
   "metadata": {
    "id": "1iQ5p3d0o1HV"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Using [Completion](https://platform.openai.com/docs/api-reference/completions/create) create a response to your fine-tuned model.\n",
    "- `max_tokens = 100`\n",
    "- `temperature = 0.3`\n",
    "- `stop = '.\\n'`\n",
    "- `model = your fine-tuned model name`"
   ],
   "metadata": {
    "id": "uJni8TxZKUlL"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "### START CODE HERE ###\n",
    "answer = ...\n",
    "### END CODE HERE ###\n",
    "print(answer['choices'][0]['text'])"
   ],
   "metadata": {
    "id": "lSYOLxR8pHj5"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Expected output:**\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <td>Result:</td>\n",
    "    <td> Yuliya Sychikova </td> \n",
    "  </tr>\n",
    "</table>"
   ],
   "metadata": {
    "id": "wa0eQUmHfpvv"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Conclusion\n",
    "As we can see, our model fits well the hypothesis function to the data.\n",
    "\n",
    "### What's next:\n",
    "1. Try experimenting with GPT models and prompt\n",
    "2. Using OpenAI API try text to image DALL·E or speech to text Whisper models.\n",
    "3. Fine-tune own model using another QA dataset."
   ],
   "metadata": {
    "id": "s9ulVzpblF7A"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Make sure that you didn't add or delete any notebook cells. Otherwise your work may not be accepted by the validator!"
   ],
   "metadata": {
    "id": "elOxBe5wqfkX"
   }
  }
 ]
}
