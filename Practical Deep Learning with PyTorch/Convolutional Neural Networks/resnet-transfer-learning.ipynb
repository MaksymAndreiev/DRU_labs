{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BkVnnBSUmxK9-gh"
   },
   "source": [
    "# Transfer learning on ResNet\n",
    "\n",
    "Hi, and welcome to your next lab! In this lab, you will learn how to use a pre-trained network for an image classification task.\n",
    "\n",
    "**GPU** is recomended for this assignment. `Runtime` -> `Change runtime type` -> `GPU`\n",
    "\n",
    "**Instructions**\n",
    "- Write code in the space indicated with `### START CODE HERE ###`\n",
    "- Do not use loops (for/while) unless instructions explicitly tell you so. Parallelization in Deep Learning is key!\n",
    "- If you get stuck, ask for help in Slack or DM `@DRU Team`\n",
    "\n",
    "**You will learn**\n",
    "- How to fine-tune a pre-trained ResNet18 for multi-class classification \n",
    "    - How to preprocess and augment image dataset\n",
    "    - How to evaluate the model with train/validation/test splits\n",
    "    - How to visualize the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y1ytD-713JyG"
   },
   "source": [
    "# 0 - Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9837,
     "status": "ok",
     "timestamp": 1627471598866,
     "user": {
      "displayName": "Andrii Torchylo",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhFTWqrU3m-Wp-bUa2OXsooRztUv4cznKjeaEV8YFFpjQygmUGowgZHBW_vtX1MkHaFbXTu_X-7ofPcehpOxdMB6ioGBAESNXPxl7QQUA8tcWsv5CQWVjgOO7zGpy1OYlyhSVCow7xySYmBEb8o-mMBuzgfyBX2d8ZtdTGkIx7YyCCajdzbk0kgCAGFGPxaVtx--pJO4X5PnsLoWquOZ_VlruRtEZ7p7gsSzzwBlhPII6o5J-MOL1TWKl-0kDtS8_kPG1fGuwKmVjY2nXN2oewFaAKc9ajLNHKBCfwQu_nJ0KT4sHWfBIGjebKBa0fk_Xb0DdXgz8eh33PNWk8unTN3862OyaqzPbE4jXgYGRW1WELosWHw59Vkvm9Hdr89I_XP6eHd1-L46_kdP84LAXF9q5nxnFFNCJmLRYSm0Wti3Dn8uc6Ixa9kyx5KqBn4ThycCUKxWN6o2fM0kQ6rxt1KBQeRKuGtSbuM7RM-8cfh2zB4CrHJ_Xsunj8k3O27212Nerlf4UtA5OKCpAz2XjSPcG6OxGhxe6NArua8Sobb4Un7FrRvgxLsQhwt4CGV3GFSkGSorJ_S3SRlPWFD2VgW8VMvAyqPenJAVXh6AQWA45opLwFqsVB49_XVUv-9cRS9d1oTbIG06hJ2-b3Adew2vX6HOckNoaF-hagdtAmE-aTslmgG38GF4hwbofHCbUALdsDJz728mXsJiTKqHL-DOEIdO1m2gn7W9MXx57wVy-Wu5xJSFhd7kNfMlR2MlpdV6Q=s64",
      "userId": "18357331370165036741"
     },
     "user_tz": -180
    },
    "id": "F0HOZ5r13JyH",
    "outputId": "40ccafbc-8306-4a05-d163-d387a5854c14",
    "ExecuteTime": {
     "end_time": "2024-05-31T22:55:31.329177Z",
     "start_time": "2024-05-31T22:55:28.534665Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wget in c:\\users\\maxog\\pycharmprojects\\dru_labs\\.venv\\lib\\site-packages (3.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install wget\n",
    "import wget \n",
    "#wget.download('https://dru.fra1.digitaloceanspaces.com/DL_pytorch/static/3_conv_nn/resnet_transfer_learning/template.zip')\n",
    "#!unzip -q 'template.zip'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jYXr__3hS0fQ"
   },
   "source": [
    "# 1 - Import packages\n",
    "Here we will import our regular packages with the addition of [torchvision](https://pytorch.org/vision/stable/index.html), which is a part of the PyTorch framework that implements useful utils for image-related tasks. Also, `torchvision` has popular image datasets, and pre-trained models like `ResNet18`, which we will be using in this lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 1344,
     "status": "ok",
     "timestamp": 1627471628725,
     "user": {
      "displayName": "Andrii Torchylo",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhFTWqrU3m-Wp-bUa2OXsooRztUv4cznKjeaEV8YFFpjQygmUGowgZHBW_vtX1MkHaFbXTu_X-7ofPcehpOxdMB6ioGBAESNXPxl7QQUA8tcWsv5CQWVjgOO7zGpy1OYlyhSVCow7xySYmBEb8o-mMBuzgfyBX2d8ZtdTGkIx7YyCCajdzbk0kgCAGFGPxaVtx--pJO4X5PnsLoWquOZ_VlruRtEZ7p7gsSzzwBlhPII6o5J-MOL1TWKl-0kDtS8_kPG1fGuwKmVjY2nXN2oewFaAKc9ajLNHKBCfwQu_nJ0KT4sHWfBIGjebKBa0fk_Xb0DdXgz8eh33PNWk8unTN3862OyaqzPbE4jXgYGRW1WELosWHw59Vkvm9Hdr89I_XP6eHd1-L46_kdP84LAXF9q5nxnFFNCJmLRYSm0Wti3Dn8uc6Ixa9kyx5KqBn4ThycCUKxWN6o2fM0kQ6rxt1KBQeRKuGtSbuM7RM-8cfh2zB4CrHJ_Xsunj8k3O27212Nerlf4UtA5OKCpAz2XjSPcG6OxGhxe6NArua8Sobb4Un7FrRvgxLsQhwt4CGV3GFSkGSorJ_S3SRlPWFD2VgW8VMvAyqPenJAVXh6AQWA45opLwFqsVB49_XVUv-9cRS9d1oTbIG06hJ2-b3Adew2vX6HOckNoaF-hagdtAmE-aTslmgG38GF4hwbofHCbUALdsDJz728mXsJiTKqHL-DOEIdO1m2gn7W9MXx57wVy-Wu5xJSFhd7kNfMlR2MlpdV6Q=s64",
      "userId": "18357331370165036741"
     },
     "user_tz": -180
    },
    "id": "RiddvWBtaVJ5",
    "ExecuteTime": {
     "end_time": "2024-05-31T22:55:35.570992Z",
     "start_time": "2024-05-31T22:55:31.331563Z"
    }
   },
   "outputs": [],
   "source": [
    "import os, shutil\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4OrOtAmUTb8w"
   },
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W8yIUOtP7gfO"
   },
   "source": [
    "In `Config` class, we will define standard hyperparameters for training. There are also few additional hyperparameters: <br><br>\n",
    "**Data Preparation**\n",
    "1. test_fraction - the fraction of the whole dataset that will be used for final evaluation\n",
    "2. validation_fraction - the fraction of the whole dataset that will be used for validating model performance during training\n",
    "3. num_workers - the number of CPU cores to use for the data loading process \n",
    "**Data Transformation**\n",
    "1. mean - the desired mean across each channel after normalization\n",
    "2. std - the desired standard deviation across each channel after normalization\n",
    "3. resize_to - the size of the resized images\n",
    "4. img_size - the final size of cropped images\n",
    "\n",
    "**Note:** in our previous labs, we used a mean of $0$ and a standard deviation of $1$ as our normalization parameters. However, in this lab, we will be using a neural network that was trained on [ImageNet](https://image-net.org/). So, we will have to standardize our data to the format of `ImageNet`, in which images have mean of $(0.485, 0.456, 0.406)$, and standard deviation of $(0.229, 0.224, 0.225)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 233,
     "status": "ok",
     "timestamp": 1627471638721,
     "user": {
      "displayName": "Andrii Torchylo",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhFTWqrU3m-Wp-bUa2OXsooRztUv4cznKjeaEV8YFFpjQygmUGowgZHBW_vtX1MkHaFbXTu_X-7ofPcehpOxdMB6ioGBAESNXPxl7QQUA8tcWsv5CQWVjgOO7zGpy1OYlyhSVCow7xySYmBEb8o-mMBuzgfyBX2d8ZtdTGkIx7YyCCajdzbk0kgCAGFGPxaVtx--pJO4X5PnsLoWquOZ_VlruRtEZ7p7gsSzzwBlhPII6o5J-MOL1TWKl-0kDtS8_kPG1fGuwKmVjY2nXN2oewFaAKc9ajLNHKBCfwQu_nJ0KT4sHWfBIGjebKBa0fk_Xb0DdXgz8eh33PNWk8unTN3862OyaqzPbE4jXgYGRW1WELosWHw59Vkvm9Hdr89I_XP6eHd1-L46_kdP84LAXF9q5nxnFFNCJmLRYSm0Wti3Dn8uc6Ixa9kyx5KqBn4ThycCUKxWN6o2fM0kQ6rxt1KBQeRKuGtSbuM7RM-8cfh2zB4CrHJ_Xsunj8k3O27212Nerlf4UtA5OKCpAz2XjSPcG6OxGhxe6NArua8Sobb4Un7FrRvgxLsQhwt4CGV3GFSkGSorJ_S3SRlPWFD2VgW8VMvAyqPenJAVXh6AQWA45opLwFqsVB49_XVUv-9cRS9d1oTbIG06hJ2-b3Adew2vX6HOckNoaF-hagdtAmE-aTslmgG38GF4hwbofHCbUALdsDJz728mXsJiTKqHL-DOEIdO1m2gn7W9MXx57wVy-Wu5xJSFhd7kNfMlR2MlpdV6Q=s64",
      "userId": "18357331370165036741"
     },
     "user_tz": -180
    },
    "id": "CKlaSKY38MzX",
    "ExecuteTime": {
     "end_time": "2024-05-31T22:55:35.586911Z",
     "start_time": "2024-05-31T22:55:35.572096Z"
    }
   },
   "outputs": [],
   "source": [
    "# VALIDATION_FIELD[cls] Config\n",
    "\n",
    "class Config:\n",
    "    # Training\n",
    "    seed = 21\n",
    "    epochs = 30\n",
    "    learning_rate = 0.001\n",
    "    num_classes = 6\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "    # Data preparation\n",
    "    data_root = './Chess'\n",
    "    test_fraction = 0.2\n",
    "    validation_fraction = 0.15\n",
    "    batch_size = 16\n",
    "    num_workers = 2\n",
    "    classes = ('Bishop', 'King', 'Knight', 'Pawn', 'Queen', 'Rook')\n",
    "\n",
    "    # Data transformation\n",
    "    mean = (0.485, 0.456, 0.406) \n",
    "    std = (0.229, 0.224, 0.225) \n",
    "    resize_to = 256\n",
    "    img_size = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 309,
     "status": "ok",
     "timestamp": 1627471669361,
     "user": {
      "displayName": "Andrii Torchylo",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhFTWqrU3m-Wp-bUa2OXsooRztUv4cznKjeaEV8YFFpjQygmUGowgZHBW_vtX1MkHaFbXTu_X-7ofPcehpOxdMB6ioGBAESNXPxl7QQUA8tcWsv5CQWVjgOO7zGpy1OYlyhSVCow7xySYmBEb8o-mMBuzgfyBX2d8ZtdTGkIx7YyCCajdzbk0kgCAGFGPxaVtx--pJO4X5PnsLoWquOZ_VlruRtEZ7p7gsSzzwBlhPII6o5J-MOL1TWKl-0kDtS8_kPG1fGuwKmVjY2nXN2oewFaAKc9ajLNHKBCfwQu_nJ0KT4sHWfBIGjebKBa0fk_Xb0DdXgz8eh33PNWk8unTN3862OyaqzPbE4jXgYGRW1WELosWHw59Vkvm9Hdr89I_XP6eHd1-L46_kdP84LAXF9q5nxnFFNCJmLRYSm0Wti3Dn8uc6Ixa9kyx5KqBn4ThycCUKxWN6o2fM0kQ6rxt1KBQeRKuGtSbuM7RM-8cfh2zB4CrHJ_Xsunj8k3O27212Nerlf4UtA5OKCpAz2XjSPcG6OxGhxe6NArua8Sobb4Un7FrRvgxLsQhwt4CGV3GFSkGSorJ_S3SRlPWFD2VgW8VMvAyqPenJAVXh6AQWA45opLwFqsVB49_XVUv-9cRS9d1oTbIG06hJ2-b3Adew2vX6HOckNoaF-hagdtAmE-aTslmgG38GF4hwbofHCbUALdsDJz728mXsJiTKqHL-DOEIdO1m2gn7W9MXx57wVy-Wu5xJSFhd7kNfMlR2MlpdV6Q=s64",
      "userId": "18357331370165036741"
     },
     "user_tz": -180
    },
    "id": "FpoOE6mM3JyM",
    "ExecuteTime": {
     "end_time": "2024-05-31T22:55:35.602854Z",
     "start_time": "2024-05-31T22:55:35.589193Z"
    }
   },
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    np.random.seed(seed) \n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "set_seed(Config.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S6Lsh8di-0U-"
   },
   "source": [
    "# 2 - Data Preprocessing and visualizations\n",
    "\n",
    "For our dataset, we will be using [Chessman Image Dataset](https://www.kaggle.com/niteshfre/chessman-image-dataset) from [Kaggle](https://www.kaggle.com). This dataset contains ~80 - 100 images of each chess piece. Our task will be to classify the chess piece by the image. Let's visualize how many images of each class we have:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 441
    },
    "executionInfo": {
     "elapsed": 406,
     "status": "ok",
     "timestamp": 1627471689290,
     "user": {
      "displayName": "Andrii Torchylo",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhFTWqrU3m-Wp-bUa2OXsooRztUv4cznKjeaEV8YFFpjQygmUGowgZHBW_vtX1MkHaFbXTu_X-7ofPcehpOxdMB6ioGBAESNXPxl7QQUA8tcWsv5CQWVjgOO7zGpy1OYlyhSVCow7xySYmBEb8o-mMBuzgfyBX2d8ZtdTGkIx7YyCCajdzbk0kgCAGFGPxaVtx--pJO4X5PnsLoWquOZ_VlruRtEZ7p7gsSzzwBlhPII6o5J-MOL1TWKl-0kDtS8_kPG1fGuwKmVjY2nXN2oewFaAKc9ajLNHKBCfwQu_nJ0KT4sHWfBIGjebKBa0fk_Xb0DdXgz8eh33PNWk8unTN3862OyaqzPbE4jXgYGRW1WELosWHw59Vkvm9Hdr89I_XP6eHd1-L46_kdP84LAXF9q5nxnFFNCJmLRYSm0Wti3Dn8uc6Ixa9kyx5KqBn4ThycCUKxWN6o2fM0kQ6rxt1KBQeRKuGtSbuM7RM-8cfh2zB4CrHJ_Xsunj8k3O27212Nerlf4UtA5OKCpAz2XjSPcG6OxGhxe6NArua8Sobb4Un7FrRvgxLsQhwt4CGV3GFSkGSorJ_S3SRlPWFD2VgW8VMvAyqPenJAVXh6AQWA45opLwFqsVB49_XVUv-9cRS9d1oTbIG06hJ2-b3Adew2vX6HOckNoaF-hagdtAmE-aTslmgG38GF4hwbofHCbUALdsDJz728mXsJiTKqHL-DOEIdO1m2gn7W9MXx57wVy-Wu5xJSFhd7kNfMlR2MlpdV6Q=s64",
      "userId": "18357331370165036741"
     },
     "user_tz": -180
    },
    "id": "Vul0dDbjbAho",
    "outputId": "dd0de133-b7a4-4d42-894a-1487db850649",
    "ExecuteTime": {
     "end_time": "2024-05-31T22:55:35.830780Z",
     "start_time": "2024-05-31T22:55:35.605203Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 1000x600 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class_counts = {}\n",
    "for root, dirs, files in os.walk(Config.data_root):\n",
    "    if root != Config.data_root:\n",
    "        class_counts[root.split('/')[-1]] = len(files)\n",
    "        \n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(class_counts.keys(), class_counts.values())\n",
    "plt.xticks(np.arange(Config.num_classes), Config.classes) \n",
    "plt.ylabel('Count')\n",
    "plt.xlabel('Chess piece')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Av6D4ArexTd"
   },
   "source": [
    "## Train/Test/Validation split\n",
    "Here we will split each class of images into `train`, `test`, and `valid` subsets. We will create three directories with respective names and then copy files into those directories. Please note the structure of each folder (it will be important for using `ImageFolder` dataset from `torchvision`):\n",
    "\n",
    "```\n",
    "test/\n",
    "    Bishop/\n",
    "        00000001.jpg\n",
    "        00000002.jpg\n",
    "        ...\n",
    "   King/\n",
    "        00000004.jpg\n",
    "        00000007.jpg\n",
    "        ...\n",
    "   Knight/\n",
    "       ...\n",
    "   Pawn/\n",
    "       ...\n",
    "   Queen/\n",
    "       ...\n",
    "   Rook/\n",
    "       ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 241,
     "status": "ok",
     "timestamp": 1627471709190,
     "user": {
      "displayName": "Andrii Torchylo",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhFTWqrU3m-Wp-bUa2OXsooRztUv4cznKjeaEV8YFFpjQygmUGowgZHBW_vtX1MkHaFbXTu_X-7ofPcehpOxdMB6ioGBAESNXPxl7QQUA8tcWsv5CQWVjgOO7zGpy1OYlyhSVCow7xySYmBEb8o-mMBuzgfyBX2d8ZtdTGkIx7YyCCajdzbk0kgCAGFGPxaVtx--pJO4X5PnsLoWquOZ_VlruRtEZ7p7gsSzzwBlhPII6o5J-MOL1TWKl-0kDtS8_kPG1fGuwKmVjY2nXN2oewFaAKc9ajLNHKBCfwQu_nJ0KT4sHWfBIGjebKBa0fk_Xb0DdXgz8eh33PNWk8unTN3862OyaqzPbE4jXgYGRW1WELosWHw59Vkvm9Hdr89I_XP6eHd1-L46_kdP84LAXF9q5nxnFFNCJmLRYSm0Wti3Dn8uc6Ixa9kyx5KqBn4ThycCUKxWN6o2fM0kQ6rxt1KBQeRKuGtSbuM7RM-8cfh2zB4CrHJ_Xsunj8k3O27212Nerlf4UtA5OKCpAz2XjSPcG6OxGhxe6NArua8Sobb4Un7FrRvgxLsQhwt4CGV3GFSkGSorJ_S3SRlPWFD2VgW8VMvAyqPenJAVXh6AQWA45opLwFqsVB49_XVUv-9cRS9d1oTbIG06hJ2-b3Adew2vX6HOckNoaF-hagdtAmE-aTslmgG38GF4hwbofHCbUALdsDJz728mXsJiTKqHL-DOEIdO1m2gn7W9MXx57wVy-Wu5xJSFhd7kNfMlR2MlpdV6Q=s64",
      "userId": "18357331370165036741"
     },
     "user_tz": -180
    },
    "id": "Skq0hhfqbzfx",
    "ExecuteTime": {
     "end_time": "2024-05-31T22:55:36.469584Z",
     "start_time": "2024-05-31T22:55:35.833084Z"
    }
   },
   "outputs": [],
   "source": [
    "def split_data_folder(input_root, train2rest, test2valid):\n",
    "    # iterate over all classes \n",
    "    for directory in os.listdir(input_root):\n",
    "        # skip jupyter and IDE files\n",
    "        if directory.startswith('.'):\n",
    "            continue\n",
    "\n",
    "        # split paths into train, test, and valid\n",
    "        all_imgs = os.listdir(os.path.join(input_root, directory))\n",
    "        train, rest = train_test_split(all_imgs, random_state=Config.seed, test_size=train2rest) \n",
    "        test, valid  = train_test_split(rest, random_state=Config.seed, test_size=test2valid)\n",
    "\n",
    "        # copy files into target directories\n",
    "        data_paths = {'train': train, 'valid': valid, 'test': test}\n",
    "        for name, subset in data_paths.items():\n",
    "            for img in subset:   \n",
    "                src = os.path.join(input_root, directory, img)\n",
    "                dst = os.path.join(name, directory)\n",
    "                if not os.path.exists(dst):\n",
    "                    os.makedirs(dst)\n",
    "                shutil.copy(src, dst)\n",
    "\n",
    "                \n",
    "train2rest = Config.validation_fraction + Config.test_fraction # fraction of data for training\n",
    "test2valid = Config.validation_fraction/train2rest # fraction of remaining data (validation + test) for testing \n",
    "split_data_folder(Config.data_root, train2rest, test2valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1JLcFSRY7gfT"
   },
   "source": [
    "## PyTorch Data Pipeline\n",
    "\n",
    "PyTorch provides many useful functions and classes to speed up the data preparation and feeding process and decrease RAM usage during training. Here are three main components of the PyTorch Data Pipeline:\n",
    "1. **Data transforms** - set of functions for data preprocessing and augmentation\n",
    "2. **PyTorch Dataset** - a base class for PyTorch-like datasets\n",
    "3. **PyTorch Dataloader** - a class that optimizes the data feeding process and performs batching on a dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c-1nvXIffofs"
   },
   "source": [
    "## Data transforms\n",
    "\n",
    "**PyTorch transform** is a class that initializes from the transformation parameters like desired resize size or rotation angle for an image and has a method `__call__` that applies the transformation on the input data example. \n",
    "\n",
    "**Example:** `transforms.Normalize` takes desired `mean` and `std` as inputs and performs image normalization when called. \n",
    "\n",
    "```python\n",
    "norm_transform = transforms.Normalize(mean, std)\n",
    "normalized_img = norm_transform(img)\n",
    "```\n",
    "\n",
    "The key feature of data transforms is that you can easily stack them together using `transforms.Compose()`, and pass them to a PyTorch Dataset as a `transform` parameter. So that transforms will be applied every time an image gets extracted from the dataset.\n",
    "Also, PyTorch uses transform for the **data augmentation** process. So, instead of generating and saving augmented images, PyTorch simply applies a **random** transform on every iteration over the dataset. By doing so, we don't physically increase the size of our dataset but randomly change each image before feeding it into our model.<br><br>\n",
    "**Excercise:** Using [documentation](https://pytorch.org/vision/stable/auto_examples/plot_transforms.html#sphx-glr-auto-examples-plot-transforms-py) of transforms, implement `get_transforms` function that returns composed set of transforms for train, validation, and test sets.\n",
    "**Directions:** \n",
    "* For `train` set, create the following transforms:\n",
    "    1. Resize images to `Config.resize_to`\n",
    "    2. Perform random resized cropping with a size of `Config.img_size`\n",
    "    3. Perform random horizontal flipping\n",
    "    4. Convert an image to tensor\n",
    "    5. Normalize the image to `Config.mean` and `Config.std`\n",
    "* For `test` and `valid` set create the following:\n",
    "    1. Resize images to `Config.img_size`\n",
    "    2. Convert an image to tensor\n",
    "    3. Normalize the image to `Config.mean` and `Config.std`\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 242,
     "status": "ok",
     "timestamp": 1627471729268,
     "user": {
      "displayName": "Andrii Torchylo",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhFTWqrU3m-Wp-bUa2OXsooRztUv4cznKjeaEV8YFFpjQygmUGowgZHBW_vtX1MkHaFbXTu_X-7ofPcehpOxdMB6ioGBAESNXPxl7QQUA8tcWsv5CQWVjgOO7zGpy1OYlyhSVCow7xySYmBEb8o-mMBuzgfyBX2d8ZtdTGkIx7YyCCajdzbk0kgCAGFGPxaVtx--pJO4X5PnsLoWquOZ_VlruRtEZ7p7gsSzzwBlhPII6o5J-MOL1TWKl-0kDtS8_kPG1fGuwKmVjY2nXN2oewFaAKc9ajLNHKBCfwQu_nJ0KT4sHWfBIGjebKBa0fk_Xb0DdXgz8eh33PNWk8unTN3862OyaqzPbE4jXgYGRW1WELosWHw59Vkvm9Hdr89I_XP6eHd1-L46_kdP84LAXF9q5nxnFFNCJmLRYSm0Wti3Dn8uc6Ixa9kyx5KqBn4ThycCUKxWN6o2fM0kQ6rxt1KBQeRKuGtSbuM7RM-8cfh2zB4CrHJ_Xsunj8k3O27212Nerlf4UtA5OKCpAz2XjSPcG6OxGhxe6NArua8Sobb4Un7FrRvgxLsQhwt4CGV3GFSkGSorJ_S3SRlPWFD2VgW8VMvAyqPenJAVXh6AQWA45opLwFqsVB49_XVUv-9cRS9d1oTbIG06hJ2-b3Adew2vX6HOckNoaF-hagdtAmE-aTslmgG38GF4hwbofHCbUALdsDJz728mXsJiTKqHL-DOEIdO1m2gn7W9MXx57wVy-Wu5xJSFhd7kNfMlR2MlpdV6Q=s64",
      "userId": "18357331370165036741"
     },
     "user_tz": -180
    },
    "id": "DHy20Vzv7gfV",
    "ExecuteTime": {
     "end_time": "2024-05-31T23:06:06.133432Z",
     "start_time": "2024-05-31T23:06:06.120545Z"
    }
   },
   "outputs": [],
   "source": [
    "# VALIDATION_FIELD[func] get_transforms\n",
    "\n",
    "def get_transforms(data_subset):\n",
    "    \"\"\"\n",
    "    Generates composed set of transformations for a given dataset split.\n",
    "    \n",
    "    Arguments:\n",
    "    data_subset -- string, one of ['train', 'test', 'valid'] \n",
    "    \n",
    "    Return:\n",
    "    transformations -- transforms.Compose, a set of transformations\n",
    "    \"\"\"\n",
    "    import PIL\n",
    "    if data_subset == 'train':\n",
    "        transformations = transforms.Compose([\n",
    "            ### START CODE HERE ### (≈5 lines of code)\n",
    "            transforms.Resize(size=(Config.resize_to, Config.resize_to), interpolation=PIL.Image.BILINEAR),\n",
    "            transforms.RandomResizedCrop(size=(Config.img_size, Config.img_size)),\n",
    "            transforms.RandomHorizontalFlip(p=0.5),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=Config.mean, std=Config.std)\n",
    "            ### END CODE HERE ### \n",
    "        ])\n",
    "    else:\n",
    "        ### START CODE HERE ### (≈4 lines of code)\n",
    "        transformations = transforms.Compose([\n",
    "            transforms.Resize(size=(Config.img_size, Config.img_size)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=Config.mean, std=Config.std)\n",
    "        ])\n",
    "        ### END CODE HERE ###\n",
    "    return transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 238,
     "status": "ok",
     "timestamp": 1627471786830,
     "user": {
      "displayName": "Andrii Torchylo",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhFTWqrU3m-Wp-bUa2OXsooRztUv4cznKjeaEV8YFFpjQygmUGowgZHBW_vtX1MkHaFbXTu_X-7ofPcehpOxdMB6ioGBAESNXPxl7QQUA8tcWsv5CQWVjgOO7zGpy1OYlyhSVCow7xySYmBEb8o-mMBuzgfyBX2d8ZtdTGkIx7YyCCajdzbk0kgCAGFGPxaVtx--pJO4X5PnsLoWquOZ_VlruRtEZ7p7gsSzzwBlhPII6o5J-MOL1TWKl-0kDtS8_kPG1fGuwKmVjY2nXN2oewFaAKc9ajLNHKBCfwQu_nJ0KT4sHWfBIGjebKBa0fk_Xb0DdXgz8eh33PNWk8unTN3862OyaqzPbE4jXgYGRW1WELosWHw59Vkvm9Hdr89I_XP6eHd1-L46_kdP84LAXF9q5nxnFFNCJmLRYSm0Wti3Dn8uc6Ixa9kyx5KqBn4ThycCUKxWN6o2fM0kQ6rxt1KBQeRKuGtSbuM7RM-8cfh2zB4CrHJ_Xsunj8k3O27212Nerlf4UtA5OKCpAz2XjSPcG6OxGhxe6NArua8Sobb4Un7FrRvgxLsQhwt4CGV3GFSkGSorJ_S3SRlPWFD2VgW8VMvAyqPenJAVXh6AQWA45opLwFqsVB49_XVUv-9cRS9d1oTbIG06hJ2-b3Adew2vX6HOckNoaF-hagdtAmE-aTslmgG38GF4hwbofHCbUALdsDJz728mXsJiTKqHL-DOEIdO1m2gn7W9MXx57wVy-Wu5xJSFhd7kNfMlR2MlpdV6Q=s64",
      "userId": "18357331370165036741"
     },
     "user_tz": -180
    },
    "id": "XJpPyA6S7gfV",
    "outputId": "492e1b6c-1315-42e4-f95b-160645652462",
    "ExecuteTime": {
     "end_time": "2024-05-31T23:06:07.011788Z",
     "start_time": "2024-05-31T23:06:06.996809Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train transforms: Compose(\n",
      "    Resize(size=(256, 256), interpolation=bilinear, max_size=None, antialias=True)\n",
      "    RandomResizedCrop(size=(224, 224), scale=(0.08, 1.0), ratio=(0.75, 1.3333), interpolation=bilinear, antialias=True)\n",
      "    RandomHorizontalFlip(p=0.5)\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "Test transforms: Compose(\n",
      "    Resize(size=(224, 224), interpolation=bilinear, max_size=None, antialias=True)\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print('Train transforms:', get_transforms('train'))\n",
    "print('Test transforms:', get_transforms('test'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uMpgXdac7gfW"
   },
   "source": [
    "**Expected Output: (some default parameters might differ depending on the version of PyTorch)**\n",
    "\n",
    "```\n",
    "Train transforms: Compose(\n",
    "    Resize(size=(256, 256), interpolation=PIL.Image.BILINEAR)\n",
    "    RandomResizedCrop(size=(224, 224), scale=(0.08, 1.0), ratio=(0.75, 1.3333), interpolation=PIL.Image.BILINEAR)\n",
    "    RandomHorizontalFlip(p=0.5)\n",
    "    ToTensor()\n",
    "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
    ")\n",
    "Test transforms: Compose(\n",
    "    Resize(size=(224, 224), interpolation=PIL.Image.BILINEAR)\n",
    "    ToTensor()\n",
    "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qtv1HaQa7gfW"
   },
   "source": [
    "## Dataset and Dataloader\n",
    "\n",
    "Every PyTorch Dataset inherits from a base **Dataset** class, and implements three methods `__init__`, `__len__`, and `__getitem__`. By definition, the dataset doesn't store every example in the memory; instead, it only saves the meta-information, like image path, about how to extract a data sample. Then, the `__getitem__` method extracts the data sample (image in our case) by id. In this lab, we will use `torchvision.ImageFolder` dataset that implements image loading from a folder, and pass our `transforms` as a parameter. In future labs, we will implement our own datasets, so stay tuned! <br><br>\n",
    "To load images in batches, we use **DataLoader** class that performs batching operation on the given dataset. In our case, we will use `shuffle=True` to shuffle the examples and indicate `num_workers=Config.num_workers` to set the number of CPU cores that would work in parallel on extracting images from the dataset (standard Google Colab has only 2 CPU cores). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 240,
     "status": "ok",
     "timestamp": 1627471812476,
     "user": {
      "displayName": "Andrii Torchylo",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhFTWqrU3m-Wp-bUa2OXsooRztUv4cznKjeaEV8YFFpjQygmUGowgZHBW_vtX1MkHaFbXTu_X-7ofPcehpOxdMB6ioGBAESNXPxl7QQUA8tcWsv5CQWVjgOO7zGpy1OYlyhSVCow7xySYmBEb8o-mMBuzgfyBX2d8ZtdTGkIx7YyCCajdzbk0kgCAGFGPxaVtx--pJO4X5PnsLoWquOZ_VlruRtEZ7p7gsSzzwBlhPII6o5J-MOL1TWKl-0kDtS8_kPG1fGuwKmVjY2nXN2oewFaAKc9ajLNHKBCfwQu_nJ0KT4sHWfBIGjebKBa0fk_Xb0DdXgz8eh33PNWk8unTN3862OyaqzPbE4jXgYGRW1WELosWHw59Vkvm9Hdr89I_XP6eHd1-L46_kdP84LAXF9q5nxnFFNCJmLRYSm0Wti3Dn8uc6Ixa9kyx5KqBn4ThycCUKxWN6o2fM0kQ6rxt1KBQeRKuGtSbuM7RM-8cfh2zB4CrHJ_Xsunj8k3O27212Nerlf4UtA5OKCpAz2XjSPcG6OxGhxe6NArua8Sobb4Un7FrRvgxLsQhwt4CGV3GFSkGSorJ_S3SRlPWFD2VgW8VMvAyqPenJAVXh6AQWA45opLwFqsVB49_XVUv-9cRS9d1oTbIG06hJ2-b3Adew2vX6HOckNoaF-hagdtAmE-aTslmgG38GF4hwbofHCbUALdsDJz728mXsJiTKqHL-DOEIdO1m2gn7W9MXx57wVy-Wu5xJSFhd7kNfMlR2MlpdV6Q=s64",
      "userId": "18357331370165036741"
     },
     "user_tz": -180
    },
    "id": "Xz1kpmgOf1by"
   },
   "outputs": [],
   "source": [
    "dataloaders = {}\n",
    "for name in ['train', 'valid', 'test']:\n",
    "    dataset = ImageFolder(name, transform=get_transforms(name))\n",
    "    dataloaders[name] = DataLoader(dataset, shuffle=True, \n",
    "                                   batch_size=Config.batch_size,\n",
    "                                   num_workers=Config.num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aFyUTuCy725q"
   },
   "source": [
    "## Show batches of data\n",
    "Let's visualize images that we feed into our neural network!<br>\n",
    "For data visualization, we will use the `matplotlib` library.\n",
    "But if we try to pass the tensor image to `plt.imshow()`,  we will get an error because `matplotlib` is using a different image format. For torch.Tensor — $[C, H, W]$ is used, but for `plt.imshow()` $[H, W, C]$ is required, where: $C$ is the number of channels, $H$ is the height,  and $W$ is the width of an image.\n",
    "\n",
    "Also, our images are normalized, so we have to renormalize them back using the initial mean and standard deviation to get the original images.\n",
    "Here is our normalization formula:\n",
    "$$X_{new}= \\frac{X - \\mu}{\\sigma}$$\n",
    "So, to get the original image, we have to perform the following:\n",
    "$$X= X _{new} * \\sigma + \\mu$$\n",
    "\n",
    "**Excercise:** Convert image to a displayable format. \n",
    "To do so, you will have to implement the following:\n",
    "1. Convert `torch.Tensor` to `np.array`\n",
    "2. Change $[C, H, W]$ to $[H, W, C]$ format\n",
    "3. Renormalize the image using `Config.mean` and `Config.std`\n",
    "4. Clip image to $[0, 1]$ range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 234,
     "status": "ok",
     "timestamp": 1627471817234,
     "user": {
      "displayName": "Andrii Torchylo",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhFTWqrU3m-Wp-bUa2OXsooRztUv4cznKjeaEV8YFFpjQygmUGowgZHBW_vtX1MkHaFbXTu_X-7ofPcehpOxdMB6ioGBAESNXPxl7QQUA8tcWsv5CQWVjgOO7zGpy1OYlyhSVCow7xySYmBEb8o-mMBuzgfyBX2d8ZtdTGkIx7YyCCajdzbk0kgCAGFGPxaVtx--pJO4X5PnsLoWquOZ_VlruRtEZ7p7gsSzzwBlhPII6o5J-MOL1TWKl-0kDtS8_kPG1fGuwKmVjY2nXN2oewFaAKc9ajLNHKBCfwQu_nJ0KT4sHWfBIGjebKBa0fk_Xb0DdXgz8eh33PNWk8unTN3862OyaqzPbE4jXgYGRW1WELosWHw59Vkvm9Hdr89I_XP6eHd1-L46_kdP84LAXF9q5nxnFFNCJmLRYSm0Wti3Dn8uc6Ixa9kyx5KqBn4ThycCUKxWN6o2fM0kQ6rxt1KBQeRKuGtSbuM7RM-8cfh2zB4CrHJ_Xsunj8k3O27212Nerlf4UtA5OKCpAz2XjSPcG6OxGhxe6NArua8Sobb4Un7FrRvgxLsQhwt4CGV3GFSkGSorJ_S3SRlPWFD2VgW8VMvAyqPenJAVXh6AQWA45opLwFqsVB49_XVUv-9cRS9d1oTbIG06hJ2-b3Adew2vX6HOckNoaF-hagdtAmE-aTslmgG38GF4hwbofHCbUALdsDJz728mXsJiTKqHL-DOEIdO1m2gn7W9MXx57wVy-Wu5xJSFhd7kNfMlR2MlpdV6Q=s64",
      "userId": "18357331370165036741"
     },
     "user_tz": -180
    },
    "id": "7kr-W57b7gfX"
   },
   "outputs": [],
   "source": [
    "# VALIDATION_FIELD[func] tensor_to_array\n",
    "\n",
    "def tensor_to_array(img, mean = Config.mean, std = Config.std):\n",
    "    ### START CODE HERE ### (≈4 lines of code)\n",
    "    np_img = img.cpu().numpy().transpose(1, 2, 0)\n",
    "    np_img = np_img * std + mean\n",
    "    np_img = np.clip(np_img, 0, 1)\n",
    "    ### END CODE HERE ### \n",
    "    return np_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 260,
     "status": "ok",
     "timestamp": 1627471819316,
     "user": {
      "displayName": "Andrii Torchylo",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhFTWqrU3m-Wp-bUa2OXsooRztUv4cznKjeaEV8YFFpjQygmUGowgZHBW_vtX1MkHaFbXTu_X-7ofPcehpOxdMB6ioGBAESNXPxl7QQUA8tcWsv5CQWVjgOO7zGpy1OYlyhSVCow7xySYmBEb8o-mMBuzgfyBX2d8ZtdTGkIx7YyCCajdzbk0kgCAGFGPxaVtx--pJO4X5PnsLoWquOZ_VlruRtEZ7p7gsSzzwBlhPII6o5J-MOL1TWKl-0kDtS8_kPG1fGuwKmVjY2nXN2oewFaAKc9ajLNHKBCfwQu_nJ0KT4sHWfBIGjebKBa0fk_Xb0DdXgz8eh33PNWk8unTN3862OyaqzPbE4jXgYGRW1WELosWHw59Vkvm9Hdr89I_XP6eHd1-L46_kdP84LAXF9q5nxnFFNCJmLRYSm0Wti3Dn8uc6Ixa9kyx5KqBn4ThycCUKxWN6o2fM0kQ6rxt1KBQeRKuGtSbuM7RM-8cfh2zB4CrHJ_Xsunj8k3O27212Nerlf4UtA5OKCpAz2XjSPcG6OxGhxe6NArua8Sobb4Un7FrRvgxLsQhwt4CGV3GFSkGSorJ_S3SRlPWFD2VgW8VMvAyqPenJAVXh6AQWA45opLwFqsVB49_XVUv-9cRS9d1oTbIG06hJ2-b3Adew2vX6HOckNoaF-hagdtAmE-aTslmgG38GF4hwbofHCbUALdsDJz728mXsJiTKqHL-DOEIdO1m2gn7W9MXx57wVy-Wu5xJSFhd7kNfMlR2MlpdV6Q=s64",
      "userId": "18357331370165036741"
     },
     "user_tz": -180
    },
    "id": "yM8moE6_7gfX",
    "outputId": "eb9e0fb3-a0de-4410-f8fb-95dece848945"
   },
   "outputs": [],
   "source": [
    "img = torch.tensor([[[-0.6254, -1.4061],\n",
    "                     [-0.0780,  0.3829]],\n",
    "                    \n",
    "                    [[ 0.9516,  0.0839],\n",
    "                     [-0.6721,  0.5566]],\n",
    "                    \n",
    "                    [[ 0.8674, -0.5620],\n",
    "                     [ 0.4436, -1.0338]]])\n",
    "print('np_img:', tensor_to_array(img))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DphNkFdm7gfX"
   },
   "source": [
    "**Expected output:**\n",
    "    \n",
    "<table>\n",
    "    <tr>\n",
    "        <td><b>np_img</b></td>\n",
    "        <td>[[[0.3417834  0.6691584  0.601165  ]<br>\n",
    "              [0.16300309 0.4747936  0.27955001]]<br><br>\n",
    "             [[0.467138   0.3054496  0.50581   ]<br>\n",
    "              [0.5726841  0.58067839 0.173395  ]]]\n",
    "        </td>\n",
    "    </tr>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 285,
     "status": "ok",
     "timestamp": 1627471825345,
     "user": {
      "displayName": "Andrii Torchylo",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhFTWqrU3m-Wp-bUa2OXsooRztUv4cznKjeaEV8YFFpjQygmUGowgZHBW_vtX1MkHaFbXTu_X-7ofPcehpOxdMB6ioGBAESNXPxl7QQUA8tcWsv5CQWVjgOO7zGpy1OYlyhSVCow7xySYmBEb8o-mMBuzgfyBX2d8ZtdTGkIx7YyCCajdzbk0kgCAGFGPxaVtx--pJO4X5PnsLoWquOZ_VlruRtEZ7p7gsSzzwBlhPII6o5J-MOL1TWKl-0kDtS8_kPG1fGuwKmVjY2nXN2oewFaAKc9ajLNHKBCfwQu_nJ0KT4sHWfBIGjebKBa0fk_Xb0DdXgz8eh33PNWk8unTN3862OyaqzPbE4jXgYGRW1WELosWHw59Vkvm9Hdr89I_XP6eHd1-L46_kdP84LAXF9q5nxnFFNCJmLRYSm0Wti3Dn8uc6Ixa9kyx5KqBn4ThycCUKxWN6o2fM0kQ6rxt1KBQeRKuGtSbuM7RM-8cfh2zB4CrHJ_Xsunj8k3O27212Nerlf4UtA5OKCpAz2XjSPcG6OxGhxe6NArua8Sobb4Un7FrRvgxLsQhwt4CGV3GFSkGSorJ_S3SRlPWFD2VgW8VMvAyqPenJAVXh6AQWA45opLwFqsVB49_XVUv-9cRS9d1oTbIG06hJ2-b3Adew2vX6HOckNoaF-hagdtAmE-aTslmgG38GF4hwbofHCbUALdsDJz728mXsJiTKqHL-DOEIdO1m2gn7W9MXx57wVy-Wu5xJSFhd7kNfMlR2MlpdV6Q=s64",
      "userId": "18357331370165036741"
     },
     "user_tz": -180
    },
    "id": "EJrjLcrwp9eg"
   },
   "outputs": [],
   "source": [
    "def show_grid(grid, title=None):\n",
    "    \"\"\"\n",
    "    displays a grid of images\n",
    "    \"\"\"\n",
    "    np_grid = tensor_to_array(grid)\n",
    "    \n",
    "    # display\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.axis('off')\n",
    "    plt.imshow(np_grid)\n",
    "    if title is not None:\n",
    "        plt.title(title) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7zUbKaeQ7gfY"
   },
   "source": [
    "Now let's use torchvision's utility `make_grid` function to visualize a batch of data from each dataloader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 527
    },
    "executionInfo": {
     "elapsed": 4995,
     "status": "ok",
     "timestamp": 1627471833081,
     "user": {
      "displayName": "Andrii Torchylo",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhFTWqrU3m-Wp-bUa2OXsooRztUv4cznKjeaEV8YFFpjQygmUGowgZHBW_vtX1MkHaFbXTu_X-7ofPcehpOxdMB6ioGBAESNXPxl7QQUA8tcWsv5CQWVjgOO7zGpy1OYlyhSVCow7xySYmBEb8o-mMBuzgfyBX2d8ZtdTGkIx7YyCCajdzbk0kgCAGFGPxaVtx--pJO4X5PnsLoWquOZ_VlruRtEZ7p7gsSzzwBlhPII6o5J-MOL1TWKl-0kDtS8_kPG1fGuwKmVjY2nXN2oewFaAKc9ajLNHKBCfwQu_nJ0KT4sHWfBIGjebKBa0fk_Xb0DdXgz8eh33PNWk8unTN3862OyaqzPbE4jXgYGRW1WELosWHw59Vkvm9Hdr89I_XP6eHd1-L46_kdP84LAXF9q5nxnFFNCJmLRYSm0Wti3Dn8uc6Ixa9kyx5KqBn4ThycCUKxWN6o2fM0kQ6rxt1KBQeRKuGtSbuM7RM-8cfh2zB4CrHJ_Xsunj8k3O27212Nerlf4UtA5OKCpAz2XjSPcG6OxGhxe6NArua8Sobb4Un7FrRvgxLsQhwt4CGV3GFSkGSorJ_S3SRlPWFD2VgW8VMvAyqPenJAVXh6AQWA45opLwFqsVB49_XVUv-9cRS9d1oTbIG06hJ2-b3Adew2vX6HOckNoaF-hagdtAmE-aTslmgG38GF4hwbofHCbUALdsDJz728mXsJiTKqHL-DOEIdO1m2gn7W9MXx57wVy-Wu5xJSFhd7kNfMlR2MlpdV6Q=s64",
      "userId": "18357331370165036741"
     },
     "user_tz": -180
    },
    "id": "2vt6RPGggRbX",
    "outputId": "46c7f6b3-3fe5-4206-cbd9-14f452f2bb84"
   },
   "outputs": [],
   "source": [
    "for name in dataloaders.keys():\n",
    "    imgs, labels = next(dataloaders[name].__iter__()) \n",
    "    out = make_grid(imgs)\n",
    "    show_grid(out, f'{name} samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - ResNet\n",
    "\n",
    "ResNet was first proposed in the paper [Deep Residual Learning for Image Recognition](https://arxiv.org/pdf/1512.03385.pdf). Its revolutionary idea to use residual **skip connections** allowed researchers to build even deeper networks without vanishing gradient problems during training. This advancement allowed to improve the state-of-the-art on image classification. At the same time, residual connections became a new standard in building a deep neural network in domains outside of computer vision.\n",
    "\n",
    "The original paper proposed several variations of the ResNet with 18, 34, 50, 101, 152 layers, respectively. In ResNet18 model, there were 18 convolutional layers (denoted **3x3** or **7x7**  on the diagram) followed by pooling layers, **pool**, and **ReLU** activation functions. Starting from the second layer, after every two layers, there is a residual skip connection to improve the flow of gradients through the model. The solid residual is a simple addition, while the dotted residuals increase dimensions before adding the inputs. The output layer is a fully connected layer with 1000 output neurons (ImageNet has 1000 classes), which we will replace with our own linear layer with six output neurons (one for each chess piece).\n",
    "\n",
    "\n",
    "<img src=\"https://dru.fra1.digitaloceanspaces.com/DL_pytorch/static/ntbk_images/resnet_network.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JS1DESblSqsd"
   },
   "source": [
    "## Creating the model class\n",
    "\n",
    "Here we define the model class itself. First, we import the model from `torchvision` (using `pretrained=True` to download the weights. Then, we replace the last fully connected layer with our own layer that has `num_classes` outputs. For training a ResNet model, we have two options: we can \"freeze\" (set `requires_grad` to False) all parameters of a neural network and train only our fully connected layer, or we can train all parameters, which is computationally more expensive but usually gives better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 340,
     "status": "ok",
     "timestamp": 1627471840593,
     "user": {
      "displayName": "Andrii Torchylo",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhFTWqrU3m-Wp-bUa2OXsooRztUv4cznKjeaEV8YFFpjQygmUGowgZHBW_vtX1MkHaFbXTu_X-7ofPcehpOxdMB6ioGBAESNXPxl7QQUA8tcWsv5CQWVjgOO7zGpy1OYlyhSVCow7xySYmBEb8o-mMBuzgfyBX2d8ZtdTGkIx7YyCCajdzbk0kgCAGFGPxaVtx--pJO4X5PnsLoWquOZ_VlruRtEZ7p7gsSzzwBlhPII6o5J-MOL1TWKl-0kDtS8_kPG1fGuwKmVjY2nXN2oewFaAKc9ajLNHKBCfwQu_nJ0KT4sHWfBIGjebKBa0fk_Xb0DdXgz8eh33PNWk8unTN3862OyaqzPbE4jXgYGRW1WELosWHw59Vkvm9Hdr89I_XP6eHd1-L46_kdP84LAXF9q5nxnFFNCJmLRYSm0Wti3Dn8uc6Ixa9kyx5KqBn4ThycCUKxWN6o2fM0kQ6rxt1KBQeRKuGtSbuM7RM-8cfh2zB4CrHJ_Xsunj8k3O27212Nerlf4UtA5OKCpAz2XjSPcG6OxGhxe6NArua8Sobb4Un7FrRvgxLsQhwt4CGV3GFSkGSorJ_S3SRlPWFD2VgW8VMvAyqPenJAVXh6AQWA45opLwFqsVB49_XVUv-9cRS9d1oTbIG06hJ2-b3Adew2vX6HOckNoaF-hagdtAmE-aTslmgG38GF4hwbofHCbUALdsDJz728mXsJiTKqHL-DOEIdO1m2gn7W9MXx57wVy-Wu5xJSFhd7kNfMlR2MlpdV6Q=s64",
      "userId": "18357331370165036741"
     },
     "user_tz": -180
    },
    "id": "HgaqP6gEHeYi"
   },
   "outputs": [],
   "source": [
    "# VALIDATION_FIELD[cls] ResNet\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, freeze=True):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.resnet = torchvision.models.resnet18(pretrained=True)\n",
    "        if freeze:\n",
    "            self.freeze_resnet()\n",
    "        output_features = self.resnet.fc.in_features\n",
    "        self.resnet.fc = nn.Linear(output_features, Config.num_classes)\n",
    "\n",
    "    def freeze_resnet(self):\n",
    "        for param in self.resnet.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.resnet(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XPthQGFk8Mmw"
   },
   "source": [
    "# 4 - Training the model\n",
    "\n",
    "We will make few important modifications to our regular training loop:\n",
    "1. Callbacker dictionary that saves metrics and losses \n",
    "2. Dataloaders instead of passing all input data directly to the function. \n",
    "3. Evaluation loop every epoch to check our model performance on the validation set.\n",
    "4. Save our model every time validation loss decreases.\n",
    "5. To count the average loss, we will multiply loss at each iteration by the size of the batch, add it to `train_loss`, and then divide it by a number of examples in a dataset, `len(dataloader.sampler)`\n",
    "\n",
    "**Excercise:** Implement parts of the train and evaluation loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 243,
     "status": "ok",
     "timestamp": 1627472742419,
     "user": {
      "displayName": "Andrii Torchylo",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhFTWqrU3m-Wp-bUa2OXsooRztUv4cznKjeaEV8YFFpjQygmUGowgZHBW_vtX1MkHaFbXTu_X-7ofPcehpOxdMB6ioGBAESNXPxl7QQUA8tcWsv5CQWVjgOO7zGpy1OYlyhSVCow7xySYmBEb8o-mMBuzgfyBX2d8ZtdTGkIx7YyCCajdzbk0kgCAGFGPxaVtx--pJO4X5PnsLoWquOZ_VlruRtEZ7p7gsSzzwBlhPII6o5J-MOL1TWKl-0kDtS8_kPG1fGuwKmVjY2nXN2oewFaAKc9ajLNHKBCfwQu_nJ0KT4sHWfBIGjebKBa0fk_Xb0DdXgz8eh33PNWk8unTN3862OyaqzPbE4jXgYGRW1WELosWHw59Vkvm9Hdr89I_XP6eHd1-L46_kdP84LAXF9q5nxnFFNCJmLRYSm0Wti3Dn8uc6Ixa9kyx5KqBn4ThycCUKxWN6o2fM0kQ6rxt1KBQeRKuGtSbuM7RM-8cfh2zB4CrHJ_Xsunj8k3O27212Nerlf4UtA5OKCpAz2XjSPcG6OxGhxe6NArua8Sobb4Un7FrRvgxLsQhwt4CGV3GFSkGSorJ_S3SRlPWFD2VgW8VMvAyqPenJAVXh6AQWA45opLwFqsVB49_XVUv-9cRS9d1oTbIG06hJ2-b3Adew2vX6HOckNoaF-hagdtAmE-aTslmgG38GF4hwbofHCbUALdsDJz728mXsJiTKqHL-DOEIdO1m2gn7W9MXx57wVy-Wu5xJSFhd7kNfMlR2MlpdV6Q=s64",
      "userId": "18357331370165036741"
     },
     "user_tz": -180
    },
    "id": "ozO3vOp30gET"
   },
   "outputs": [],
   "source": [
    "# VALIDATION_FIELD[func] train\n",
    "\n",
    "def train(model, optimizer, criterion, train_loader, valid_loader,\n",
    "          num_epochs=Config.epochs, valid_loss_min=np.inf, device=Config.device):\n",
    "    callbackers = {}\n",
    "    callbackers['train_loss'] = []\n",
    "    callbackers['valid_loss'] = []\n",
    "    callbackers['accuracy'] = []\n",
    "    callbackers['f1_macro'] = []\n",
    "\n",
    "    for e in range(num_epochs):\n",
    "        train_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "        ###################\n",
    "        # train the model #\n",
    "        ###################\n",
    "        model.train()\n",
    "        for imgs, labels in train_loader:\n",
    "            ### START CODE HERE ### \n",
    "            # pass the data to the device (≈2 lines of code)\n",
    "            imgs = imgs.clone().detach().to(device)\n",
    "            labels = labels.clone().detach().to(device)\n",
    "            # forward (≈2 lines of code)\n",
    "            out = model.forward(imgs)\n",
    "            loss = criterion(out, labels)\n",
    "            # optimize (≈3 lines of code)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            ### END CODE HERE ### \n",
    "            train_loss += loss.item()*imgs.shape[0]\n",
    "        ######################\n",
    "        # evaluate the model #\n",
    "        ######################\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            targets, predictions = [], []\n",
    "            for imgs, labels in valid_loader:\n",
    "                ### START CODE HERE ### (≈ 6 lines of code)\n",
    "                imgs = imgs.clone().detach().to(device)\n",
    "                labels = labels.clone().detach().to(device)\n",
    "                out = model.forward(imgs)\n",
    "                loss = criterion(out, labels)\n",
    "                # convert prediction and targets to numpy\n",
    "                prediction = out.argmax(dim=1)\n",
    "                target = labels.clone().detach()\n",
    "                ### END CODE HERE ### \n",
    "                predictions.extend(prediction)\n",
    "                targets.extend(target)\n",
    "                valid_loss += loss.item()*imgs.shape[0]\n",
    "\n",
    "        # calculate metrics\n",
    "        train_loss /= len(train_loader.sampler)\n",
    "        valid_loss /= len(valid_loader.sampler)\n",
    "        acc = accuracy_score(predictions, targets)\n",
    "        f1_macro = f1_score(predictions, targets, average='macro')\n",
    "\n",
    "        # print values\n",
    "        print(f'Epoch: {e+1} TrainLoss: {train_loss:.3f} ValidLoss: {valid_loss:.3f} ACC: {acc:.3f} F1_MACRO: {f1_macro}')\n",
    "        \n",
    "        # save metrics\n",
    "        callbackers['train_loss'].append(train_loss)\n",
    "        callbackers['valid_loss'].append(valid_loss)\n",
    "        callbackers['accuracy'].append(acc)\n",
    "        callbackers['f1_macro'].append(f1_macro)\n",
    "\n",
    "        ######################\n",
    "        #   save the model   #\n",
    "        ######################\n",
    "        if valid_loss <= valid_loss_min:\n",
    "            torch.save(model.state_dict(), 'ResNet.pt')\n",
    "            valid_loss_min = valid_loss\n",
    "        \n",
    "    return callbackers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1627472744233,
     "user": {
      "displayName": "Andrii Torchylo",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhFTWqrU3m-Wp-bUa2OXsooRztUv4cznKjeaEV8YFFpjQygmUGowgZHBW_vtX1MkHaFbXTu_X-7ofPcehpOxdMB6ioGBAESNXPxl7QQUA8tcWsv5CQWVjgOO7zGpy1OYlyhSVCow7xySYmBEb8o-mMBuzgfyBX2d8ZtdTGkIx7YyCCajdzbk0kgCAGFGPxaVtx--pJO4X5PnsLoWquOZ_VlruRtEZ7p7gsSzzwBlhPII6o5J-MOL1TWKl-0kDtS8_kPG1fGuwKmVjY2nXN2oewFaAKc9ajLNHKBCfwQu_nJ0KT4sHWfBIGjebKBa0fk_Xb0DdXgz8eh33PNWk8unTN3862OyaqzPbE4jXgYGRW1WELosWHw59Vkvm9Hdr89I_XP6eHd1-L46_kdP84LAXF9q5nxnFFNCJmLRYSm0Wti3Dn8uc6Ixa9kyx5KqBn4ThycCUKxWN6o2fM0kQ6rxt1KBQeRKuGtSbuM7RM-8cfh2zB4CrHJ_Xsunj8k3O27212Nerlf4UtA5OKCpAz2XjSPcG6OxGhxe6NArua8Sobb4Un7FrRvgxLsQhwt4CGV3GFSkGSorJ_S3SRlPWFD2VgW8VMvAyqPenJAVXh6AQWA45opLwFqsVB49_XVUv-9cRS9d1oTbIG06hJ2-b3Adew2vX6HOckNoaF-hagdtAmE-aTslmgG38GF4hwbofHCbUALdsDJz728mXsJiTKqHL-DOEIdO1m2gn7W9MXx57wVy-Wu5xJSFhd7kNfMlR2MlpdV6Q=s64",
      "userId": "18357331370165036741"
     },
     "user_tz": -180
    },
    "id": "JD2r85IPQiwz"
   },
   "outputs": [],
   "source": [
    "def visualize_callbackers(callbackers):\n",
    "    fig, ax =  plt.subplots(2, 1, figsize=(10, 6))\n",
    "    ax[0].plot(callbackers['train_loss'], label='Train Loss')\n",
    "    ax[0].plot(callbackers['valid_loss'], label='Valid Loss')\n",
    "    ax[0].set_title('Losses')\n",
    "    ax[0].legend()\n",
    "    ax[1].plot(callbackers['accuracy'], label='Accuracy')\n",
    "    ax[1].plot(callbackers['f1_macro'], label='F1-macro')\n",
    "    ax[1].set_title('Classification metrics')\n",
    "    ax[1].legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "htpy9nuEg0Lo"
   },
   "source": [
    "# 5 - Evaluating the model\n",
    "\n",
    "Let's also implement our regular evaluation loop that will run the model evaluation on the test set <br>\n",
    "**Excercise:** implement function `evaluate`\n",
    "\n",
    "**Hints:**\n",
    "* To effectively compute predictions consider using `torch.argmax`\n",
    "* Don't forget to call `cpu()` and `numpy()` when appropriate.\n",
    "* Make sure that targets and predictions in the output of the function is a list of integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 312,
     "status": "ok",
     "timestamp": 1627472746787,
     "user": {
      "displayName": "Andrii Torchylo",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhFTWqrU3m-Wp-bUa2OXsooRztUv4cznKjeaEV8YFFpjQygmUGowgZHBW_vtX1MkHaFbXTu_X-7ofPcehpOxdMB6ioGBAESNXPxl7QQUA8tcWsv5CQWVjgOO7zGpy1OYlyhSVCow7xySYmBEb8o-mMBuzgfyBX2d8ZtdTGkIx7YyCCajdzbk0kgCAGFGPxaVtx--pJO4X5PnsLoWquOZ_VlruRtEZ7p7gsSzzwBlhPII6o5J-MOL1TWKl-0kDtS8_kPG1fGuwKmVjY2nXN2oewFaAKc9ajLNHKBCfwQu_nJ0KT4sHWfBIGjebKBa0fk_Xb0DdXgz8eh33PNWk8unTN3862OyaqzPbE4jXgYGRW1WELosWHw59Vkvm9Hdr89I_XP6eHd1-L46_kdP84LAXF9q5nxnFFNCJmLRYSm0Wti3Dn8uc6Ixa9kyx5KqBn4ThycCUKxWN6o2fM0kQ6rxt1KBQeRKuGtSbuM7RM-8cfh2zB4CrHJ_Xsunj8k3O27212Nerlf4UtA5OKCpAz2XjSPcG6OxGhxe6NArua8Sobb4Un7FrRvgxLsQhwt4CGV3GFSkGSorJ_S3SRlPWFD2VgW8VMvAyqPenJAVXh6AQWA45opLwFqsVB49_XVUv-9cRS9d1oTbIG06hJ2-b3Adew2vX6HOckNoaF-hagdtAmE-aTslmgG38GF4hwbofHCbUALdsDJz728mXsJiTKqHL-DOEIdO1m2gn7W9MXx57wVy-Wu5xJSFhd7kNfMlR2MlpdV6Q=s64",
      "userId": "18357331370165036741"
     },
     "user_tz": -180
    },
    "id": "rktCxbwhgujD"
   },
   "outputs": [],
   "source": [
    "# VALIDATION_FIELD[func] evaluate\n",
    "\n",
    "def evaluate(model, criterion, test_loader, device=Config.device): \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        targets, predictions = [], []\n",
    "        overall_loss = 0\n",
    "        for imgs, labels in test_loader:\n",
    "            ### START CODE HERE ### (≈6 lines of code)\n",
    "            imgs = imgs.clone().detach().to(device)\n",
    "            labels = labels.clone().detach().to(device)\n",
    "            out = model.forward(imgs)\n",
    "            loss = criterion(out, labels)\n",
    "            prediction = out.argmax(dim=1)\n",
    "            target = labels.clone().detach()\n",
    "            ### END CODE HERE ### \n",
    "            targets.extend(target)\n",
    "            predictions.extend(prediction)\n",
    "            overall_loss += loss.item()*imgs.shape[0]\n",
    "        overall_loss /= len(test_loader.sampler)\n",
    "        accuracy = accuracy_score(targets, predictions)\n",
    "        f1_macro = f1_score(targets, predictions, average='macro')\n",
    "        return {'targets': targets,\n",
    "                'predictions': predictions,\n",
    "                'accuracy': accuracy,\n",
    "                'f1_macro': f1_macro,\n",
    "                'loss': overall_loss\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1627472748021,
     "user": {
      "displayName": "Andrii Torchylo",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhFTWqrU3m-Wp-bUa2OXsooRztUv4cznKjeaEV8YFFpjQygmUGowgZHBW_vtX1MkHaFbXTu_X-7ofPcehpOxdMB6ioGBAESNXPxl7QQUA8tcWsv5CQWVjgOO7zGpy1OYlyhSVCow7xySYmBEb8o-mMBuzgfyBX2d8ZtdTGkIx7YyCCajdzbk0kgCAGFGPxaVtx--pJO4X5PnsLoWquOZ_VlruRtEZ7p7gsSzzwBlhPII6o5J-MOL1TWKl-0kDtS8_kPG1fGuwKmVjY2nXN2oewFaAKc9ajLNHKBCfwQu_nJ0KT4sHWfBIGjebKBa0fk_Xb0DdXgz8eh33PNWk8unTN3862OyaqzPbE4jXgYGRW1WELosWHw59Vkvm9Hdr89I_XP6eHd1-L46_kdP84LAXF9q5nxnFFNCJmLRYSm0Wti3Dn8uc6Ixa9kyx5KqBn4ThycCUKxWN6o2fM0kQ6rxt1KBQeRKuGtSbuM7RM-8cfh2zB4CrHJ_Xsunj8k3O27212Nerlf4UtA5OKCpAz2XjSPcG6OxGhxe6NArua8Sobb4Un7FrRvgxLsQhwt4CGV3GFSkGSorJ_S3SRlPWFD2VgW8VMvAyqPenJAVXh6AQWA45opLwFqsVB49_XVUv-9cRS9d1oTbIG06hJ2-b3Adew2vX6HOckNoaF-hagdtAmE-aTslmgG38GF4hwbofHCbUALdsDJz728mXsJiTKqHL-DOEIdO1m2gn7W9MXx57wVy-Wu5xJSFhd7kNfMlR2MlpdV6Q=s64",
      "userId": "18357331370165036741"
     },
     "user_tz": -180
    },
    "id": "qndkd0XfhjkO"
   },
   "outputs": [],
   "source": [
    "def draw_confusion(targets, predictions):\n",
    "    confusion = confusion_matrix(targets, predictions)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(confusion, annot=True, cbar=False,\n",
    "                xticklabels=Config.classes,\n",
    "                yticklabels=Config.classes)\n",
    "    plt.ylabel('Predicted class')\n",
    "    plt.xlabel('Actual class')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ev1ERKivofMA"
   },
   "source": [
    "# 6 - \"frozen\" vs. full\n",
    "\n",
    "**Note:** you don't have to wait for the model to finish training to submit the lab. We evaluate your code only in cells where we ask you to implement a funciton or a class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E3LwHk2upi4S"
   },
   "source": [
    "## Training \"frozen\" model\n",
    "\n",
    "Train loss after 30 epochs should be around 0.854."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 266983,
     "status": "ok",
     "timestamp": 1627473026285,
     "user": {
      "displayName": "Andrii Torchylo",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhFTWqrU3m-Wp-bUa2OXsooRztUv4cznKjeaEV8YFFpjQygmUGowgZHBW_vtX1MkHaFbXTu_X-7ofPcehpOxdMB6ioGBAESNXPxl7QQUA8tcWsv5CQWVjgOO7zGpy1OYlyhSVCow7xySYmBEb8o-mMBuzgfyBX2d8ZtdTGkIx7YyCCajdzbk0kgCAGFGPxaVtx--pJO4X5PnsLoWquOZ_VlruRtEZ7p7gsSzzwBlhPII6o5J-MOL1TWKl-0kDtS8_kPG1fGuwKmVjY2nXN2oewFaAKc9ajLNHKBCfwQu_nJ0KT4sHWfBIGjebKBa0fk_Xb0DdXgz8eh33PNWk8unTN3862OyaqzPbE4jXgYGRW1WELosWHw59Vkvm9Hdr89I_XP6eHd1-L46_kdP84LAXF9q5nxnFFNCJmLRYSm0Wti3Dn8uc6Ixa9kyx5KqBn4ThycCUKxWN6o2fM0kQ6rxt1KBQeRKuGtSbuM7RM-8cfh2zB4CrHJ_Xsunj8k3O27212Nerlf4UtA5OKCpAz2XjSPcG6OxGhxe6NArua8Sobb4Un7FrRvgxLsQhwt4CGV3GFSkGSorJ_S3SRlPWFD2VgW8VMvAyqPenJAVXh6AQWA45opLwFqsVB49_XVUv-9cRS9d1oTbIG06hJ2-b3Adew2vX6HOckNoaF-hagdtAmE-aTslmgG38GF4hwbofHCbUALdsDJz728mXsJiTKqHL-DOEIdO1m2gn7W9MXx57wVy-Wu5xJSFhd7kNfMlR2MlpdV6Q=s64",
      "userId": "18357331370165036741"
     },
     "user_tz": -180
    },
    "id": "nq_dZ_o-2-0f",
    "outputId": "b2521c11-c3d3-4165-9ae5-2ce1f3f00a48"
   },
   "outputs": [],
   "source": [
    "set_seed(Config.seed)\n",
    "model = ResNet(freeze=True).to(Config.device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=Config.learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "callbackers = train(model, optimizer, criterion, dataloaders['train'], dataloaders['valid'])\n",
    "visualize_callbackers(callbackers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 439
    },
    "executionInfo": {
     "elapsed": 2843,
     "status": "ok",
     "timestamp": 1627473148421,
     "user": {
      "displayName": "Andrii Torchylo",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhFTWqrU3m-Wp-bUa2OXsooRztUv4cznKjeaEV8YFFpjQygmUGowgZHBW_vtX1MkHaFbXTu_X-7ofPcehpOxdMB6ioGBAESNXPxl7QQUA8tcWsv5CQWVjgOO7zGpy1OYlyhSVCow7xySYmBEb8o-mMBuzgfyBX2d8ZtdTGkIx7YyCCajdzbk0kgCAGFGPxaVtx--pJO4X5PnsLoWquOZ_VlruRtEZ7p7gsSzzwBlhPII6o5J-MOL1TWKl-0kDtS8_kPG1fGuwKmVjY2nXN2oewFaAKc9ajLNHKBCfwQu_nJ0KT4sHWfBIGjebKBa0fk_Xb0DdXgz8eh33PNWk8unTN3862OyaqzPbE4jXgYGRW1WELosWHw59Vkvm9Hdr89I_XP6eHd1-L46_kdP84LAXF9q5nxnFFNCJmLRYSm0Wti3Dn8uc6Ixa9kyx5KqBn4ThycCUKxWN6o2fM0kQ6rxt1KBQeRKuGtSbuM7RM-8cfh2zB4CrHJ_Xsunj8k3O27212Nerlf4UtA5OKCpAz2XjSPcG6OxGhxe6NArua8Sobb4Un7FrRvgxLsQhwt4CGV3GFSkGSorJ_S3SRlPWFD2VgW8VMvAyqPenJAVXh6AQWA45opLwFqsVB49_XVUv-9cRS9d1oTbIG06hJ2-b3Adew2vX6HOckNoaF-hagdtAmE-aTslmgG38GF4hwbofHCbUALdsDJz728mXsJiTKqHL-DOEIdO1m2gn7W9MXx57wVy-Wu5xJSFhd7kNfMlR2MlpdV6Q=s64",
      "userId": "18357331370165036741"
     },
     "user_tz": -180
    },
    "id": "mRsZ-FpniZAn",
    "outputId": "76a4f733-c689-4687-a52c-77f5a6de2f3e"
   },
   "outputs": [],
   "source": [
    "# Load the best model\n",
    "model.load_state_dict(torch.load('ResNet.pt'))\n",
    "results = evaluate(model, criterion, dataloaders['test'])\n",
    "print('Test Loss:', results['loss'])\n",
    "print('Test Accuracy:', results['accuracy'])\n",
    "print('Test F1 Macro:', results['f1_macro'])\n",
    "draw_confusion(results['targets'], results['predictions'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9s6eKlQX9fcz"
   },
   "source": [
    "**Expected output:**\n",
    "    \n",
    "<table>\n",
    "    <tr>\n",
    "        <td><b>Test Loss</b></td>\n",
    "        <td>1.066782832145691\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><b>Test Accuracy</b></td>\n",
    "        <td>0.5833333333333334\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><b>Test F1 Macro</b></td>\n",
    "        <td>0.5721092524836143\n",
    "        </td>\n",
    "    </tr>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cooe13kappxZ"
   },
   "source": [
    "## Training the full model\n",
    "\n",
    "Training loss after 30 epochs should be around 0.628."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 255691,
     "status": "ok",
     "timestamp": 1627473875454,
     "user": {
      "displayName": "Andrii Torchylo",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhFTWqrU3m-Wp-bUa2OXsooRztUv4cznKjeaEV8YFFpjQygmUGowgZHBW_vtX1MkHaFbXTu_X-7ofPcehpOxdMB6ioGBAESNXPxl7QQUA8tcWsv5CQWVjgOO7zGpy1OYlyhSVCow7xySYmBEb8o-mMBuzgfyBX2d8ZtdTGkIx7YyCCajdzbk0kgCAGFGPxaVtx--pJO4X5PnsLoWquOZ_VlruRtEZ7p7gsSzzwBlhPII6o5J-MOL1TWKl-0kDtS8_kPG1fGuwKmVjY2nXN2oewFaAKc9ajLNHKBCfwQu_nJ0KT4sHWfBIGjebKBa0fk_Xb0DdXgz8eh33PNWk8unTN3862OyaqzPbE4jXgYGRW1WELosWHw59Vkvm9Hdr89I_XP6eHd1-L46_kdP84LAXF9q5nxnFFNCJmLRYSm0Wti3Dn8uc6Ixa9kyx5KqBn4ThycCUKxWN6o2fM0kQ6rxt1KBQeRKuGtSbuM7RM-8cfh2zB4CrHJ_Xsunj8k3O27212Nerlf4UtA5OKCpAz2XjSPcG6OxGhxe6NArua8Sobb4Un7FrRvgxLsQhwt4CGV3GFSkGSorJ_S3SRlPWFD2VgW8VMvAyqPenJAVXh6AQWA45opLwFqsVB49_XVUv-9cRS9d1oTbIG06hJ2-b3Adew2vX6HOckNoaF-hagdtAmE-aTslmgG38GF4hwbofHCbUALdsDJz728mXsJiTKqHL-DOEIdO1m2gn7W9MXx57wVy-Wu5xJSFhd7kNfMlR2MlpdV6Q=s64",
      "userId": "18357331370165036741"
     },
     "user_tz": -180
    },
    "id": "xVD-gLMHdL8X",
    "outputId": "02f32e7a-03e4-4579-fdf7-8e9ca68acec7"
   },
   "outputs": [],
   "source": [
    "set_seed(Config.seed)\n",
    "model = ResNet(freeze=False).to(Config.device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=Config.learning_rate)\n",
    "callbackers = train(model, optimizer, criterion, dataloaders['train'], dataloaders['valid'], num_epochs=Config.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 439
    },
    "executionInfo": {
     "elapsed": 2374,
     "status": "ok",
     "timestamp": 1627473936306,
     "user": {
      "displayName": "Andrii Torchylo",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhFTWqrU3m-Wp-bUa2OXsooRztUv4cznKjeaEV8YFFpjQygmUGowgZHBW_vtX1MkHaFbXTu_X-7ofPcehpOxdMB6ioGBAESNXPxl7QQUA8tcWsv5CQWVjgOO7zGpy1OYlyhSVCow7xySYmBEb8o-mMBuzgfyBX2d8ZtdTGkIx7YyCCajdzbk0kgCAGFGPxaVtx--pJO4X5PnsLoWquOZ_VlruRtEZ7p7gsSzzwBlhPII6o5J-MOL1TWKl-0kDtS8_kPG1fGuwKmVjY2nXN2oewFaAKc9ajLNHKBCfwQu_nJ0KT4sHWfBIGjebKBa0fk_Xb0DdXgz8eh33PNWk8unTN3862OyaqzPbE4jXgYGRW1WELosWHw59Vkvm9Hdr89I_XP6eHd1-L46_kdP84LAXF9q5nxnFFNCJmLRYSm0Wti3Dn8uc6Ixa9kyx5KqBn4ThycCUKxWN6o2fM0kQ6rxt1KBQeRKuGtSbuM7RM-8cfh2zB4CrHJ_Xsunj8k3O27212Nerlf4UtA5OKCpAz2XjSPcG6OxGhxe6NArua8Sobb4Un7FrRvgxLsQhwt4CGV3GFSkGSorJ_S3SRlPWFD2VgW8VMvAyqPenJAVXh6AQWA45opLwFqsVB49_XVUv-9cRS9d1oTbIG06hJ2-b3Adew2vX6HOckNoaF-hagdtAmE-aTslmgG38GF4hwbofHCbUALdsDJz728mXsJiTKqHL-DOEIdO1m2gn7W9MXx57wVy-Wu5xJSFhd7kNfMlR2MlpdV6Q=s64",
      "userId": "18357331370165036741"
     },
     "user_tz": -180
    },
    "id": "MOaGhdn4kEBG",
    "outputId": "9ea8cdb4-bf3e-4f70-993a-131c417ebc6d"
   },
   "outputs": [],
   "source": [
    "# Load the best model\n",
    "model.load_state_dict(torch.load('ResNet.pt'))\n",
    "results = evaluate(model, criterion, dataloaders['test'])\n",
    "print('Test Loss:', results['loss'])\n",
    "print('Test Accuracy:', results['accuracy'])\n",
    "print('Test F1 Macro:', results['f1_macro'])\n",
    "draw_confusion(results['targets'], results['predictions'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z51HJX9xAdCs"
   },
   "source": [
    "**Expected output:**\n",
    "    \n",
    "<table>\n",
    "    <tr>\n",
    "        <td><b>Test Loss</b></td>\n",
    "        <td>0.5063939381528784\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><b>Test Accuracy</b></td>\n",
    "        <td>0.8703703703703703\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><b>Test F1 Macro</b></td>\n",
    "        <td>0.8524070691111768\n",
    "        </td>\n",
    "    </tr>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "12lOm9O9k7nl"
   },
   "source": [
    "**Interpretation:** not surprisingly, training a full model gave us a better accuracy than freezing a part of it. This is because training all parameters allows the model to learn task-specific features, including some specific edges of chess pieces in our case. Whereas training a \"frozen\" network, we only create a linear mapping from some features that ResNet learned on ImageNet to classes of our chess pieces, which might be not as effective but computationally faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qjF999lcmL45"
   },
   "source": [
    "# 6 - What to do next\n",
    "\n",
    "1. Submit your lab to `@DRU bot`. Don't worry if your accuracy is not that high; you don't have to wait for the model to finish training to submit the lab.  <br>\n",
    "2. After you submit your lab, try playing with model architecture and hyperparameters. Add metrics to callbackers and see how they change during the training process. Experiment with the loss scaling method, just like we did in the Multi-Class Classification problem in the previous module."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "ResNet_Transfer_Learning_solution.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
