{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "template.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# **TorchScript**\n",
    "\n",
    "In this lab, you will learn how to use TorchScript to optimise inference runtime using examples of Yolov5s and BERT models.\n",
    "\n",
    ">GPU is recomended for this assignment. `Runtime` -> `Change runtime type` -> `GPU`\n",
    "\n",
    "**Instructions**\n",
    "- Write code in the space indicated with `### START CODE HERE ###`\n",
    "- Do not use loops (for/while) unless instructions explicitly tell you so. Parallelization in Deep Learning is key!\n",
    "- If you get stuck, ask for help in Slack or DM `@DRU Team`\n",
    "\n",
    "**You will learn**\n",
    "- How to use tracing or scripting to convert a model to TorchScript\n",
    "- How to measure Inference time of a model"
   ],
   "metadata": {
    "id": "mfR8P8DEkkTf"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **Import packages**"
   ],
   "metadata": {
    "id": "YoylOzlFhmgD"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install yolort==0.6.2\n",
    "!pip install transformers==4.18.0"
   ],
   "metadata": {
    "id": "RsTR4b643m9T",
    "ExecuteTime": {
     "end_time": "2024-08-08T13:28:39.955550Z",
     "start_time": "2024-08-08T13:28:03.617554Z"
    }
   },
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting yolort==0.6.2\n",
      "  Downloading yolort-0.6.2-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: matplotlib>=3.2.2 in c:\\users\\maxog\\pycharmprojects\\dru_labs\\.venv\\lib\\site-packages (from yolort==0.6.2) (3.7.5)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\maxog\\pycharmprojects\\dru_labs\\.venv\\lib\\site-packages (from yolort==0.6.2) (1.24.4)\n",
      "Requirement already satisfied: Pillow>=8.0.0 in c:\\users\\maxog\\pycharmprojects\\dru_labs\\.venv\\lib\\site-packages (from yolort==0.6.2) (10.3.0)\n",
      "Requirement already satisfied: scipy>=1.4.1 in c:\\users\\maxog\\pycharmprojects\\dru_labs\\.venv\\lib\\site-packages (from yolort==0.6.2) (1.10.1)\n",
      "Requirement already satisfied: tqdm>=4.41.0 in c:\\users\\maxog\\pycharmprojects\\dru_labs\\.venv\\lib\\site-packages (from yolort==0.6.2) (4.66.4)\n",
      "Collecting onnx>=1.8.0 (from yolort==0.6.2)\n",
      "  Downloading onnx-1.16.2-cp38-cp38-win_amd64.whl.metadata (16 kB)\n",
      "Requirement already satisfied: ipython in c:\\users\\maxog\\pycharmprojects\\dru_labs\\.venv\\lib\\site-packages (from yolort==0.6.2) (8.12.3)\n",
      "Collecting tabulate (from yolort==0.6.2)\n",
      "  Using cached tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\n",
      "Requirement already satisfied: pandas in c:\\users\\maxog\\pycharmprojects\\dru_labs\\.venv\\lib\\site-packages (from yolort==0.6.2) (2.0.3)\n",
      "Collecting thop (from yolort==0.6.2)\n",
      "  Downloading thop-0.1.1.post2209072238-py3-none-any.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\maxog\\pycharmprojects\\dru_labs\\.venv\\lib\\site-packages (from matplotlib>=3.2.2->yolort==0.6.2) (1.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\maxog\\pycharmprojects\\dru_labs\\.venv\\lib\\site-packages (from matplotlib>=3.2.2->yolort==0.6.2) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\maxog\\pycharmprojects\\dru_labs\\.venv\\lib\\site-packages (from matplotlib>=3.2.2->yolort==0.6.2) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\maxog\\pycharmprojects\\dru_labs\\.venv\\lib\\site-packages (from matplotlib>=3.2.2->yolort==0.6.2) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\maxog\\pycharmprojects\\dru_labs\\.venv\\lib\\site-packages (from matplotlib>=3.2.2->yolort==0.6.2) (24.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\maxog\\pycharmprojects\\dru_labs\\.venv\\lib\\site-packages (from matplotlib>=3.2.2->yolort==0.6.2) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\maxog\\pycharmprojects\\dru_labs\\.venv\\lib\\site-packages (from matplotlib>=3.2.2->yolort==0.6.2) (2.9.0.post0)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in c:\\users\\maxog\\pycharmprojects\\dru_labs\\.venv\\lib\\site-packages (from matplotlib>=3.2.2->yolort==0.6.2) (6.4.0)\n",
      "Requirement already satisfied: protobuf>=3.20.2 in c:\\users\\maxog\\pycharmprojects\\dru_labs\\.venv\\lib\\site-packages (from onnx>=1.8.0->yolort==0.6.2) (5.27.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\maxog\\pycharmprojects\\dru_labs\\.venv\\lib\\site-packages (from tqdm>=4.41.0->yolort==0.6.2) (0.4.6)\n",
      "Requirement already satisfied: backcall in c:\\users\\maxog\\pycharmprojects\\dru_labs\\.venv\\lib\\site-packages (from ipython->yolort==0.6.2) (0.2.0)\n",
      "Requirement already satisfied: decorator in c:\\users\\maxog\\pycharmprojects\\dru_labs\\.venv\\lib\\site-packages (from ipython->yolort==0.6.2) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\maxog\\pycharmprojects\\dru_labs\\.venv\\lib\\site-packages (from ipython->yolort==0.6.2) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\maxog\\pycharmprojects\\dru_labs\\.venv\\lib\\site-packages (from ipython->yolort==0.6.2) (0.1.6)\n",
      "Requirement already satisfied: pickleshare in c:\\users\\maxog\\pycharmprojects\\dru_labs\\.venv\\lib\\site-packages (from ipython->yolort==0.6.2) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in c:\\users\\maxog\\pycharmprojects\\dru_labs\\.venv\\lib\\site-packages (from ipython->yolort==0.6.2) (3.0.43)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\maxog\\pycharmprojects\\dru_labs\\.venv\\lib\\site-packages (from ipython->yolort==0.6.2) (2.17.2)\n",
      "Requirement already satisfied: stack-data in c:\\users\\maxog\\pycharmprojects\\dru_labs\\.venv\\lib\\site-packages (from ipython->yolort==0.6.2) (0.6.3)\n",
      "Requirement already satisfied: traitlets>=5 in c:\\users\\maxog\\pycharmprojects\\dru_labs\\.venv\\lib\\site-packages (from ipython->yolort==0.6.2) (5.14.2)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\maxog\\pycharmprojects\\dru_labs\\.venv\\lib\\site-packages (from ipython->yolort==0.6.2) (4.11.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\maxog\\pycharmprojects\\dru_labs\\.venv\\lib\\site-packages (from pandas->yolort==0.6.2) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\maxog\\pycharmprojects\\dru_labs\\.venv\\lib\\site-packages (from pandas->yolort==0.6.2) (2024.1)\n",
      "Requirement already satisfied: torch in c:\\users\\maxog\\pycharmprojects\\dru_labs\\.venv\\lib\\site-packages (from thop->yolort==0.6.2) (2.3.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in c:\\users\\maxog\\pycharmprojects\\dru_labs\\.venv\\lib\\site-packages (from importlib-resources>=3.2.0->matplotlib>=3.2.2->yolort==0.6.2) (3.18.1)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in c:\\users\\maxog\\pycharmprojects\\dru_labs\\.venv\\lib\\site-packages (from jedi>=0.16->ipython->yolort==0.6.2) (0.8.4)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\maxog\\pycharmprojects\\dru_labs\\.venv\\lib\\site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython->yolort==0.6.2) (0.2.13)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\maxog\\pycharmprojects\\dru_labs\\.venv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib>=3.2.2->yolort==0.6.2) (1.16.0)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\maxog\\pycharmprojects\\dru_labs\\.venv\\lib\\site-packages (from stack-data->ipython->yolort==0.6.2) (2.0.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\maxog\\pycharmprojects\\dru_labs\\.venv\\lib\\site-packages (from stack-data->ipython->yolort==0.6.2) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\maxog\\pycharmprojects\\dru_labs\\.venv\\lib\\site-packages (from stack-data->ipython->yolort==0.6.2) (0.2.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\maxog\\pycharmprojects\\dru_labs\\.venv\\lib\\site-packages (from torch->thop->yolort==0.6.2) (3.14.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\maxog\\pycharmprojects\\dru_labs\\.venv\\lib\\site-packages (from torch->thop->yolort==0.6.2) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\maxog\\pycharmprojects\\dru_labs\\.venv\\lib\\site-packages (from torch->thop->yolort==0.6.2) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\maxog\\pycharmprojects\\dru_labs\\.venv\\lib\\site-packages (from torch->thop->yolort==0.6.2) (3.1.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\maxog\\pycharmprojects\\dru_labs\\.venv\\lib\\site-packages (from torch->thop->yolort==0.6.2) (2024.3.1)\n",
      "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\maxog\\pycharmprojects\\dru_labs\\.venv\\lib\\site-packages (from torch->thop->yolort==0.6.2) (2021.4.0)\n",
      "Requirement already satisfied: intel-openmp==2021.* in c:\\users\\maxog\\pycharmprojects\\dru_labs\\.venv\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch->thop->yolort==0.6.2) (2021.4.0)\n",
      "Requirement already satisfied: tbb==2021.* in c:\\users\\maxog\\pycharmprojects\\dru_labs\\.venv\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch->thop->yolort==0.6.2) (2021.12.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\maxog\\pycharmprojects\\dru_labs\\.venv\\lib\\site-packages (from jinja2->torch->thop->yolort==0.6.2) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\maxog\\pycharmprojects\\dru_labs\\.venv\\lib\\site-packages (from sympy->torch->thop->yolort==0.6.2) (1.3.0)\n",
      "Downloading yolort-0.6.2-py3-none-any.whl (163 kB)\n",
      "   ---------------------------------------- 0.0/163.9 kB ? eta -:--:--\n",
      "   ---------------------------------------  163.8/163.9 kB 5.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 163.9/163.9 kB 3.3 MB/s eta 0:00:00\n",
      "Downloading onnx-1.16.2-cp38-cp38-win_amd64.whl (14.4 MB)\n",
      "   ---------------------------------------- 0.0/14.4 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.7/14.4 MB 15.0 MB/s eta 0:00:01\n",
      "   ----- ---------------------------------- 2.1/14.4 MB 22.7 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 4.3/14.4 MB 30.4 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 6.1/14.4 MB 32.4 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 9.1/14.4 MB 38.6 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 11.1/14.4 MB 43.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  14.4/14.4 MB 54.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  14.4/14.4 MB 50.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 14.4/14.4 MB 43.7 MB/s eta 0:00:00\n",
      "Using cached tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Downloading thop-0.1.1.post2209072238-py3-none-any.whl (15 kB)\n",
      "Installing collected packages: tabulate, onnx, thop, yolort\n",
      "Successfully installed onnx-1.16.2 tabulate-0.9.0 thop-0.1.1.post2209072238 yolort-0.6.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==4.18.0\n",
      "  Downloading transformers-4.18.0-py3-none-any.whl.metadata (70 kB)\n",
      "     ---------------------------------------- 0.0/70.3 kB ? eta -:--:--\n",
      "     ---------------------------------------- 70.3/70.3 kB 1.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: filelock in c:\\users\\maxog\\pycharmprojects\\dru_labs\\.venv\\lib\\site-packages (from transformers==4.18.0) (3.14.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in c:\\users\\maxog\\pycharmprojects\\dru_labs\\.venv\\lib\\site-packages (from transformers==4.18.0) (0.23.4)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\maxog\\pycharmprojects\\dru_labs\\.venv\\lib\\site-packages (from transformers==4.18.0) (1.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\maxog\\pycharmprojects\\dru_labs\\.venv\\lib\\site-packages (from transformers==4.18.0) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\maxog\\pycharmprojects\\dru_labs\\.venv\\lib\\site-packages (from transformers==4.18.0) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\maxog\\pycharmprojects\\dru_labs\\.venv\\lib\\site-packages (from transformers==4.18.0) (2024.5.15)\n",
      "Requirement already satisfied: requests in c:\\users\\maxog\\pycharmprojects\\dru_labs\\.venv\\lib\\site-packages (from transformers==4.18.0) (2.31.0)\n",
      "Requirement already satisfied: sacremoses in c:\\users\\maxog\\pycharmprojects\\dru_labs\\.venv\\lib\\site-packages (from transformers==4.18.0) (0.1.1)\n",
      "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1 (from transformers==4.18.0)\n",
      "  Downloading tokenizers-0.12.1-cp38-cp38-win_amd64.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\maxog\\pycharmprojects\\dru_labs\\.venv\\lib\\site-packages (from transformers==4.18.0) (4.66.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\maxog\\pycharmprojects\\dru_labs\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.18.0) (2024.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\maxog\\pycharmprojects\\dru_labs\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.18.0) (4.11.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\maxog\\pycharmprojects\\dru_labs\\.venv\\lib\\site-packages (from tqdm>=4.27->transformers==4.18.0) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\maxog\\pycharmprojects\\dru_labs\\.venv\\lib\\site-packages (from requests->transformers==4.18.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\maxog\\pycharmprojects\\dru_labs\\.venv\\lib\\site-packages (from requests->transformers==4.18.0) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\maxog\\pycharmprojects\\dru_labs\\.venv\\lib\\site-packages (from requests->transformers==4.18.0) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\maxog\\pycharmprojects\\dru_labs\\.venv\\lib\\site-packages (from requests->transformers==4.18.0) (2024.2.2)\n",
      "Requirement already satisfied: click in c:\\users\\maxog\\pycharmprojects\\dru_labs\\.venv\\lib\\site-packages (from sacremoses->transformers==4.18.0) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\maxog\\pycharmprojects\\dru_labs\\.venv\\lib\\site-packages (from sacremoses->transformers==4.18.0) (1.4.0)\n",
      "Downloading transformers-4.18.0-py3-none-any.whl (4.0 MB)\n",
      "   ---------------------------------------- 0.0/4.0 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.4/4.0 MB 11.6 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 1.3/4.0 MB 13.4 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 2.7/4.0 MB 19.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  4.0/4.0 MB 23.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 4.0/4.0 MB 21.2 MB/s eta 0:00:00\n",
      "Downloading tokenizers-0.12.1-cp38-cp38-win_amd64.whl (3.3 MB)\n",
      "   ---------------------------------------- 0.0/3.3 MB ? eta -:--:--\n",
      "   -------------------------------- ------- 2.6/3.3 MB 56.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 3.3/3.3 MB 42.0 MB/s eta 0:00:00\n",
      "Installing collected packages: tokenizers, transformers\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.19.1\n",
      "    Uninstalling tokenizers-0.19.1:\n",
      "      Successfully uninstalled tokenizers-0.19.1\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.17.0\n",
      "    Uninstalling transformers-4.17.0:\n",
      "      Successfully uninstalled transformers-4.17.0\n",
      "Successfully installed tokenizers-0.12.1 transformers-4.18.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\maxog\\PycharmProjects\\DRU_labs\\.venv\\Lib\\site-packages\\~okenizers'.\n",
      "  You can safely remove it manually.\n",
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import trange\n",
    "\n",
    "from yolort.models import yolov5s\n",
    "from transformers import BertTokenizer, BertModel"
   ],
   "metadata": {
    "id": "zyHg9h8VklD5",
    "ExecuteTime": {
     "end_time": "2024-08-08T13:37:11.223347Z",
     "start_time": "2024-08-08T13:37:00.884014Z"
    }
   },
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Font error: cannot open resource\n",
      "Downloading https://ultralytics.com/assets/Arial.ttf to C:\\Users\\maxog\\AppData\\Roaming\\Ultralytics\\Arial.ttf...\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# VALIDATION_FIELD[cls] Config\n",
    "\n",
    "class Config:\n",
    "\n",
    "    n_imgs = 32\n",
    "    input_shape = (3, 256, 256)\n",
    "    yolo_nwarmup = 20\n",
    "    yolo_nruns = 100\n",
    "\n",
    "    bert_nwarmup = 50\n",
    "    bert_nruns = 500\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ],
   "metadata": {
    "id": "vfKv2QgN5Sz4",
    "ExecuteTime": {
     "end_time": "2024-08-08T13:37:11.239353Z",
     "start_time": "2024-08-08T13:37:11.225347Z"
    }
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **Runtime optimization with TochScript**\n",
    "\n",
    "## **What is TorchScript ?**\n",
    "[TorchScript](https://pytorch.org/docs/stable/jit.html#torchscript-language) is a high-performance subset of Python language, specialised for ML applications. Using TorchScript, you can easily create serialisable and optimisable models from PyTorch code to load into a process with no dependency on Python, such as in a C++ program. It will run your models faster and independent.\n",
    "\n",
    "## **The Script mode and PyTorch JIT**\n",
    "\n",
    "It is a part of the PyTorch focused on the production use case, which has 2 components PyTorch JIT (an optimised compiler) and TorchScript.\n",
    "\n",
    "Script mode creates an intermediate representation (IR) through [`torch.jit.trace`](https://pytorch.org/docs/stable/generated/torch.jit.trace.html) and [`torch.jit.script`](https://pytorch.org/docs/stable/generated/torch.jit.script.html) to represent computation. The IR is internally optimised and utilises PyTorch JIT compilation at runtime.\n",
    "\n",
    "PyTorch JIT is a compiler for PyTorch programs:\n",
    "- It is a lightweight threadsafe interpreter\n",
    "- Supports easy to write custom transformations\n",
    "- It’s not just for inference as it has auto diff support\n",
    "\n",
    "Pytorch provides two methods for generating TorchScript from your model code:\n",
    "\n",
    "- **Tracing**: [`torch.jit.trace`](https://pytorch.org/docs/stable/generated/torch.jit.trace.html) takes a data instance and trained model as input. It runs the model and then records and traces the executed operations performed on all the tensors. This recording is turned into a TorchScript.\n",
    "\n",
    "- **Scripting**: [`torch.jit.script`](https://pytorch.org/docs/stable/generated/torch.jit.script.html) allows writing code directly into TorchScript, which will be generated from the static inspection of the nn.Module contents. So, in contrast to trace mode, you only need to pass an instance of your model to torch.jit.script and a data sample are not necessary.\n",
    "\n",
    "### When to use:\n",
    "\n",
    "- **Tracing**: \n",
    "    - Use `torch.jit.trace` if you are unable to modify the model code. In this case scripting the model simply will not work, because it uses unsupported Pytorch/Python functionality.\n",
    "    - You may use the logic [freezing](https://pytorch.org/tutorials/prototype/torchscript_freezing.html) behaviour of tracing if you need to gain better performance or to make changes in architectural decisions.\n",
    "\n",
    "- **Scripting**:\n",
    "    - `torch.jit.script` is easy to use because it captures both your model's operations and full conditional logic. An export is likely to either fail for a well-defined reason, which you can solve by implying a clear code modification or even succeed without warnings.\n",
    "\n",
    "Scripted and traced code can be mixed. You can see the existing [documentation](https://pytorch.org/docs/stable/jit.html#mixing-tracing-and-scripting) for details and examples.\n"
   ],
   "metadata": {
    "id": "PpGFg8LhTRBp"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **Yolov5s inference optimising**"
   ],
   "metadata": {
    "id": "-f4zbrPSghkh"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **Load yolov5s model and scripting it**\n",
    "\n",
    "We will use yolov5s from the [yolort](https://github.com/zhiqwang/yolov5-rt-stack) library, trained on the COCO dataset, to test speed inference time."
   ],
   "metadata": {
    "id": "qjUFmgevvvjf"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# VALIDATION_FIELD[func] native_yolov5s_model\n",
    "\n",
    "native_yolov5s_model = yolov5s(pretrained=True, size=(256, 256))"
   ],
   "metadata": {
    "id": "iDNiHM1AqX3c",
    "ExecuteTime": {
     "end_time": "2024-08-08T13:37:13.272732Z",
     "start_time": "2024-08-08T13:37:11.240355Z"
    }
   },
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/zhiqwang/yolov5-rt-stack/releases/download/v0.5.2-alpha/yolov5_darknet_pan_s_r60_coco-9f44bf3f.pt\" to C:\\Users\\maxog/.cache\\torch\\hub\\checkpoints\\yolov5_darknet_pan_s_r60_coco-9f44bf3f.pt\n",
      "100%|██████████| 14.0M/14.0M [00:00<00:00, 21.0MB/s]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Excercise**: create the script of yolov5s model, passing an instance of the model to `torch.jit.script`:"
   ],
   "metadata": {
    "id": "E1-HAKtQMOl1"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# VALIDATION_FIELD[func] scripted_yolov5s_model\n",
    "\n",
    "### START CODE HERE ### (1 line of code)\n",
    "scripted_yolov5s_model = torch.jit.script(native_yolov5s_model)\n",
    "### END CODE HERE ###"
   ],
   "metadata": {
    "id": "xFKqY0XE5WXE",
    "ExecuteTime": {
     "end_time": "2024-08-08T13:37:42.171796Z",
     "start_time": "2024-08-08T13:37:41.121461Z"
    }
   },
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "For clarity, let's save the scripted model and load it."
   ],
   "metadata": {
    "id": "IZ2WzYlTN-8i"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "torch.jit.save(scripted_yolov5s_model,'scripted_yolov5s.pt')\n",
    "loaded_scripted_yolov5s_model = torch.jit.load('scripted_yolov5s.pt')"
   ],
   "metadata": {
    "id": "5n_8P4jn5WZw",
    "ExecuteTime": {
     "end_time": "2024-08-08T13:37:44.681130Z",
     "start_time": "2024-08-08T13:37:44.358257Z"
    }
   },
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **Benchmarking native and scripted Yolov5s model**"
   ],
   "metadata": {
    "id": "o2_559cbv8rd"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Before making inference time measurements, we need to run some dummy examples through the model to do a GPU warm-up. Warm-up will initialise the GPU and prevent it from going into a power-saving mode.\n",
    "\n",
    "Next, we need to use [`torch.cuda.event`](https://pytorch.org/docs/stable/generated/torch.cuda.Event.html) and init loggers to measure time on the GPU.\n",
    "\n",
    "Finally, after the model is warmed-up, we can measure performance. To do this correctly, we need to use [`torch.cuda.synchronize()`](https://alband.github.io/doc_view/cuda.html#torch.cuda.synchronize). This will perform synchronisation between the host and device (i.e., GPU and CPU), so the time recording takes place only after the process running on the GPU is finished.\n",
    "\n",
    "Below is an implemented function that will perform warming up and the main inference of the model with measuring performance time.\n"
   ],
   "metadata": {
    "id": "n5U3rQ0Iv4re"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# VALIDATION_FIELD[func] run_model\n",
    "\n",
    "def run_model(model, input, nruns, desc=None, unpack=False):\n",
    "    \"\"\"\n",
    "    Runs a model with inputs and measures performance time\n",
    "\n",
    "    Arguments:\n",
    "    model -- a model we will use to test performance\n",
    "    input -- inputs for a model\n",
    "    nruns -- steps to run a model\n",
    "    desc -- description of a progress bar\n",
    "    unpack -- bool indicator. Indicates unpack input or not\n",
    "\n",
    "    Return:\n",
    "    infer_time -- inference time\n",
    "    \"\"\"\n",
    "\n",
    "    pbar = trange(nruns, \n",
    "                  unit=\" runs\",\n",
    "                  desc=desc,\n",
    "                  bar_format='{desc}: {percentage:3.0f}%|{bar}| {n_fmt} run /{total_fmt} runs '\n",
    "                '[{elapsed}<{remaining}, {rate_fmt}{postfix}]')\n",
    "    \n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        # init loggers\n",
    "        star_measure_time, end_measure_time = torch.cuda.Event(enable_timing=True), torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "        torch.cuda.synchronize()\n",
    "        timings=np.zeros((nruns,1))\n",
    "        with torch.no_grad():\n",
    "            for i in pbar:\n",
    "                star_measure_time.record()\n",
    "                if unpack == False:\n",
    "                    _ = model(input)\n",
    "                else:\n",
    "                    _ = model(*input)\n",
    "                end_measure_time.record()\n",
    "                # wait for gpu sync\n",
    "                torch.cuda.synchronize()\n",
    "                curr_time = star_measure_time.elapsed_time(end_measure_time)\n",
    "                timings[i] = curr_time\n",
    "\n",
    "        infer_time = np.sum(timings)\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            for _ in pbar:\n",
    "                if unpack == False:\n",
    "                    _ = model(input)\n",
    "                else:\n",
    "                    _ = model(*input)\n",
    "        \n",
    "        infer_time = pbar.format_dict[\"elapsed\"] * 1000\n",
    "\n",
    "    return infer_time"
   ],
   "metadata": {
    "id": "i_GePTGYQV33",
    "ExecuteTime": {
     "end_time": "2024-08-08T13:37:51.181047Z",
     "start_time": "2024-08-08T13:37:51.170047Z"
    }
   },
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we will use `run_model` function prepared above to benchmark the model.\n",
    "\n",
    "**Excercise**: implement `benchmark` function.\n",
    "\n",
    "1. Pass dummy inputs to `device`. \n",
    "2. Pass the model to `device`.\n",
    "3. Switch the model to `eval` mode.\n",
    "4. Warm-up the model using `run_model` with params `nwarmup` and `desc=\"Warm-up\"`.\n",
    "5. Measure inference time of the model using `run_model` with params `ninfer` and `desc=\"Inference timing\"`."
   ],
   "metadata": {
    "id": "uppYUXWo_GgQ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# VALIDATION_FIELD[func] benchmark\n",
    "\n",
    "def benchmark(model, dummy_input, nwarmup, ninfer, unpack=False, device=Config.device):\n",
    "\n",
    "    \"\"\"\n",
    "    Benchmarks a model inference\n",
    "\n",
    "    Arguments:\n",
    "    model -- a model we will benchmark\n",
    "    dummy_input -- list of dummy inputs for model inference\n",
    "    nwarmup -- steps for warm-up model\n",
    "    ninfer -- steps for main model inference \n",
    "    unpack -- bool indicator. Indicates unpack input or not\n",
    "    device -- type of device\n",
    "\n",
    "    Return:\n",
    "    infer_time -- inference time\n",
    "    \"\"\"\n",
    "\n",
    "    ### START CODE HERE ### (≈ 5 lines of code)\n",
    "    # pass dummy inputs to device\n",
    "    dummy_input = [inp.to(device) for inp in dummy_input]\n",
    "\n",
    "    # pass the model to device\n",
    "    model = model.to(device)\n",
    "    # switch the model to eval mode\n",
    "    model.eval()\n",
    "    # warm-up the model\n",
    "    _ = run_model(model, dummy_input, nwarmup, desc=\"Warm-up\", unpack=unpack)\n",
    "\n",
    "    # measure performance of the model\n",
    "    infer_time = run_model(model, dummy_input, ninfer, desc=\"Inference timing\", unpack=unpack)\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    print(f\"\\nInference time for the model: {infer_time:.2f} ms for {ninfer} runs\")\n",
    "    print(f\"Avg. inference time for the model: {(infer_time)/ ninfer:.2f} ms\\n\")\n",
    "\n",
    "    return infer_time"
   ],
   "metadata": {
    "id": "8VUeqwLN5WcO",
    "ExecuteTime": {
     "end_time": "2024-08-08T13:38:33.291073Z",
     "start_time": "2024-08-08T13:38:33.284077Z"
    }
   },
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's benchmark the scripted and native Yolov5s model:"
   ],
   "metadata": {
    "id": "MH1_UCvaW60D"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# create dummy_input for Yolov5s\n",
    "yolo_dummy_input = [torch.rand(Config.input_shape)for i in range(Config.n_imgs)]\n",
    "\n",
    "# benchmark\n",
    "print(\"Native Yolov5s benchmark:\\n\")\n",
    "native_yolo_infer_time = benchmark(native_yolov5s_model, yolo_dummy_input, nwarmup=Config.yolo_nwarmup, ninfer=Config.yolo_nruns)\n",
    "print(\"Scripted Yolov5s benchmark:\\n\")\n",
    "scripted_yolo_infer_time = benchmark(loaded_scripted_yolov5s_model, yolo_dummy_input, nwarmup=Config.yolo_nwarmup, ninfer=Config.yolo_nruns)\n",
    "\n",
    "print(f\"Scripted Yolov5s is {((native_yolo_infer_time - scripted_yolo_infer_time)/ native_yolo_infer_time) * 100 :.2f} percent faster than Native model\")"
   ],
   "metadata": {
    "id": "IFx2XzVI5Weu",
    "ExecuteTime": {
     "end_time": "2024-08-08T13:41:11.718661Z",
     "start_time": "2024-08-08T13:38:34.858791Z"
    }
   },
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Native Yolov5s benchmark:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warm-up:   0%|          | 0 run /20 runs [00:00<?, ? runs/s]C:\\Users\\maxog\\PycharmProjects\\DRU_labs\\.venv\\lib\\site-packages\\torch\\functional.py:512: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\TensorShape.cpp:3588.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "Warm-up: 100%|██████████| 20 run /20 runs [00:13<00:00,  1.50 runs/s]\n",
      "Inference timing: 100%|██████████| 100 run /100 runs [01:05<00:00,  1.53 runs/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Inference time for the model: 65346.44 ms for 100 runs\n",
      "Avg. inference time for the model: 653.46 ms\n",
      "\n",
      "Scripted Yolov5s benchmark:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warm-up:   0%|          | 0 run /20 runs [00:00<?, ? runs/s]code/__torch__/yolort/models/yolo.py:42: UserWarning: YOLO always returns a (Losses, Detections) tuple in scripting.\n",
      "code/__torch__/yolort/models/yolov5.py:56: UserWarning: YOLOv5 always returns a (Losses, Detections) tuple in scripting.\n",
      "Warm-up: 100%|██████████| 20 run /20 runs [00:14<00:00,  1.38 runs/s]\n",
      "Inference timing: 100%|██████████| 100 run /100 runs [01:03<00:00,  1.57 runs/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Inference time for the model: 63609.57 ms for 100 runs\n",
      "Avg. inference time for the model: 636.10 ms\n",
      "\n",
      "Scripted Yolov5s is 2.66 percent faster than Native model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Expected output**:\n",
    "```\n",
    "Native Yolov5s benchmark:\n",
    "\n",
    "Warm-up: 100%|██████████| 20 run /20 runs [00:03<00:00,  5.37 runs/s]\n",
    "Inference timing: 100%|██████████| 100 run /100 runs [00:18<00:00,  5.42 runs/s]\n",
    "\n",
    "Inference time for the model: 18185.01 ms for 100 runs\n",
    "Avg. inference time for the model: 181.85 ms\n",
    "\n",
    "Scripted Yolov5s benchmark:\n",
    "\n",
    "Warm-up: 100%|██████████| 20 run /20 runs [00:03<00:00,  5.94 runs/s]\n",
    "Inference timing: 100%|██████████| 100 run /100 runs [00:16<00:00,  5.94 runs/s]\n",
    "Inference time for the model: 16608.96 ms for 100 runs\n",
    "Avg. inference time for the model: 166.09 ms\n",
    "\n",
    "Scripted Yolov5s is 8.67 percent faster than Native model\n",
    "```\n",
    ">(Elapsed time and speed may slightly vary)\n",
    "\n",
    ">If you are running on Tesla K80, the scripted Yolov5s model will be **~ 6-10** percent faster than native Yolov5s model. For Telsa T4, the performance difference will be more significant on average **~ 15-20** percent.\n"
   ],
   "metadata": {
    "id": "bgh1Vev3HigM"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **BERT inference optimising**\n",
    "\n",
    "Here we will use BERT from the transformer’s library provided by HuggingFace."
   ],
   "metadata": {
    "id": "HMvV2OwqwLL5"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "## **Creating dummy input for BERT**\n",
    "\n",
    "Firstly let's initialise BERT tokenizer:"
   ],
   "metadata": {
    "id": "vQYGbKsOdK_w"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# VALIDATION_FIELD[func] tokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', torchscript=True)"
   ],
   "metadata": {
    "id": "bp47y22Llnk-",
    "ExecuteTime": {
     "end_time": "2024-08-08T13:41:42.677874Z",
     "start_time": "2024-08-08T13:41:40.289795Z"
    }
   },
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "The next step is to create a sample input data for inference."
   ],
   "metadata": {
    "id": "ljZEmcISO0fH"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# VALIDATION_FIELD[str]\n",
    "\n",
    "text = \"I live in [MASK], this country is located on the continent of the same name.\""
   ],
   "metadata": {
    "id": "aIy-rOVl0NC_",
    "ExecuteTime": {
     "end_time": "2024-08-08T13:41:42.693747Z",
     "start_time": "2024-08-08T13:41:42.679930Z"
    }
   },
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Excercise**: implement `create_dummy_input_for_bert` function to create dummy input for BERT model.\n",
    "\n",
    "- tokenize `text_example` using `encode_plus` with parameter `return_tensors=\"pt\"` and return a list with `'input_ids'` and `'attention_mask'`:"
   ],
   "metadata": {
    "id": "OUmMg2ZFidoy"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# VALIDATION_FIELD[func] create_dummy_input_for_bert\n",
    "\n",
    "def create_dummy_input_for_bert(text_example):\n",
    "\n",
    "    \"\"\"\n",
    "    Creates dummy input for BERT model\n",
    "\n",
    "    Arguments:\n",
    "    text_example -- string with a sentence example\n",
    "\n",
    "    Return:\n",
    "    dummy_input -- list of 'input_ids' and 'attention_mask'\n",
    "    \"\"\"\n",
    "\n",
    "    ### START CODE HERE ### (≈ 2-4 lines of code)\n",
    "    encoded_input = tokenizer.encode_plus(text_example, return_tensors=\"pt\")\n",
    "    return [encoded_input['input_ids'], encoded_input['attention_mask']]\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "dummy_input = create_dummy_input_for_bert(text)\n",
    "print(f\"{dummy_input[0]}\\n{dummy_input[1]}\")"
   ],
   "metadata": {
    "id": "qCdVVAv6bXm6",
    "ExecuteTime": {
     "end_time": "2024-08-08T13:43:42.408396Z",
     "start_time": "2024-08-08T13:43:42.387726Z"
    }
   },
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 101, 1045, 2444, 1999,  103, 1010, 2023, 2406, 2003, 2284, 2006, 1996, 9983, 1997, 1996, 2168, 2171, 1012,  102]])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Expected output**:\n",
    "\n",
    "```\n",
    "tensor([[ 101, 1045, 2444, 1999,  103, 1010, 2023, 2406, 2003, 2284, 2006, 1996, 9983, 1997, 1996, 2168, 2171, 1012,  102]]) \n",
    "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
    "```\n",
    "\n"
   ],
   "metadata": {
    "id": "TXqTNg7CqJW8"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **Loading native BERT model and tracing it**"
   ],
   "metadata": {
    "id": "r47jFRS2e2q7"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, we will load the pretrained BERT model:"
   ],
   "metadata": {
    "id": "YtWicjXvND4H"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# VALIDATION_FIELD[func] native_bert_model\n",
    "\n",
    "native_bert_model = BertModel.from_pretrained(\"bert-base-uncased\", torchscript=True)"
   ],
   "metadata": {
    "id": "k8UV9-ZCxBwk",
    "ExecuteTime": {
     "end_time": "2024-08-08T13:43:51.080953Z",
     "start_time": "2024-08-08T13:43:48.968154Z"
    }
   },
   "execution_count": 13,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we can create the trace of BERT model.\n",
    "\n",
    "**Excercise**: create the trace of BERT model, passing an instance of the model and `dummy_input` to `torch.jit.trace`:"
   ],
   "metadata": {
    "id": "W8lhzanJqyKH"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# VALIDATION_FIELD[func] traced_bert_model\n",
    "\n",
    "### START CODE HERE ### (1 line of code)\n",
    "traced_bert_model = torch.jit.trace(native_bert_model, dummy_input)\n",
    "### END CODE HERE ###"
   ],
   "metadata": {
    "id": "PskRGXoX4GTX",
    "ExecuteTime": {
     "end_time": "2024-08-08T13:44:01.529174Z",
     "start_time": "2024-08-08T13:43:59.915425Z"
    }
   },
   "execution_count": 14,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "For clarity, save the trace of BERT model and load it."
   ],
   "metadata": {
    "id": "i9HGjeRfy8dG"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "torch.jit.save(traced_bert_model, \"traced_bert.pt\")\n",
    "loaded_traced_bert_model = torch.jit.load(\"traced_bert.pt\")"
   ],
   "metadata": {
    "id": "IzMhAL4i4KF9",
    "ExecuteTime": {
     "end_time": "2024-08-08T13:44:03.722811Z",
     "start_time": "2024-08-08T13:44:02.862632Z"
    }
   },
   "execution_count": 15,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **Benchmarking native and traced BERT model**"
   ],
   "metadata": {
    "id": "rBdRV1V7lE-A"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# benchmark\n",
    "print(\"Native BERT benchmark:\\n\")\n",
    "native_bert_infer_time = benchmark(native_bert_model, dummy_input, nwarmup=Config.bert_nwarmup, ninfer=Config.bert_nruns, unpack=True)\n",
    "print(\"Traced BERT benchmark:\\n\")\n",
    "traced_bert_infer_time = benchmark(loaded_traced_bert_model, dummy_input, nwarmup=Config.bert_nwarmup, ninfer=Config.bert_nruns, unpack=True)\n",
    "\n",
    "print(f\"Traced BERT is {((native_bert_infer_time - traced_bert_infer_time)/ native_bert_infer_time) * 100 :.2f} percent faster than Native model\")"
   ],
   "metadata": {
    "id": "wqDlNgCnew-c",
    "ExecuteTime": {
     "end_time": "2024-08-08T13:44:47.915708Z",
     "start_time": "2024-08-08T13:44:05.038788Z"
    }
   },
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Native BERT benchmark:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warm-up: 100%|██████████| 50 run /50 runs [00:02<00:00, 24.20 runs/s]\n",
      "Inference timing: 100%|██████████| 500 run /500 runs [00:20<00:00, 24.94 runs/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Inference time for the model: 20049.34 ms for 500 runs\n",
      "Avg. inference time for the model: 40.10 ms\n",
      "\n",
      "Traced BERT benchmark:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warm-up: 100%|██████████| 50 run /50 runs [00:02<00:00, 23.83 runs/s]\n",
      "Inference timing: 100%|██████████| 500 run /500 runs [00:18<00:00, 26.83 runs/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Inference time for the model: 18639.31 ms for 500 runs\n",
      "Avg. inference time for the model: 37.28 ms\n",
      "\n",
      "Traced BERT is 7.03 percent faster than Native model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Expected output**:\n",
    "```\n",
    "Native BERT benchmark:\n",
    "\n",
    "Warm-up: 100%|██████████| 50 run /50 runs [00:00<00:00, 58.19 runs/s]\n",
    "Inference timing: 100%|██████████| 500 run /500 runs [00:07<00:00, 63.76 runs/s]\n",
    "\n",
    "Inference time for the model: 7643.96 ms for 500 runs\n",
    "Avg. inference time for the model: 15.29 ms\n",
    "\n",
    "Traced BERT benchmark:\n",
    "\n",
    "Warm-up: 100%|██████████| 50 run /50 runs [00:00<00:00, 75.80 runs/s]\n",
    "Inference timing: 100%|██████████| 500 run /500 runs [00:06<00:00, 76.36 runs/s]\n",
    "Inference time for the model: 6347.65 ms for 500 runs\n",
    "Avg. inference time for the model: 12.70 ms\n",
    "\n",
    "Traced BERT is 16.96 percent faster than Native model\n",
    "```\n",
    ">(Elapsed time and speed may slightly vary)\n",
    "\n",
    ">If you are running on Tesla K80, the traced BERT model will be **~ 10-20** percent faster than native BERT model. For Telsa T4, the performance difference will be more significant on average **~ 40-50** percent."
   ],
   "metadata": {
    "id": "amjnlwkGlM01"
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ]
}
